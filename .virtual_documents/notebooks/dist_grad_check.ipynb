import torch
import numpy as np
import os
VERSION = 11
if VERSION is not None:
    os.environ["CC"] = f"/usr/bin/gcc-{VERSION}"
    os.environ["CXX"] = f"/usr/bin/g++-{VERSION}"
from pathlib import Path
import sys
sys.path.insert(0, str(Path(os.path.abspath('')).parent))
from utils.train_util import render
from data.camera import Camera
from utils.compare_quad import setup_camera
height, width = 1, 1
fov = 90
g_vertices = torch.tensor([[ 0.2430675477, -0.3873929977,  0.3781652451],
        [ 0.2899414003, -0.1144087315,  0.2279857695],
        [-0.3941484988,  0.2187604010,  0.3265861571],
        [-0.3360970914,  0.1398774236,  0.1634573936]], device='cuda:0')
viewmat = torch.tensor([[-0.7677200437,  0.1756590605,  0.6162384152, -0.0968539640],
        [ 0.3055213988,  0.9456868172,  0.1110551804,  0.0558680296],
        [-0.5632607341,  0.2735333145, -0.7796902657,  0.2879556417],
        [ 0.0000000000,  0.0000000000,  0.0000000000,  1.0000000000]],
       device='cuda:0')
indices = torch.tensor([[0, 1, 2, 3]]).int().cuda()

tet_density = torch.tensor([[0.01]]).cuda()

# Setup camera
viewmat, projection_matrix, cam_pos, fovy, fovx, fx, fy = setup_camera(height, width, fov, viewmat)
# Now extract R,T from viewmat
# If viewmat is truly "World->View", then R is top-left 3x3, T is top-right 3x1
# V = torch.inverse(viewmat)
V = viewmat
R = V[:3, :3].T
T = V[:3, 3]

# Create a blank image for the camera
blank_image = torch.zeros((3, height, width), device="cuda")

# Instantiate the camera
camera = Camera(
    colmap_id = 0,
    R = R.cpu().numpy(),
    T = T.cpu().numpy(),
    fovx = fovx,
    fovy = fovy,
    image = blank_image,
    gt_alpha_mask = None,
    uid = 0,
    cx = -1,
    cy = -1,
    trans = np.array([0.0, 0.0, 0.0]), # or any translation offset you need
    scale = 1.0,
    data_device = "cuda",
    # You can add any extra distortions, exposure, etc. you might need
)
def f(v, t):
    model = lambda x: x
    model.vertices = v

    vertex_color = torch.tensor([
            [1.0, 0.0, 0.0],
            [0.0, 1.0, 0.0],
            [0.0, 0.0, 1.0],
            [0.5, 0.0, 0.5],
    ]).cuda()
    model.vertex_color = vertex_color
    model.tet_density = t
    model.indices = indices
    model.scene_scaling = 1
    def get_cell_values(camera, mask=None):
        if mask is not None:
            return vertex_color, tet_density[mask]
        else:
            return vertex_color, tet_density
    model.get_cell_values = get_cell_values

    render_pkg = render(camera, model, tile_size=16, min_t=0, ladder_p=1, pre_multi=1)
    # torch_loss = render_pkg['distortion_loss'].mean()
    torch_loss = render_pkg['render'].mean()
    return torch_loss


# Detach and enable gradients
def f_density(t):
    model = lambda x: x
    vertices = g_vertices.clone()
    model.vertices = vertices

    vertex_color = torch.tensor([
            [1.0, 0.0, 0.0],
            [0.0, 1.0, 0.0],
            [0.0, 0.0, 1.0],
            [0.5, 0.0, 0.5],
    ]).cuda()
    model.vertex_color = vertex_color
    model.tet_density = t
    model.indices = indices
    model.scene_scaling = 1
    def get_cell_values(camera, mask=None):
        if mask is not None:
            return vertex_color, tet_density[mask]
        else:
            return vertex_color, tet_density
    model.get_cell_values = get_cell_values

    render_pkg = render(camera, model, tile_size=16, min_t=0, ladder_p=1, pre_multi=1)
    torch_loss = render_pkg['distortion_loss'].mean()
    # print('alpha', render_pkg['render'].reshape(-1), 1-render_pkg['alpha'])
    # torch_loss = render_pkg['render'].mean()
    return torch_loss

vertices = vertices.requires_grad_(True)
tet_density = tet_density.requires_grad_(True)
slang_dist_loss = f_density(tet_density)
print(slang_dist_loss)
slang_dist_loss.backward()
print(tet_density.grad)
# torch.autograd.gradcheck(f_density, (tet_density))
# torch.autograd.gradcheck(f, (g_vertices, tet_density))


from jaxutil import tetra_quad
import matplotlib.pyplot as plt

vertex_color = torch.tensor([
        [1.0, 0.0, 0.0],
        [0.0, 1.0, 0.0],
        [0.0, 0.0, 1.0],
        [0.5, 0.0, 0.5],
]).cuda()

n_samples = 10000
tmin = 0

jax_image, extras = tetra_quad.render_camera(
    g_vertices.detach().cpu().numpy(), indices.cpu().numpy(),
    vertex_color.detach().cpu().numpy(),
    tet_density.detach().cpu().numpy(),
    height, width, viewmat.cpu().numpy(),
    fx.item(), fy.item(), tmin, np.linspace(0, 1, n_samples))
print(jax_image)
tdist = extras['tdist'][0, 0]
weights = extras['weights'][0, 0, 0]
plt.plot(tdist[:-1], weights, c='black')
plt.show()


def bar_plot(ft_np, fv):
    mid_points = 0.5 * (ft_np[:-1] + ft_np[1:])
    widths = ft_np[1:] - ft_np[:-1]

    plt.bar(mid_points, fv, width=widths)#, edgecolor="black")
    plt.ylabel('density')
    
def compress_piecewise_constant(t, v):
    ft = [t[0]]
    fv = [v[0]]
    for i in range(1, len(t)):
        if v[i] != v[i - 1]:
            ft.append(t[i])
            fv.append(v[i])

    return np.array(ft), np.array(fv)
density_samples = extras['total_density']
ft, fv = compress_piecewise_constant(tdist.reshape(-1), density_samples.reshape(-1))
print(ft, fv, density_samples)
bar_plot(ft, fv)
plt.show()


import jax.numpy as jnp
from jax import grad

def jf(verts_and_rgbs):
    verts, vertex_colors, tet_density = verts_and_rgbs
    img, _ = tetra_quad.render_camera(
        verts,
        indices.cpu().numpy(),
        vertex_colors,
        tet_density,
        height, width, viewmat.cpu().numpy(),
        fx.item(), fy.item(),
        tmin,
        jnp.linspace(0, 1, n_samples)
    )
    dist = img[..., 4]
    return dist.mean()
    return img[..., :3].mean()

# Compute JAX gradients using jacrev
jax_verts_grad, jax_vertex_color_grad, jax_tet_density_grad = grad(jf)((
    vertices.detach().cpu().numpy(),
    vertex_color.detach().cpu().numpy(),
    tet_density.detach().cpu().numpy()
))

print(jax_tet_density_grad)


def moment_0_int_torch(a, b, s):
    # a, b, s are torch Tensors
    return (1.0 - torch.exp(s * (a - b))) / s

def moment_1_int_torch(a, b, s):
    return (a*s - (b*s + 1.0)*torch.exp(s*(a - b)) + 1.0) / (s**2)

def self_dist_torch(a, b, s):
    ds = s*(a - b)
    return (2.0*ds*torch.exp(ds) + 1.0 - torch.exp(2.0*ds)) / (s**3)

def update_distortion_state_torch(state, t1, t2, sigma):
    """
    state is [x, y, z, w, v, T]
    t1, t2, sigma are scalars (torch tensors)
    """
    x, y, z, w, v, T = state

    m0 = T * moment_0_int_torch(t1, t2, sigma)
    m1 = T * moment_1_int_torch(t1, t2, sigma)

    new_z = z + m0
    new_w = w + m1
    new_x = x + z*m1
    new_y = y + w*m0

    # self_dist doesn't get multiplied by state-dependent terms (except T^2),
    # so that is direct from the integral:
    new_v = v + T*T* self_dist_torch(t1, t2, sigma)

    alpha = 1.0 - torch.exp(-sigma * (t2 - t1))
    new_T = T * (1.0 - alpha)

    return torch.stack([new_x, new_y, new_z, new_w, new_v, new_T])

def incremental_distortion_torch(t, sigma):
    """
    t: sorted boundaries, shape [N+1]
    sigma: densities, shape [N]
    """
    # state = [x, y, z, w, v, T], initialize as zeros but T=1
    state = torch.zeros(6, dtype=torch.float32, device=t.device)
    state[-1] = 1.0  # T=1
    for i in range(len(sigma)):
        state = update_distortion_state_torch(
            state,
            t[i], t[i+1],
            sigma[i]
        )
    x, y, _, _, v, _ = state
    # print(x, y, v)
    return 2.0*(x - y) + v

ft_t = torch.as_tensor(ft)
ft_t.requires_grad = True
fv_t = torch.as_tensor(fv[:1])
fv_t.requires_grad = True
print(ft_t, fv_t)
dist_loss = incremental_distortion_torch(ft_t, fv_t)
print(dist_loss)
dist_loss.backward()
print(fv_t.grad)






