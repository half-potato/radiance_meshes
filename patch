diff --git a/data/camera.py b/data/camera.py
index 5ed48ff..186e462 100644
--- a/data/camera.py
+++ b/data/camera.py
@@ -37,8 +37,6 @@ class Camera(nn.Module):
         self.T = T
         self.fovx = fovx
         self.fovy = fovy
-        self.cx = cx
-        self.cy = cy
         self.image_name = image_name
         self.model = model
         self.distortion_params = torch.as_tensor(distortion_params).float().clone() if distortion_params is not None else torch.zeros((4))
@@ -57,11 +55,15 @@ class Camera(nn.Module):
         self.original_image = image.clamp(0.0, 1.0).to(self.data_device)
         self.image_width = self.original_image.shape[2]
         self.image_height = self.original_image.shape[1]
+        self.cx = cx
+        self.cy = cy
 
         if gt_alpha_mask is not None:
             self.original_image *= gt_alpha_mask.to(self.data_device)
         else:
             self.original_image *= torch.ones((1, self.image_height, self.image_width), device=self.data_device)
+        self.fy = fov2focal(self.fovy, self.image_height)
+        self.fx = fov2focal(self.fovx, self.image_width)
 
         self.zfar = 100.0
         self.znear = 0.01
diff --git a/data/dataset_readers.py b/data/dataset_readers.py
index becd145..00d5e65 100644
--- a/data/dataset_readers.py
+++ b/data/dataset_readers.py
@@ -229,6 +229,7 @@ def readColmapSceneInfo(path, images, eval, llffhold=8):
                            train_cameras=train_cam_infos,
                            test_cameras=test_cam_infos,
                            nerf_normalization=nerf_normalization,
+                           transform=np.eye(4),
                            ply_path=ply_path)
     return scene_info
 
@@ -310,6 +311,7 @@ def readNerfSyntheticInfo(path, white_background, eval, extension=".png"):
                            train_cameras=train_cam_infos,
                            test_cameras=test_cam_infos,
                            nerf_normalization=nerf_normalization,
+                           transform=np.eye(4),
                            ply_path=ply_path)
     return scene_info
 
diff --git a/data/loader.py b/data/loader.py
index 08873bf..c45184a 100644
--- a/data/loader.py
+++ b/data/loader.py
@@ -7,10 +7,11 @@ from data.dataset_readers import sceneLoadTypeCallbacks
 from data.camera import Camera
 from data.types import BasicPointCloud
 from tqdm import tqdm
+from typing import Optional, Tuple
 
 WARNED = False
 
-def transform_poses_pca(poses):
+def transform_poses_pca(poses) -> Tuple[np.ndarray, np.ndarray]:
     """
     Transforms poses so principal components lie on XYZ axes.
 
@@ -87,8 +88,11 @@ def load_cam(data_device, resolution, id, cam_info, resolution_scale):
     if resized_image_rgb.shape[1] == 4:
         loaded_mask = resized_image_rgb[3:4, ...]
 
+    W = gt_image.shape[2]
+    H = gt_image.shape[1]
     return Camera(colmap_id=cam_info.uid, R=cam_info.R, T=cam_info.T, 
-                  fovx=cam_info.fovx, fovy=cam_info.fovy, cx=cam_info.cx, cy=cam_info.cy, 
+                  fovx=cam_info.fovx, fovy=cam_info.fovy, cx=cam_info.cx/cam_info.width*W, cy=cam_info.cy/cam_info.height*H, 
+                #   fovx=cam_info.fovx, fovy=cam_info.fovy, cx=cam_info.cx, cy=cam_info.cy,
                   image=gt_image, gt_alpha_mask=loaded_mask,
                   image_name=cam_info.image_name, uid=id, data_device=data_device,
                   model=cam_info.model, distortion_params=cam_info.distortion_params,
@@ -98,10 +102,11 @@ def load_cameras(cam_infos, resolution_scale, resolution, data_device):
     camera_list = []
     for id, c in tqdm(enumerate(cam_infos), total=len(cam_infos), desc="Loading cameras"):
         camera_list.append(load_cam(data_device, resolution, id, c, resolution_scale))
-    print(cam_infos[0])
+    if len(cam_infos) > 0:
+        print(cam_infos[0])
     return camera_list
 
-def transform_cameras_pca(cameras):
+def transform_cameras_pca(cameras) -> Tuple[np.ndarray, np.ndarray]:
     if len(cameras) == 0:
         return cameras, np.eye(4)
     poses = np.stack([
@@ -117,18 +122,13 @@ def transform_cameras_pca(cameras):
     return cameras, transform
 
 def set_pose(camera, T):
-    # camera.world_view_transform = T.T
-    # camera.full_proj_transform = (
-    #     camera.world_view_transform.unsqueeze(0).bmm(
-    #         camera.projection_matrix.unsqueeze(0))).squeeze(0)
-    # camera.camera_center = camera.world_view_transform.inverse()[3, :3]
     camera.R = T[:3, :3].T.numpy()
     camera.T = T[:3, 3].numpy()
     camera.update()
     return camera
 
 
-def load_dataset(source_path, images_folder, data_device, eval, white_background=True, resolution_scale=1.0, resolution=-1):
+def load_dataset(source_path, images_folder, data_device, eval, white_background=True, resolution_scale=1.0, resolution=-1, orient_scene=True):
     if os.path.exists(os.path.join(source_path, "sparse")):
         scene_info = sceneLoadTypeCallbacks["Colmap"](source_path, images_folder, eval)
     elif os.path.exists(os.path.join(source_path, "transforms_train.json")):
@@ -143,15 +143,17 @@ def load_dataset(source_path, images_folder, data_device, eval, white_background
     test_cameras = load_cameras(scene_info.test_cameras, resolution_scale, resolution, data_device)
     print(f"Loaded Test Cameras: {len(test_cameras)}")
 
-    print("Transforming poses")
-    _, pca_transform = transform_cameras_pca(train_cameras + test_cameras)
-    xyz = scene_info.point_cloud.points
-    xyz_hom = np.hstack((xyz, np.ones((xyz.shape[0], 1))))
-    xyz_transformed_hom = (pca_transform @ xyz_hom.T).T
-    transformed_pcd = scene_info.point_cloud._replace(points=xyz_transformed_hom[:, :3])
-    scene_info = scene_info._replace(
-        point_cloud=transformed_pcd,
-    )
+    if orient_scene:
+        print("Transforming poses")
+        _, pca_transform = transform_cameras_pca(train_cameras + test_cameras)
+        xyz = scene_info.point_cloud.points
+        xyz_hom = np.hstack((xyz, np.ones((xyz.shape[0], 1))))
+        xyz_transformed_hom = (pca_transform @ xyz_hom.T).T
+        transformed_pcd = scene_info.point_cloud._replace(points=xyz_transformed_hom[:, :3])
+        scene_info = scene_info._replace(
+            point_cloud=transformed_pcd,
+            transform=pca_transform,
+        )
 
 
     return train_cameras, test_cameras, scene_info
diff --git a/data/types.py b/data/types.py
index 0d77578..e4a9e2b 100644
--- a/data/types.py
+++ b/data/types.py
@@ -40,4 +40,5 @@ class SceneInfo(NamedTuple):
     train_cameras: list
     test_cameras: list
     nerf_normalization: dict
+    transform: np.array
     ply_path: str
diff --git a/delaunay_rasterization/__init__.py b/delaunay_rasterization/__init__.py
index 38545ce..7310951 100644
--- a/delaunay_rasterization/__init__.py
+++ b/delaunay_rasterization/__init__.py
@@ -1,17 +1,106 @@
-# Copyright 2024 Google LLC
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-__version__ = "0.1.0"
-__author__ = 'George Kopanas'
-__credits__ = 'Google'
+import numpy as np
+import torch
+from delaunay_rasterization.internal.alphablend_tiled_slang import render_constant_color
+from delaunay_rasterization.internal.alphablend_tiled_slang_interp import AlphaBlendTiledRender as Render
+from delaunay_rasterization.internal.render_grid import RenderGrid
+from delaunay_rasterization.internal.tile_shader_slang import vertex_and_tile_shader
+from data.camera import Camera
+
+import matplotlib.pyplot as plt
+cmap = plt.get_cmap("jet")
+
+def render_debug(render_tensor, model, camera, density_multi=1):
+
+    # Convert to RGB (NxMx3) using the colormap
+    _, features = model.get_cell_values(camera)
+    tet_grad_color = torch.zeros((features.shape[0], 4), device=features.device)
+    if render_tensor.shape[1] == 1:
+        tensor_min, tensor_max = render_tensor.min(), torch.quantile(render_tensor, 0.99)
+        normalized_tensor = ((render_tensor - tensor_min) / (tensor_max - tensor_min)).clip(0, 1)
+        normalized_tensor = torch.as_tensor(
+            cmap(normalized_tensor.reshape(-1).cpu().numpy())).float().cuda()
+    else:
+        normalized_tensor = render_tensor
+    tet_grad_color[:, :normalized_tensor.shape[1]] = normalized_tensor
+    if render_tensor.shape[1] < 4:
+        tet_grad_color[:, 3] = features[:, 0] * density_multi# * render_tensor.reshape(-1)
+    render_pkg = render_constant_color(model.indices, model.vertices, None, camera, cell_values=tet_grad_color)
+
+    image = render_pkg['render']
+    image = image.permute(1, 2, 0)
+    image = (image.detach().cpu().numpy() * 255).clip(min=0, max=255).astype(np.uint8)
+
+    del render_pkg, render_tensor
+    return image
+
+
+def render(camera: Camera, model, cell_values=None, tile_size=16, min_t=0.1,
+           scene_scaling=1, clip_multi=0, ray_jitter=None, glo=None,
+           **kwargs):
+    device = model.device
+    if ray_jitter is None:
+        ray_jitter = 0.5*torch.ones((camera.image_height, camera.image_width, 2), device=device)
+    else:
+        assert(ray_jitter.shape[0] == camera.image_height)
+        assert(ray_jitter.shape[1] == camera.image_width)
+        assert(ray_jitter.shape[2] == 2)
+    vertices = model.vertices
+    
+    render_grid = RenderGrid(camera.image_height,
+                             camera.image_width,
+                             tile_height=tile_size,
+                             tile_width=tile_size)
+    tcam = dict(
+        tile_height=tile_size,
+        tile_width=tile_size,
+        grid_height=render_grid.grid_height,
+        grid_width=render_grid.grid_width,
+        min_t=min_t,
+        **camera.to_dict(device)
+    )
+    sorted_tetra_idx, tile_ranges, vs_tetra, circumcenter, mask, _ = vertex_and_tile_shader(
+        model.indices,
+        vertices,
+        tcam,
+        render_grid)
+    extras = {}
+    if cell_values is None:
+        cell_values = torch.zeros((mask.shape[0], model.feature_dim), device=circumcenter.device)
+        if mask.sum() > 0 and model.mask_values:
+            normed_cc, cell_values[mask] = model.get_cell_values(camera, mask, circumcenter[mask], glo=glo)
+        else:
+            normed_cc, cell_values = model.get_cell_values(camera, all_circumcenters=circumcenter, glo=glo)
+        if clip_multi > 0 and not model.frozen:
+            with torch.no_grad():
+                tet_sens, sensitivity = topo_utils.compute_vertex_sensitivity(model.indices[mask],
+                                                                            vertices, normed_cc, True)
+                scaling = clip_multi*sensitivity.reshape(-1, 1).clip(min=1e-5)
+            vertices = ClippedGradients.apply(vertices, scaling)
+
+    image_rgb, distortion_img, tet_alive = Render.apply(
+        sorted_tetra_idx,
+        tile_ranges,
+        model.indices,
+        vertices,
+        cell_values,
+        render_grid,
+        tcam,
+        ray_jitter)
+    alpha = image_rgb.permute(2,0,1)[3, ...]
+    total_density = (distortion_img[:, :, 2]**2).clip(min=1e-6)
+    distortion_loss = (((distortion_img[:, :, 0] - distortion_img[:, :, 1]) + distortion_img[:, :, 4]) / total_density).clip(min=0)
+
+    
+    render_pkg = {
+        'render': image_rgb.permute(2,0,1)[:3, ...],
+        'alpha': alpha,
+        'distortion_img': distortion_img,
+        'distortion_loss': distortion_loss.mean(),
+        'visibility_filter': mask,
+        'circumcenters': circumcenter,
+        'density': cell_values[:, 0],
+        'color': cell_values[:, 1:],
+        'mask': mask,
+        **extras
+    }
+    return render_pkg
diff --git a/delaunay_rasterization/internal/render_err.py b/delaunay_rasterization/internal/render_err.py
index 6624a0b..1d4ef17 100644
--- a/delaunay_rasterization/internal/render_err.py
+++ b/delaunay_rasterization/internal/render_err.py
@@ -31,7 +31,7 @@ def gaussian_blur(img: torch.Tensor,
                     groups=img.shape[0]).squeeze(0)
 
 def render_err(gt_image, camera: Camera, model, tile_size=16,
-               scene_scaling=1, min_t=0.1, lambda_ssim=0.2, 
+               scene_scaling=1, min_t=0.1, glo=None,
                **kwargs):
     device = model.device
     indices = model.indices
@@ -63,7 +63,7 @@ def render_err(gt_image, camera: Camera, model, tile_size=16,
     except:
         pass
     cell_values = torch.zeros((mask.shape[0], model.feature_dim), device=circumcenter.device)
-    vertex_color, cell_values[mask] = model.get_cell_values(camera, mask)
+    vertex_color, cell_values[mask] = model.get_cell_values(camera, mask, glo=glo)
     # vertex_color, cell_values = model.get_cell_values(camera)
 
     # torch.cuda.synchronize()
@@ -81,7 +81,7 @@ def render_err(gt_image, camera: Camera, model, tile_size=16,
     tet_alive = torch.zeros((indices.shape[0]), dtype=bool, device=device)
     ray_jitter = 0.5*torch.ones((camera.image_height, camera.image_width, 2), device=device)
 
-    mod = slang_modules.alpha_blend_shaders_linear if model.linear else slang_modules.alpha_blend_shaders_interp
+    mod = slang_modules.alpha_blend_shaders_interp
     assert (render_grid.tile_height, render_grid.tile_width) in mod, (
         'Alpha Blend Shader was not compiled for this tile'
         f' {render_grid.tile_height}x{render_grid.tile_width} configuration, available configurations:'
@@ -110,17 +110,16 @@ def render_err(gt_image, camera: Camera, model, tile_size=16,
                     render_grid.grid_height, 1)
     )
     torch.cuda.synchronize()
-    alpha = 1-output_img.permute(2,0,1)[3, ...]
     render_img = output_img.permute(2,0,1)[:3, ...].clip(min=0, max=1)
     l1_err = ((render_img - gt_image)).mean(dim=0)
     ssim_err = (1-ssim(render_img, gt_image).mean(dim=0)).clip(min=0, max=1)
-    pixel_err = ((1-lambda_ssim) * l1_err + lambda_ssim * ssim_err).contiguous()
+    pixel_err = l1_err.contiguous()
 
     assert(pixel_err.shape[0] == render_grid.image_height)
     assert(pixel_err.shape[1] == render_grid.image_width)
 
     tet_err = torch.zeros((tet_vertices.shape[0], 16), dtype=torch.float, device=device)
-    tet_count = torch.zeros((tet_vertices.shape[0]), dtype=torch.int32, device=device)
+    tet_count = torch.zeros((tet_vertices.shape[0], 2), dtype=torch.int32, device=device)
 
     debug_img = torch.zeros((render_grid.image_height, 
                                 render_grid.image_width, 4), 
diff --git a/delaunay_rasterization/internal/slang/alphablend_shader_interp.slang b/delaunay_rasterization/internal/slang/alphablend_shader_interp.slang
index 0f21acd..747ce23 100644
--- a/delaunay_rasterization/internal/slang/alphablend_shader_interp.slang
+++ b/delaunay_rasterization/internal/slang/alphablend_shader_interp.slang
@@ -32,23 +32,6 @@ struct DistortionState5 : IDifferentiable {
     float v;  // new 5th term, e.g., for w_i^2 * interval / 3, or anything else
 };
 
-[Differentiable]
-float tukey_power_ladder(float x, float p) {
-  // Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)
-  if (p == 1) {
-    return x;
-  }
-  let xp = abs(x);
-  let xs = xp / max(TINY_VAL, abs(p - 1));
-  let y = sign(x) * abs(p - 1) / p * (pow(xs + 1, p) - 1);
-  return y;
-}
-
-[Differentiable]
-float safe_exp4alpha(float x) {
-    return safe_clip(safe_exp(x), 0, 1);
-}
-
 // [Differentiable]
 // float moment_0_int(float a, float b, float s1, float T) {
 //     return T * safe_div((-safe_expm1(s1*(a - b))),s1);
@@ -102,10 +85,14 @@ DistortionState5 update_distortion_state(
     float density,
     float2 dist)
 {
-    no_diff float start = dist.x;//tukey_power_ladder(dist.x, ladder_p);
-    no_diff float end = dist.y;  // tukey_power_ladder(dist.y, ladder_p);
-    float scaling = 1;//no_diff scale_dist((dist.x+dist.y)*0.5);
-    float d = density;
+    // no_diff float start = tukey_power_ladder(500*dist.x, -0.1);
+    // no_diff float end = tukey_power_ladder(500*dist.y, -0.1);
+    float start = (2*dist.x) / (2+dist.x);
+    float end = (2*dist.y) / (2+dist.y);
+    //float start = dist.x;
+    //float end = dist.y;
+    float scaling = abs(safe_div(dist.y - dist.x, end - start));//no_diff scale_dist((dist.x+dist.y)*0.5);
+    float d = density * scaling;//safe_clip(-safe_expm1(-density * (end-start)), 0.f, 1.f);
 
     float m0 = moment_0_int(start, end, d, T); // just weight
     float m1 = moment_1_int(start, end, d, T);
@@ -128,10 +115,14 @@ DistortionState5 undo_distortion_state(
     float density,
     float2 dist)
 {
-    float start = dist.x;
-    float end = dist.y;
-    float scaling = 1;
-    float d = density;
+    float start = (2*dist.x) / (2+dist.x);
+    float end = (2*dist.y) / (2+dist.y);
+    //float start = dist.x;
+    //float end = dist.y;
+    // no_diff float start = tukey_power_ladder(500*dist.x, -0.1);
+    // no_diff float end = tukey_power_ladder(500*dist.y, -0.1);
+    float scaling = abs(safe_div(dist.y - dist.x, end - start));//no_diff scale_dist((dist.x+dist.y)*0.5);
+    float d = density * scaling;//safe_clip(-safe_expm1(-density * (end-start)), 0.f, 1.f);
 
     float m0 = moment_0_int(start, end, d, T);
     float m1 = moment_1_int(start, end, d, T);
@@ -149,7 +140,7 @@ DistortionState5 undo_distortion_state(
 float4 update_pixel_state(float4 pixel_state_t_nm1, float4 gauss_rgba_t_n)
 {
     float3 color_t_n = pixel_state_t_nm1.rgb + gauss_rgba_t_n.rgb * pixel_state_t_nm1.a;
-    float transmittance_t_n = pixel_state_t_nm1.a * (1 - gauss_rgba_t_n.a);
+    float transmittance_t_n = pixel_state_t_nm1.a * safe_clip(1 - gauss_rgba_t_n.a, 0, 1);
     return float4(color_t_n, transmittance_t_n);
 }
 
@@ -216,14 +207,8 @@ AlphaOut alpha_blend(TensorView<int32_t> sorted_gauss_idx,
 
     const int shared_memory_rounds = ((tile_idx_end - tile_idx_start) + block_size - 1) / block_size;
     uint32_t thread_rank = cudaThreadIdx().y * cudaBlockDim().x + cudaThreadIdx().x;
-    //bool print = thread_rank == 0;
-    bool print = false;
-
 
     Ray ray = get_ray(cam, center_pix_coord);
-    // if (print) {
-    //     printf("ray_o: %f, %f, %f, ray_d: %f, %f, %f\n", ray.o.x, ray.o.y, ray.o.z, ray.d.x, ray.d.y, ray.d.z);
-    // }
 
     int32_t local_n_contrib = 0;
     // int32_t low = 0;
@@ -250,15 +235,7 @@ AlphaOut alpha_blend(TensorView<int32_t> sorted_gauss_idx,
                 if (evaluate_tetra_interp(g, ray.o, ray.d, cam.min_t, tetra_ctrl) && tetra_ctrl.rgba.a > 0) {
                     AlphaOut new_ao = update_alpha_out(ao, tetra_ctrl);
                     ao = new_ao;
-                    // tet_alive[collected_idx[j]] = true;
-                    // if (pix_coord.x == 3 && pix_coord.y == 0) {
-                    //     printf("%f-%f (%f), (%f, %f, %f, %f, %f)\n", tetra_ctrl.dist.x, tetra_ctrl.dist.y, tetra_ctrl.density,
-                    //     new_ao.distortion_state.x, new_ao.distortion_state.y, new_ao.distortion_state.z, new_ao.distortion_state.w, new_ao.distortion_state.v);
-                    // }
-
                     if (new_ao.rgba.a < 1 / 255.f) {
-                        // This Splat never registred so we subtract it before we break.
-                        // local_n_contrib--;
                         thread_active = false;
                         break;
                     }
@@ -270,7 +247,6 @@ AlphaOut alpha_blend(TensorView<int32_t> sorted_gauss_idx,
 
     if (is_inside)
         n_contributors[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)] = local_n_contrib;
-    // printf("%i/%i,", low, local_n_contrib);
 
     return ao;
 }
@@ -297,8 +273,6 @@ void bwd_alpha_blend(TensorView<int32_t> sorted_gauss_idx,
     uint32_t block_size = cam.tile_height * cam.tile_width;
     const int rounds = ((tile_idx_end - tile_idx_start + block_size - 1) / block_size);
 
-    bool print = false;//pix_coord.x == 0 && pix_coord.y == 0;
-
     int splats_left_to_process = tile_idx_end - tile_idx_start;
     int64_t current_splat_offset = tile_idx_end - tile_idx_start;
 
@@ -486,14 +460,9 @@ VarInfo kern_calc_tet_err(TensorView<int32_t> sorted_gauss_idx,
 
     const int shared_memory_rounds = ((tile_idx_end - tile_idx_start) + block_size - 1) / block_size;
     uint32_t thread_rank = cudaThreadIdx().y * cudaBlockDim().x + cudaThreadIdx().x;
-    //bool print = thread_rank == 0;
-    bool print = false;
 
 
     Ray ray = get_ray(cam, center_pix_coord);
-    // if (print) {
-    //     printf("ray_o: %f, %f, %f, ray_d: %f, %f, %f\n", ray.o.x, ray.o.y, ray.o.z, ray.d.x, ray.d.y, ray.d.z);
-    // }
 
     float max_split_alpha = 0.f;
     float max_grow_alpha = 0.f;
@@ -563,7 +532,9 @@ VarInfo kern_calc_tet_err(TensorView<int32_t> sorted_gauss_idx,
                     tet_err.InterlockedAdd(uint2(g_idx, 14), w * centroid.y, old_val);
                     tet_err.InterlockedAdd(uint2(g_idx, 15), w * centroid.z, old_val);
 
-                    tet_count.InterlockedAdd(g_idx, 1, old_ival);
+                    tet_count.InterlockedAdd(uint2(g_idx, 0), 1, old_ival);
+                    int T_int = (65535 * T);
+                    tet_count.InterlockedMax(uint2(g_idx, 1), T_int, old_ival);
 
                     ao = new_ao;
                     if (new_ao.rgba.a < 1 / 255.f) {
diff --git a/delaunay_rasterization/internal/slang/alphablend_shader_verts.slang b/delaunay_rasterization/internal/slang/alphablend_shader_verts.slang
new file mode 100644
index 0000000..2be60c3
--- /dev/null
+++ b/delaunay_rasterization/internal/slang/alphablend_shader_verts.slang
@@ -0,0 +1,617 @@
+// Copyright 2025 Alexander Mai
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     https://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+import random;
+import utils;
+import camera;
+import safe_math;
+import verts_version;
+
+static const uint TILE_HEIGHT = PYTHON_TILE_HEIGHT;
+static const uint TILE_WIDTH = PYTHON_TILE_WIDTH;
+
+groupshared SplatTetrahedra collected_splats[TILE_HEIGHT * TILE_WIDTH];
+groupshared uint32_t collected_idx[TILE_HEIGHT * TILE_WIDTH];
+
+struct DistortionState5 : IDifferentiable {
+    float x;  // old_state.x
+    float y;  // old_state.y
+    float z;  // old_state.z
+    float w;  // old_state.w
+    float v;  // new 5th term, e.g., for w_i^2 * interval / 3, or anything else
+};
+
+// [Differentiable]
+// float moment_0_int(float a, float b, float s1, float T) {
+//     return T * safe_div((-safe_expm1(s1*(a - b))),s1);
+// }
+// [Differentiable]
+// float moment_1_int(float a, float b, float s1, float T) {
+//     float s12 = s1*s1;
+//     if (s12 < 1e-6) {
+//         return T * 0.5 * max(b * b - a * a, 0);
+//     } else {
+//         // return (a*s1 - (b*s1 + 1)*safe_exp4alpha(s1*(a - b)) + 1) / (s1*s1);
+//         return T * max(
+//             - safe_div(safe_expm1(s1*(a - b)), s12)
+//             + safe_div(a - b*safe_exp(s1*(a - b)), s1), 0);
+//     }
+// }
+// [Differentiable]
+// float self_dist(float a, float b, float s, float T) {
+//     float dt = (a - b);
+//     float ds = s * dt;
+//     float s3 = s*s*s;
+//     if (abs(ds) < 1e-4 || s3 < 1e-5) {
+//         return T * T * max(-1 / 3 * pow(dt, 3), 0);
+//     } else {
+//         return T * T * max(
+//             safe_div(
+//                 ( 2 * s * (a - b) * exp(ds) - safe_expm1(2 * ds)),
+//                 s3),
+//             0);
+//     }
+// }
+
+[Differentiable]
+float moment_0_int(float a, float b, float s1, float T) {
+    return s1;
+}
+[Differentiable]
+float moment_1_int(float a, float b, float s1, float T) {
+    return s1 * (b+a)/2;
+}
+[Differentiable]
+float self_dist(float a, float b, float s, float T) {
+    float dt = max(b - a, 0);
+    return s*s*dt/3;
+}
+
+[Differentiable]
+DistortionState5 update_distortion_state(
+    DistortionState5 old_state,
+    float T,
+    float density,
+    float2 dist)
+{
+    // no_diff float start = tukey_power_ladder(500*dist.x, -0.1);
+    // no_diff float end = tukey_power_ladder(500*dist.y, -0.1);
+    float start = (2*dist.x) / (2+dist.x);
+    float end = (2*dist.y) / (2+dist.y);
+    // float start = dist.x;
+    // float end = dist.y;
+    float scaling = 1;//no_diff scale_dist((dist.x+dist.y)*0.5);
+    float d = safe_clip(-safe_expm1(-density * (end-start)), 0.f, 1.f);
+
+    float m0 = moment_0_int(start, end, d, T); // just weight
+    float m1 = moment_1_int(start, end, d, T);
+
+    DistortionState5 new_state;
+    new_state.x = old_state.x + 2.0f * m1 * old_state.z;
+    new_state.y = old_state.y + 2.0f * m0 * old_state.w;
+    new_state.z = old_state.z + m0;
+    new_state.w = old_state.w + m1;
+    // new_state.v = old_state.v + T * T * self_dist(start, end, d);
+    new_state.v = old_state.v + self_dist(start, end, d, T);
+    // printf("%f - %f, d: %f, v: %f T: %f, m0: %f, m1: %f\n", start, end, d, new_state.v, T, m0, m1);
+
+    return new_state;
+}
+
+DistortionState5 undo_distortion_state(
+    DistortionState5 new_state,
+    float T,
+    float density,
+    float2 dist)
+{
+    float start = (2*dist.x) / (2+dist.x);
+    float end = (2*dist.y) / (2+dist.y);
+    // float start = dist.x;
+    // float end = dist.y;
+    // no_diff float start = tukey_power_ladder(500*dist.x, -0.1);
+    // no_diff float end = tukey_power_ladder(500*dist.y, -0.1);
+    float scaling = 1;
+    // float d = density;
+    float d = safe_clip(-safe_expm1(-density * (end-start)), 0.f, 1.f);
+
+    float m0 = moment_0_int(start, end, d, T);
+    float m1 = moment_1_int(start, end, d, T);
+
+    DistortionState5 old_state;
+    old_state.z = new_state.z - m0;
+    old_state.w = new_state.w - m1;
+    old_state.x = new_state.x - 2.0f * m1 * old_state.z;
+    old_state.y = new_state.y - 2.0f * m0 * old_state.w;
+    old_state.v = new_state.v - self_dist(start, end, d, T);
+    return old_state;
+}
+
+[Differentiable]
+float4 update_pixel_state(float4 pixel_state_t_nm1, float4 gauss_rgba_t_n)
+{
+    float3 color_t_n = pixel_state_t_nm1.rgb + gauss_rgba_t_n.rgb * pixel_state_t_nm1.a;
+    float transmittance_t_n = pixel_state_t_nm1.a * safe_clip(1 - gauss_rgba_t_n.a, 0, 1);
+    return float4(color_t_n, transmittance_t_n);
+}
+
+float4 undo_pixel_state(float4 pixel_state_t_n, float4 gauss_rgba_t_n)
+{
+    float transmittance_t_nm1;
+    if (1 - gauss_rgba_t_n.a < 1e-3) {
+        transmittance_t_nm1 = 1;
+    } else {
+        transmittance_t_nm1 = safe_div(pixel_state_t_n.a, (1 - gauss_rgba_t_n.a));
+    }
+    float3 color_t_nm1 = pixel_state_t_n.rgb - gauss_rgba_t_n.rgb * transmittance_t_nm1;
+    return float4(color_t_nm1, transmittance_t_nm1);
+}
+
+struct AlphaOut: IDifferentiable {
+    float4 rgba;
+    DistortionState5 distortion_state;
+};
+
+[Differentiable]
+AlphaOut update_alpha_out(AlphaOut prev, CtrlPt ctrl) {
+    float T = prev.rgba.a;
+    return {
+        update_pixel_state(prev.rgba, ctrl.rgba),
+        update_distortion_state(prev.distortion_state, T, ctrl.density, ctrl.dist)
+    };
+}
+
+AlphaOut undo_alpha_out(AlphaOut curr, CtrlPt ctrl) {
+    float4 pix_state = undo_pixel_state(curr.rgba, ctrl.rgba);
+    float T = pix_state.a;
+    return {
+        pix_state,
+        undo_distortion_state(curr.distortion_state, T, ctrl.density, ctrl.dist)
+    };
+}
+
+[BackwardDerivative(bwd_alpha_blend)] // Use a custom derivative so that we can hand-write the structure of the reverse loop
+AlphaOut alpha_blend(TensorView<int32_t> sorted_gauss_idx,
+                   TensorView<int32_t> indices,
+                   DiffTensorView vertices,
+                   DiffTensorView tet_density,
+                   DiffTensorView final_pixel_state,
+                   DiffTensorView final_distortion_state,
+                   TensorView<int32_t> n_contributors,
+                   TensorView<bool> tet_alive,
+                   no_diff Camera cam,
+                   no_diff float ray_jitter_x,
+                   no_diff float ray_jitter_y,
+                   uint32_t2 pix_coord,
+                   uint32_t tile_idx_start,
+                   uint32_t tile_idx_end)
+{
+    float2 center_pix_coord = {pix_coord.x+ray_jitter_x, pix_coord.y+ray_jitter_y};
+    float4 curr_pixel_state = float4(0.f, 0.f, 0.f, 1.f);
+    DistortionState5 curr_distortion_state = {0.f, 0.f, 0.f, 0.f, 0.f};
+    AlphaOut ao = {
+        curr_pixel_state, curr_distortion_state
+    };
+    uint32_t block_size = cam.tile_height * cam.tile_width;
+    bool is_inside = (pix_coord.x < cam.W && pix_coord.y < cam.H);
+    bool thread_active = is_inside;
+
+    const int shared_memory_rounds = ((tile_idx_end - tile_idx_start) + block_size - 1) / block_size;
+    uint32_t thread_rank = cudaThreadIdx().y * cudaBlockDim().x + cudaThreadIdx().x;
+
+    Ray ray = get_ray(cam, center_pix_coord);
+
+    int32_t local_n_contrib = 0;
+    // int32_t low = 0;
+    int countdown = 1;
+    int splats_left_to_process = tile_idx_end - tile_idx_start;
+    for (int i = 0; i < shared_memory_rounds; i++)
+    {
+        // Collectively fetch per-Gaussian data from global to shared
+        AllMemoryBarrierWithGroupSync();
+        int splat_pointer_offset = i * block_size + thread_rank;
+        if (tile_idx_start + splat_pointer_offset < tile_idx_end)
+        {
+            uint32_t coll_id = uint32_t(sorted_gauss_idx[tile_idx_start + splat_pointer_offset]);
+            collected_idx[thread_rank] = coll_id;
+            collected_splats[thread_rank] = load_tet_alphablend_interp(coll_id, vertices, indices, tet_density);
+        }
+        AllMemoryBarrierWithGroupSync();
+        if (thread_active) {
+            for (int j = 0; j < min(block_size, splats_left_to_process); j++)
+            {
+                local_n_contrib++;
+                SplatTetrahedra g = collected_splats[j];
+                CtrlPt tetra_ctrl;
+                if (evaluate_tetra_interp(g, ray.o, ray.d, cam.min_t, tetra_ctrl) && tetra_ctrl.rgba.a > 0) {
+                    AlphaOut new_ao = update_alpha_out(ao, tetra_ctrl);
+                    ao = new_ao;
+                    if (new_ao.rgba.a < 1 / 255.f) {
+                        thread_active = false;
+                        break;
+                    }
+                }
+            }
+        }
+        splats_left_to_process -= block_size;
+    }
+
+    if (is_inside)
+        n_contributors[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)] = local_n_contrib;
+
+    return ao;
+}
+
+void bwd_alpha_blend(TensorView<int32_t> sorted_gauss_idx,
+                     TensorView<int32_t> indices,
+                     DiffTensorView vertices,
+                     DiffTensorView tet_density,
+                     DiffTensorView final_pixel_state,
+                     DiffTensorView final_distortion_state,
+                     TensorView<int32_t> n_contributors,
+                     TensorView<bool> tet_alive,
+                     no_diff Camera cam,
+                     no_diff float ray_jitter_x,
+                     no_diff float ray_jitter_y,
+                     uint32_t2 pix_coord,
+                     uint32_t tile_idx_start,
+                     uint32_t tile_idx_end,
+                     AlphaOut d_ao)
+{
+    float2 center_pix_coord = {pix_coord.x+ray_jitter_x, pix_coord.y+ray_jitter_y};
+    // Load the final pixel state.
+    bool is_inside = (pix_coord.x < cam.W && pix_coord.y < cam.H);
+    uint32_t block_size = cam.tile_height * cam.tile_width;
+    const int rounds = ((tile_idx_end - tile_idx_start + block_size - 1) / block_size);
+
+    int splats_left_to_process = tile_idx_end - tile_idx_start;
+    int64_t current_splat_offset = tile_idx_end - tile_idx_start;
+
+    float4 current_pixel_state;
+    DistortionState5 curr_distortion_state;
+    int32_t n_contrib_fwd;
+    if (is_inside) {
+        curr_distortion_state = {final_distortion_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)],
+                                 final_distortion_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 1)],
+                                 final_distortion_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 2)],
+                                 final_distortion_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 3)],
+                                 final_distortion_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 4)]};
+        current_pixel_state = float4(final_pixel_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)],
+                                     final_pixel_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 1)],
+                                     final_pixel_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 2)],
+                                     final_pixel_state[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 3)]);
+        n_contrib_fwd = n_contributors[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)];
+    }
+    AlphaOut ao = {current_pixel_state, curr_distortion_state};
+
+    Ray ray = get_ray(cam, center_pix_coord);
+
+    DifferentialPair<float2> dp_center_pix_coord = diffPair(center_pix_coord);
+
+    uint32_t thread_rank = cudaThreadIdx().y * cudaBlockDim().x + cudaThreadIdx().x;
+    for (int i = 0; i < rounds; i++)
+    {
+        // Collectively fetch per-Gaussian data from global to shared
+        AllMemoryBarrierWithGroupSync();
+        int progress = i * block_size + thread_rank;
+        if (tile_idx_start + progress < tile_idx_end)
+        {
+            uint32_t coll_id = uint32_t(sorted_gauss_idx[tile_idx_end - progress - 1]);
+            collected_idx[thread_rank] = coll_id;
+            collected_splats[thread_rank] = load_tet_alphablend_interp(coll_id, vertices, indices, tet_density);
+        }
+        AllMemoryBarrierWithGroupSync();
+        if (is_inside) {
+            for (int j = 0; j < min(block_size, splats_left_to_process); j++)
+            {
+                current_splat_offset--;
+                if (current_splat_offset >= n_contrib_fwd || current_splat_offset < 0)
+                    continue;
+                uint32_t g_idx = collected_idx[j];
+                SplatTetrahedra g = collected_splats[j];
+
+                CtrlPt tetra_ctrl;
+                if (evaluate_tetra_interp(g, ray.o, ray.d, cam.min_t, tetra_ctrl) && tetra_ctrl.rgba.a > 0) {
+                    // CtrlPt tetra_ctrl = evaluate_tetra_2D(g, cam, min_t, center_pix_coord);
+                    // if (tetra_ctrl.rgba.a < 1 / 255.f) {
+                    //     printf("Continue\n");
+                    //     continue;
+                    // }
+
+                    // Undo pixel state
+                    ao = undo_alpha_out(ao, tetra_ctrl);
+
+                    // Back-prop automatically through blending and gaussian evaluation.
+                    DifferentialPair<SplatTetrahedra> dp_g = diffPair(g);
+                    DifferentialPair<float3> dp_ray_o = diffPair(ray.o);
+                    DifferentialPair<float3> dp_ray_d = diffPair(ray.d);
+                    DifferentialPair<CtrlPt> dp_tetra_ctrl = diffPair(tetra_ctrl);
+                    DifferentialPair<AlphaOut> dp_ao = diffPair(ao);
+
+                    bwd_diff(update_alpha_out)(dp_ao, dp_tetra_ctrl, d_ao);
+                    d_ao = dp_ao.getDifferential();
+                    float midpt = tetra_ctrl.dist.x;
+                    float grad_scale = min(1, midpt*midpt);// / scene_scaling / scene_scaling);
+                    //float grad_scale = midpt*midpt / scene_scaling / scene_scaling;
+                    CtrlPt.Differential d_tetra_scale = {
+                        grad_scale * dp_tetra_ctrl.d.rgba,
+                        grad_scale * dp_tetra_ctrl.d.dist,
+                        grad_scale * dp_tetra_ctrl.d.density,
+                    };
+                    bwd_diff(evaluate_tetra_interp)(dp_g, dp_ray_o, dp_ray_d, cam.min_t, d_tetra_scale);
+                    // bwd_diff(evaluate_tetra_2D)(dp_g, cam, min_t, dp_center_pix_coord, dp_tetra_ctrl.d);
+                    bwd_diff(load_tet_alphablend_interp)(g_idx, vertices, indices, tet_density, dp_g.d);
+
+                }
+            }
+        }
+        splats_left_to_process -= block_size;
+    }
+}
+
+[AutoPyBindCUDA]
+[CUDAKernel]
+[Differentiable]
+void splat_tiled(TensorView<int32_t> sorted_gauss_idx,
+                 TensorView<int32_t> tile_ranges,
+                 TensorView<int32_t> indices,
+                 DiffTensorView vertices,
+                 DiffTensorView tet_density,
+                 DiffTensorView output_img,
+                 DiffTensorView distortion_img,
+                 TensorView<int32_t> n_contributors,
+                 TensorView<bool> tet_alive,
+                 TensorView<float> ray_jitter,
+                 TensorCamera tcam)
+{
+    uint32_t3 globalIdx = cudaBlockIdx() * cudaBlockDim() + cudaThreadIdx();
+
+    uint32_t2 pix_coord = globalIdx.xy;
+    Camera cam = no_diff load_tensor_camera(tcam);
+
+    uint32_t tile_idx = cudaBlockIdx().y * cam.grid_width + cudaBlockIdx().x;
+    uint32_t tile_idx_start = uint32_t(tile_ranges[uint2(tile_idx, 0)]);
+    uint32_t tile_idx_end = uint32_t(tile_ranges[uint2(tile_idx, 1)]);
+
+    bool is_inside = (pix_coord.x < output_img.size(1) && pix_coord.y < output_img.size(0));
+
+    float rjx = 0;
+    float rjy = 0;
+    if (is_inside) {
+        rjx = no_diff ray_jitter[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), uint32_t(0))];
+        rjy = no_diff ray_jitter[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), uint32_t(1))];
+    }
+
+    AlphaOut pixel_states = alpha_blend(
+        sorted_gauss_idx,
+        indices,
+        vertices,
+        tet_density,
+        output_img,
+        distortion_img,
+        n_contributors,
+        tet_alive,
+        cam,
+        rjx,
+        rjy,
+        pix_coord,
+        tile_idx_start,
+        tile_idx_end);
+                                    
+    if (is_inside) {
+      output_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0), pixel_states.rgba.r);
+      output_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 1), pixel_states.rgba.g);
+      output_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 2), pixel_states.rgba.b);
+      output_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 3), pixel_states.rgba.a);
+      distortion_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0), pixel_states.distortion_state.x);
+      distortion_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 1), pixel_states.distortion_state.y);
+      distortion_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 2), pixel_states.distortion_state.z);
+      distortion_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 3), pixel_states.distortion_state.w);
+      distortion_img.storeOnce(uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 4), pixel_states.distortion_state.v);
+    }
+}
+
+struct VarInfo {
+    float S0, S1, S2;
+    float2 dist;
+    int g_idx;
+};
+
+VarInfo kern_calc_tet_err(TensorView<int32_t> sorted_gauss_idx,
+                  TensorView<int32_t> indices,
+                  TensorView<float> vertices,
+                  TensorView<float> tet_density,
+                  TensorView<float> final_pixel_state,
+                  TensorView<float> tet_err,
+                  TensorView<int32_t> tet_count,
+                  TensorView<int32_t> n_contributors,
+                  Camera cam,
+                  uint32_t2 pix_coord,
+                  uint32_t tile_idx_start,
+                  uint32_t tile_idx_end,
+                  float pixel_err,
+                  float ssim_err,
+                  float3 gt_pixel)
+{
+    uint seed = hash_wang(pix_coord.x);
+    seed = hash_wang(seed ^ pix_coord.y);
+    seed = hash_wang(seed ^ tile_idx_end);
+    seed = hash_wang(seed ^ tile_idx_start);
+    RNG rng = {seed};
+
+    float2 center_pix_coord = {pix_coord.x+0.5, pix_coord.y+0.5};
+    float4 curr_pixel_state = float4(0.f, 0.f, 0.f, 1.f);
+    DistortionState5 curr_distortion_state = {0.f, 0.f, 0.f, 0.f, 0.f};
+    AlphaOut ao = {
+        curr_pixel_state, curr_distortion_state
+    };
+    uint32_t block_size = cam.tile_height * cam.tile_width;
+    bool is_inside = (pix_coord.x < cam.W && pix_coord.y < cam.H);
+    bool thread_active = is_inside;
+
+    const int shared_memory_rounds = ((tile_idx_end - tile_idx_start) + block_size - 1) / block_size;
+    uint32_t thread_rank = cudaThreadIdx().y * cudaBlockDim().x + cudaThreadIdx().x;
+
+
+    Ray ray = get_ray(cam, center_pix_coord);
+
+    float max_split_alpha = 0.f;
+    float max_grow_alpha = 0.f;
+    VarInfo split_info = {0.f, 0.f, 0.f, -1};
+
+    int32_t local_n_contrib = 0;
+    // int32_t low = 0;
+    int countdown = 1;
+    int splats_left_to_process = tile_idx_end - tile_idx_start;
+    for (int i = 0; i < shared_memory_rounds; i++)
+    {
+        // Collectively fetch per-Gaussian data from global to shared
+        AllMemoryBarrierWithGroupSync();
+        int splat_pointer_offset = i * block_size + thread_rank;
+        if (tile_idx_start + splat_pointer_offset < tile_idx_end)
+        {
+            uint32_t coll_id = uint32_t(sorted_gauss_idx[tile_idx_start + splat_pointer_offset]);
+            collected_idx[thread_rank] = coll_id;
+            collected_splats[thread_rank] = load_tet_alphablend_interp(coll_id, vertices, indices, tet_density);
+        }
+        AllMemoryBarrierWithGroupSync();
+        if (thread_active) {
+            for (int j = 0; j < min(block_size, splats_left_to_process); j++)
+            {
+                local_n_contrib++;
+                SplatTetrahedra g = collected_splats[j];
+                uint32_t g_idx = collected_idx[j];
+                CtrlPt tetra_ctrl;
+                if (evaluate_tetra_interp(g, ray.o, ray.d, cam.min_t, tetra_ctrl)) {
+                    //if (tetra_ctrl.rgba.a < 1/255.f) continue;
+                    AlphaOut new_ao = update_alpha_out(ao, tetra_ctrl);
+
+                    // calc grad scale to remove floaters
+                    float grad_scale = min(1, tetra_ctrl.dist.x*tetra_ctrl.dist.x);
+
+                    float T = grad_scale * ao.rgba.a * tetra_ctrl.rgba.a;
+                    float T_p = grad_scale * ao.rgba.a;
+                    // grow and split quantities
+                    float err = pixel_err;
+                    float mismatch = pixel_err;
+
+                    float old_val;
+                    int32_t old_ival;
+
+                    tet_err.InterlockedAdd(uint2(g_idx, 0), T, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 1), T * mismatch, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 2), T * mismatch * mismatch, old_val);
+                    T_p = 1;
+                    tet_err.InterlockedAdd(uint2(g_idx, 3), 1, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 4), T * abs(mismatch), old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 5), T * ssim_err, old_val);
+
+                    float w = T;
+                    float3 p1 = tetra_ctrl.dist.x * ray.d + ray.o;
+                    float3 p2 = tetra_ctrl.dist.y * ray.d + ray.o;
+                    tet_err.InterlockedAdd(uint2(g_idx, 6), w * p1.x, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 7), w * p1.y, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 8), w * p1.z, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 9), w * p2.x, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 10), w * p2.y, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 11), w * p2.z, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 12), w, old_val);
+
+                    float3 centroid = 0.5*(tetra_ctrl.dist.x+tetra_ctrl.dist.y) * ray.d + ray.o;
+                    // float3 centroid = tetra_ctrl.dist.y * ray.d + ray.o;
+                    tet_err.InterlockedAdd(uint2(g_idx, 13), w * centroid.x, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 14), w * centroid.y, old_val);
+                    tet_err.InterlockedAdd(uint2(g_idx, 15), w * centroid.z, old_val);
+
+                    tet_count.InterlockedAdd(g_idx, 1, old_ival);
+
+                    ao = new_ao;
+                    if (new_ao.rgba.a < 1 / 255.f) {
+                        // This Splat never registred so we subtract it before we break.
+                        // local_n_contrib--;
+                        thread_active = false;
+                        break;
+                    }
+                }
+            }
+        }
+        splats_left_to_process -= block_size;
+    }
+    return split_info;
+}
+
+[AutoPyBindCUDA]
+[CUDAKernel]
+void calc_tet_err(TensorView<int32_t> sorted_gauss_idx,
+                 TensorView<int32_t> tile_ranges,
+                 TensorView<int32_t> indices,
+                 TensorView<float> vertices,
+                 TensorView<float> tet_density,
+                 TensorView<float> output_img,
+                 TensorView<float> gt,
+                 TensorView<float> pixel_err,
+                 TensorView<float> ssim_err,
+                 TensorView<float> tet_err,
+                 TensorView<float> debug_img,
+                 TensorView<int32_t> tet_count,
+                 TensorView<int32_t> n_contributors,
+                 TensorCamera tcam)
+{
+    uint32_t3 globalIdx = cudaBlockIdx() * cudaBlockDim() + cudaThreadIdx();
+
+    uint32_t2 pix_coord = globalIdx.xy;
+
+    Camera cam = load_tensor_camera(tcam);
+    uint32_t tile_idx = cudaBlockIdx().y * cam.grid_width + cudaBlockIdx().x;
+    uint32_t tile_idx_start = uint32_t(tile_ranges[uint2(tile_idx, 0)]);
+    uint32_t tile_idx_end = uint32_t(tile_ranges[uint2(tile_idx, 1)]);
+
+    bool is_inside = (pix_coord.x < output_img.size(1) && pix_coord.y < output_img.size(0));
+
+
+    float this_pixel_err = 0;
+    float this_ssim_err = 0;
+    float3 this_gt = {0.f, 0.f, 0.f};
+    if (is_inside) {
+        this_pixel_err = pixel_err[uint2(uint32_t(pix_coord.y), uint32_t(pix_coord.x))];
+        this_ssim_err = ssim_err[uint2(uint32_t(pix_coord.y), uint32_t(pix_coord.x))];
+        this_gt = {
+            gt[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)],
+            gt[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 1)],
+            gt[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 2)],
+        };
+    }
+
+    VarInfo vi = kern_calc_tet_err(
+        sorted_gauss_idx,
+        indices,
+        vertices,
+        tet_density,
+        output_img,
+        tet_err,
+        tet_count,
+        n_contributors,
+        cam,
+        pix_coord,
+        tile_idx_start,
+        tile_idx_end,
+        this_pixel_err,
+        this_ssim_err,
+        this_gt);
+    if (is_inside) {
+      debug_img[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 0)] = vi.S0;
+      debug_img[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 1)] = vi.S1;
+      debug_img[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 2)] = vi.S2;
+      debug_img[uint3(uint32_t(pix_coord.y), uint32_t(pix_coord.x), 3)] = (float)vi.g_idx;
+    }
+}
diff --git a/delaunay_rasterization/internal/slang/interp_version.slang b/delaunay_rasterization/internal/slang/interp_version.slang
index 902a1e1..ac17f72 100644
--- a/delaunay_rasterization/internal/slang/interp_version.slang
+++ b/delaunay_rasterization/internal/slang/interp_version.slang
@@ -56,29 +56,73 @@ float3 compute_integral(float3 c0, float3 c1, float dt, float d) {
 }
 
 [Differentiable]
-inline float3 mix_color(float4x3 vertex_color, float4 bary) {
-    return max(vertex_color[0] * bary.x + vertex_color[1] * bary.y + vertex_color[2] * bary.z + vertex_color[3] * bary.w, 0.f);
+float compute_integral_1D(float c0, float c1, float d_dt) {
+    float alpha = safe_exp(-d_dt);
+    float X = (-d_dt*alpha + 1 - alpha);
+    float Y = (d_dt-1) + alpha;
+    return safe_div(X*c0+Y*c1, d_dt);
 }
 
-// [Differentiable]
-// inline float3 linear_color(float4x3 grad, float3 v)
-// {
-//     // return grad[0] + grad[1] * v.x + grad[2] * v.y + grad[3] * v.z;
-//     return {
-//         max(grad[0].r + dot(grad[1], v), 0.f),
-//         max(grad[0].g + dot(grad[2], v), 0.f),
-//         max(grad[0].b + dot(grad[3], v), 0.f),
-//     };
-// }
-
 [Differentiable]
-inline float3 linear_color(float3 grad, float3 v)
+inline float integrate_channel(
+    float t_n, float t_f,
+    float c_at_t0, float dc_dt, float density)
 {
-    // return grad[0] + grad[1] * v.x + grad[2] * v.y + grad[3] * v.z;
-    float f = dot(grad, v);
-    return {f, f, f};
+    // Find zero-crossing and adjust integration bounds to the positive segment
+    float t_zero = - safe_div(c_at_t0, dc_dt);
+    bool change_within = (t_n < t_zero) && (t_zero < t_f);
+    float c0 = max(c_at_t0 + dc_dt * t_n, 0.f);
+    float c1 = max(c_at_t0 + dc_dt * t_f, 0.f);
+    if (change_within) {
+        float cm = max(c_at_t0 + dc_dt * t_zero, 0.f);
+        float dt1 = max(t_zero - t_n, 0);
+        float dt2 = max(t_f - t_zero, 0);
+        float onemalpha = safe_clip(safe_exp(-density * dt1), 0.f, 1.f);
+        if (dc_dt < 0.f) {
+            return compute_integral_1D(cm, c0, dt1 * density);
+        }
+        if (dc_dt > 0.f) {
+            //return compute_integral_1D(0, 0, dt1, density) + onemalpha * compute_integral_1D(c1, cm, dt2, density);
+            return onemalpha * compute_integral_1D(c1, cm, dt2 * density);
+        }
+    }
+    float dt = max(t_f - t_n, 0.f);
+    return compute_integral_1D(c1, c0, dt * density);
 }
 
+/*
+[Differentiable]
+inline float integrate_channel(
+    float t_n, float t_f,
+    float c_at_t0, float dc_dt, float density)
+{
+    // Find where the linear color function C(t) would cross zero.
+    float t_zero = clamp(-safe_div(c_at_t0, dc_dt), t_n, t_f);
+
+    // Determine the start and end of the segment where C(t) > 0.
+    // This replaces the main "if (change_within)" and nested branches.
+    float t_start = (dc_dt > 0.0f) ? t_zero : t_n;
+    float t_end   = (dc_dt < 0.0f) ? t_zero : t_f;
+
+    // Clamp this "positive" segment to the actual integration bounds [t_n, t_f].
+    float dt_pos_segment = t_end - t_start;
+    float d_dt = dt_pos_segment * density;
+    if (d_dt < 1e-3) {
+        return 0.f;
+    }
+
+    // Calculate transmittance through the initial "zero-color" segment [t_n, t_start].
+    float dt_zero_segment = t_start - t_n;
+    float T_zero_segment = safe_exp(-density * dt_zero_segment);
+
+    // Calculate the integral over the clamped positive segment [t_start, t_end].
+    float c_start = c_at_t0 + dc_dt * t_start;
+    float c_end   = c_at_t0 + dc_dt * t_end;
+
+    // The final result is the attenuated integral over the positive part.
+    return T_zero_segment * compute_integral_1D(c_end, c_start, d_dt);
+}
+*/
 
 [Differentiable]
 bool evaluate_tetra_interp(
@@ -104,32 +148,18 @@ bool evaluate_tetra_interp(
         ctrl_pt = { (float4)(0.f, 0.f, 0.f, 0.f), {0.f, 0.f}, g.density };
         return false;
     } else {
-        /*
-        float3x3 T = transpose(float3x3(verts[0] - verts[3], verts[1] - verts[3], verts[2] - verts[3]));
-        float3 uvw_enter = SolveLinearSystem(T, enter * ray_d + ray_o - verts[3]);
-        float3 uvw_exit = SolveLinearSystem(T, exit * ray_d + ray_o - verts[3]);
-
-        float w_enter = safe_clip(1.0 - uvw_enter.x - uvw_enter.y - uvw_enter.z, 0, 1);
-        float w_exit = safe_clip(1.0 - uvw_exit.x - uvw_exit.y - uvw_exit.z, 0, 1);
-
-        float4 b_enter = float4(uvw_enter, w_enter);
-        float4 b_exit = float4(uvw_exit, w_exit);
-
-        float3 c_enter = mix_color(g.vertex_color, b_enter);
-        float3 c_exit = mix_color(g.vertex_color, b_exit);
-        */
-
-        //*
         float3 v_enter = enter * ray_d + ray_o - verts[0];
         float3 v_exit = exit * ray_d + ray_o - verts[0];
-        // float3 c_enter = max(linear_color(g.vertex_color, v_enter), 0.f);
-        // float3 c_exit = max(linear_color(g.vertex_color, v_exit), 0.f);
-        float3 c_enter = max(linear_color(g.grd, v_enter) + g.base_color, 0.f);
-        float3 c_exit = max(linear_color(g.grd, v_exit) + g.base_color, 0.f);
-        //*/
 
-        float alpha = safe_clip(1.0f - safe_exp(-g.density * dt), 0.f, 1.f);
-        float3 final_color = compute_integral(c_exit, c_enter, dt, g.density);
+        float alpha = safe_clip(-safe_expm1(-g.density * dt), 0.f, 1.f);
+        float dc_dt = dot(g.grd, ray_d);
+        float offset = dot(ray_o - verts[0], g.grd);
+        float3 final_color = {
+            integrate_channel(enter, exit, g.base_color.r + offset, dc_dt, g.density),
+            integrate_channel(enter, exit, g.base_color.g + offset, dc_dt, g.density),
+            integrate_channel(enter, exit, g.base_color.b + offset, dc_dt, g.density)
+        };
+        //*/
 
         float4 rgba = {final_color.x,
                        final_color.y,
@@ -152,11 +182,12 @@ SplatTetrahedra_interp load_tet_alphablend_interp(
     TensorView<int32_t> indices,
     DiffTensorView tet_density)
 {
+    // 4 ints
     no_diff uint4 virtual_tet = load_virtual_tetrahedra(g_idx, indices);
-    Tetrahedra tet    = load_tetrahedra(vertices, virtual_tet);
+    Tetrahedra tet    = load_tetrahedra(vertices, virtual_tet); // 12 floats
 
     // Here we assume each tetrahedron has a single scalar density in tet_density:
-    float density = float(tet_density[g_idx, 0]);
+    float density = max(float(tet_density[g_idx, 0]), 0.f);
     float3 base_color = {
         float(tet_density[g_idx, 1]),
         float(tet_density[g_idx, 2]),
@@ -167,6 +198,7 @@ SplatTetrahedra_interp load_tet_alphablend_interp(
         float(tet_density[g_idx, 5]),
         float(tet_density[g_idx, 6]),
     };
+    // 7 floats
 
     SplatTetrahedra_interp result = {
         tet, base_color, grd, density
@@ -184,7 +216,7 @@ SplatTetrahedra_interp load_tet_alphablend_interp(
     uint4 virtual_tet = load_virtual_tetrahedra(g_idx, indices);
     Tetrahedra tet    = load_tetrahedra(vertices, virtual_tet);
 
-    float density = float(tet_density[g_idx, 0]);
+    float density = max(float(tet_density[g_idx, 0]), 0.f);
     SplatTetrahedra_interp result;
     result.tet       = tet;
     result.density   = density;
diff --git a/delaunay_rasterization/internal/slang/intersect.slang b/delaunay_rasterization/internal/slang/intersect.slang
index 0c17d46..d1a3f86 100644
--- a/delaunay_rasterization/internal/slang/intersect.slang
+++ b/delaunay_rasterization/internal/slang/intersect.slang
@@ -392,28 +392,29 @@ bool ray_tetrahedron_intersect_fused(
 
 
 
-    float3 v0 = verts[0];
-    float3 v1 = verts[1];
-    float3 v2 = verts[2];
-    float3 v3 = verts[3];    
-    // const float3 n0 = cross(
-    //     verts[kTetTriangles[0][2]] - verts[kTetTriangles[0][0]],
-    //     verts[kTetTriangles[0][1]] - verts[kTetTriangles[0][0]]);
-    // const float d0  = -dot(n0, verts[kTetTriangles[0][0]] - orig);
-    // const float3 n1 = cross(
-    //     verts[kTetTriangles[1][2]] - verts[kTetTriangles[1][0]],
-    //     verts[kTetTriangles[1][1]] - verts[kTetTriangles[1][0]]);
-    // const float d1  = -dot(n1, verts[kTetTriangles[1][0]] - orig);
-    // const float3 n2 = cross(
-    //     verts[kTetTriangles[2][2]] - verts[kTetTriangles[2][0]],
-    //     verts[kTetTriangles[2][1]] - verts[kTetTriangles[2][0]]);
-    // const float d2  = -dot(n2, verts[kTetTriangles[2][0]] - orig);
-    // const float3 n3 = cross(
-    //     verts[kTetTriangles[3][2]] - verts[kTetTriangles[3][0]],
-    //     verts[kTetTriangles[3][1]] - verts[kTetTriangles[3][0]]);
-    // const float d3  = -dot(n3, verts[kTetTriangles[3][0]] - orig);
-
-
+    // float3 v0 = verts[0];
+    // float3 v1 = verts[1];
+    // float3 v2 = verts[2];
+    // float3 v3 = verts[3];    
+    const float3 n0 = -cross(
+        verts[kTetTriangles[0][2]] - verts[kTetTriangles[0][0]],
+        verts[kTetTriangles[0][1]] - verts[kTetTriangles[0][0]]);
+    const float d0  = -dot(n0, verts[kTetTriangles[0][0]] - orig);
+    const float3 n1 = -cross(
+        verts[kTetTriangles[1][2]] - verts[kTetTriangles[1][0]],
+        verts[kTetTriangles[1][1]] - verts[kTetTriangles[1][0]]);
+    const float d1  = -dot(n1, verts[kTetTriangles[1][0]] - orig);
+    const float3 n2 = -cross(
+        verts[kTetTriangles[2][2]] - verts[kTetTriangles[2][0]],
+        verts[kTetTriangles[2][1]] - verts[kTetTriangles[2][0]]);
+    const float d2  = -dot(n2, verts[kTetTriangles[2][0]] - orig);
+    const float3 n3 = -cross(
+        verts[kTetTriangles[3][2]] - verts[kTetTriangles[3][0]],
+        verts[kTetTriangles[3][1]] - verts[kTetTriangles[3][0]]);
+    const float d3  = -dot(n3, verts[kTetTriangles[3][0]] - orig);
+
+
+    /*
     // Face 0: (v0, v1, v2)
     float3 n0 = cross(v1 - v0, v2 - v0);
     if (dot(n0, v3 - v0) > 0) {
@@ -441,6 +442,7 @@ bool ray_tetrahedron_intersect_fused(
         n3 = -n3;
     }
     float d3 = -dot(n3, v1-orig);
+    */
 
     // --------------------------------------------------------------------
     // 2. Initialize the intersection range [t_enter, t_exit]
@@ -580,3 +582,70 @@ bool ray_tetrahedron_intersect_fused(
     t = { t_enter, t_exit };
     return true;
 }
+
+[Differentiable]
+bool ray_tetrahedron_intersect_fused_vectorized(
+    float3 orig, float3 dir,
+    float4x3 verts,
+    inout float2 t)
+{
+    static const uint3 kTetTriangles[4] = {
+        uint3(0, 2, 1), uint3(1, 2, 3), uint3(0, 3, 2), uint3(3, 0, 1)
+    };
+    // float4x3 outward_normals;
+    // for (int i=0; i<4; i++) {
+    //     outward_normals[i] = -cross(
+    //         verts[kTetTriangles[i][2]] - verts[kTetTriangles[i][0]],
+    //         verts[kTetTriangles[i][1]] - verts[kTetTriangles[i][0]]);
+    // }
+    const float4x3 outward_normals = float4x3(
+        -cross(verts[kTetTriangles[0][2]] - verts[0], verts[kTetTriangles[0][1]] - verts[0]),
+        -cross(verts[kTetTriangles[1][2]] - verts[1], verts[kTetTriangles[1][1]] - verts[1]),
+        -cross(verts[kTetTriangles[2][2]] - verts[0], verts[kTetTriangles[2][1]] - verts[0]),
+        -cross(verts[kTetTriangles[3][2]] - verts[3], verts[kTetTriangles[3][1]] - verts[3])
+    );
+
+
+    // float4x3 outward_normals = {n0, n1, n2, n3};
+    const float4 numerators = float4(
+        -dot(outward_normals[0], verts[kTetTriangles[0][0]] - orig),
+        -dot(outward_normals[1], verts[kTetTriangles[1][0]] - orig),
+        -dot(outward_normals[2], verts[kTetTriangles[2][0]] - orig),
+        -dot(outward_normals[3], verts[kTetTriangles[3][0]] - orig)
+    );
+
+    // float4 numerators = {d0, d1, d2, d3};
+    const float4 denoms = mul(outward_normals, dir);
+
+    // --- 4. Handle parallel rays. ---
+    // For an OUTWARD normal, the origin is outside if dot(n, o-v_base) > 0.
+    // This is equivalent to our numerator > 0.
+    const float eps = 1e-10f;
+    if (any((abs(denoms) < eps) & (numerators > 0.0f))) {
+        return false;
+    }
+
+    // --- 5. Perform the slab test using the OUTWARD normal convention. ---
+    const float4 t_planes = -numerators / denoms;
+    const float t_initial_enter = -1e30f;
+    const float t_initial_exit  = 1e30f;
+
+    // FIX: This logic now correctly matches the outward normal convention.
+    // We assume a standard ternary/select(condition, if_true, if_false) function.
+    // denom < 0 means the ray is ENTERING.
+    // denom > 0 means the ray is EXITING.
+    const float4 t_enters = select(denoms < 0.0f, t_planes, float4(t_initial_enter));
+    const float4 t_exits  = select(denoms > 0.0f, t_planes, float4(t_initial_exit));
+    
+    // --- 6. Find the final interval with horizontal reductions. ---
+    float t_enter = max(max(t_enters.x, t_enters.y), max(t_enters.z, t_enters.w));
+    float t_exit  = min(min(t_exits.x, t_exits.y), min(t_exits.z, t_exits.w));
+
+    // --- 7. Final intersection checks, identical to scalar code. ---
+    if (t_enter > t_exit || t_exit <= 0.0f) {
+        return false;
+    }
+
+    t = float2(max(t_enter, 0.f), t_exit);
+    return true;
+}
diff --git a/delaunay_rasterization/internal/slang/safe-math.slang b/delaunay_rasterization/internal/slang/safe-math.slang
index 15bcc2c..152240f 100644
--- a/delaunay_rasterization/internal/slang/safe-math.slang
+++ b/delaunay_rasterization/internal/slang/safe-math.slang
@@ -159,7 +159,7 @@ float safe_log(float v) {
 }
 
 void bw_expm1(inout DifferentialPair<float> v, float.Differential R) {
-  v = DifferentialPair<float>(v.p, -exp(v.p) * R);
+  v = DifferentialPair<float>(v.p, exp(v.p) * R);
 }
 
 float _expm1(float v) {
diff --git a/delaunay_rasterization/internal/slang/slang_modules.py b/delaunay_rasterization/internal/slang/slang_modules.py
index 3b9486d..feed4c6 100644
--- a/delaunay_rasterization/internal/slang/slang_modules.py
+++ b/delaunay_rasterization/internal/slang/slang_modules.py
@@ -17,9 +17,9 @@ import os
 
 shaders_path = os.path.dirname(__file__)
 
-TILE_SIZES_HW = [(4,4), (8,8), (16,16)]
-# TILE_SIZES_HW = [(8,8), (16,16)]
-# TILE_SIZES_HW = [(16,16)]
+# TILE_SIZES_HW = [(4,4), (8,8), (16,16)]
+# TILE_SIZES_HW = [(8,8)]
+TILE_SIZES_HW = [(16,16)]
 
 vertex_shader = slangtorch.loadModule(os.path.join(shaders_path, "vertex_shader.slang"))
 tile_shader = slangtorch.loadModule(os.path.join(shaders_path, "tile_shader.slang"))
@@ -33,7 +33,7 @@ for tile_height, tile_width in TILE_SIZES_HW:
   alpha_blend_shaders_interp[(tile_height, tile_width)] = slangtorch.loadModule(os.path.join(shaders_path, "alphablend_shader_interp.slang"), 
                                                                          defines={"PYTHON_TILE_HEIGHT": tile_height, "PYTHON_TILE_WIDTH": tile_width})
 
-alpha_blend_shaders_linear = {}
-for tile_height, tile_width in TILE_SIZES_HW:
-  alpha_blend_shaders_linear[(tile_height, tile_width)] = slangtorch.loadModule(os.path.join(shaders_path, "alphablend_shader_linear.slang"), 
-                                                                         defines={"PYTHON_TILE_HEIGHT": tile_height, "PYTHON_TILE_WIDTH": tile_width})
+# alpha_blend_shaders_linear = {}
+# for tile_height, tile_width in TILE_SIZES_HW:
+#   alpha_blend_shaders_linear[(tile_height, tile_width)] = slangtorch.loadModule(os.path.join(shaders_path, "alphablend_shader_linear.slang"), 
+#                                                                          defines={"PYTHON_TILE_HEIGHT": tile_height, "PYTHON_TILE_WIDTH": tile_width})
diff --git a/delaunay_rasterization/internal/slang/stable_power.slang b/delaunay_rasterization/internal/slang/stable_power.slang
index 66f4232..a177291 100644
--- a/delaunay_rasterization/internal/slang/stable_power.slang
+++ b/delaunay_rasterization/internal/slang/stable_power.slang
@@ -93,8 +93,9 @@ double power_of_circumsphere(
         center = A + relative_circumcenter;
         radius = length(relative_circumcenter);
     }
+    if (radius * radius > 1e4) return -1e20;
 
     // Return the power of the circumsphere
     double3 diff = center - P;
     return dot(diff, diff) - radius * radius;
-}
\ No newline at end of file
+}
diff --git a/delaunay_rasterization/internal/slang/vertex_shader.slang b/delaunay_rasterization/internal/slang/vertex_shader.slang
index 3eac260..95005ae 100644
--- a/delaunay_rasterization/internal/slang/vertex_shader.slang
+++ b/delaunay_rasterization/internal/slang/vertex_shader.slang
@@ -127,7 +127,7 @@ void vertex_shader(TensorView<int32_t> indices,
     no_diff float3 minv, maxv;
     no_diff find_extent2(camspace_tet, minv, maxv, cam);
     // no_diff float max_alpha = get_max_alpha(maxv.z - minv.z, virtual_tet, densities);
-    // printf("(%f, %f, %f) - (%f, %f, %f)\n", minv.x, minv.y, minv.z, maxv.x, maxv.y, maxv.z);
+    // printf("%i: (%f, %f, %f) - (%f, %f, %f)\n", g_idx, minv.x, minv.y, minv.z, maxv.x, maxv.y, maxv.z);
 
     // Check if tetrahedron is completely outside any frustum plane
     if (maxv.z <= 0.0) {// && max_alpha < 0.5/255.f) {
@@ -148,30 +148,43 @@ void vertex_shader(TensorView<int32_t> indices,
     float sort_depth = float(power_of_circumsphere(A, B, C, D, P, circumcenter));
     if (sort_depth < -1e19) return;
 
-    bool contains_ray_origin = point_in_tetrahedron(tet, cam.position) || sort_depth < 0;
+    // bool contains_ray_origin = point_in_tetrahedron(tet, cam.position) || sort_depth < 0;
+    // if (contains_ray_origin) {
+    //     printf("%i contains ray origin\n", g_idx);
+    // }
     // printf("contains: %i\n", contains_ray_origin);
 
 
-    if (contains_ray_origin) {
-        rect_tile_space.min_x = 0;
-        rect_tile_space.max_x = cam.grid_width;
-        rect_tile_space.min_y = 0;
-        rect_tile_space.max_y = cam.grid_height;
-        out_vs.storeOnce(uint2(g_idx, 1), 1);
-    } else {
-        rect_tile_space.min_x = int32_t(floor(clip(minv.x / cam.tile_width, 0, cam.grid_width)));
-        rect_tile_space.min_y = int32_t(floor(clip(minv.y / cam.tile_height, 0, cam.grid_height)));
-        rect_tile_space.max_x = int32_t(ceil(clip(maxv.x / cam.tile_width, 0, cam.grid_width)));
-        rect_tile_space.max_y = int32_t(ceil(clip(maxv.y / cam.tile_height, 0, cam.grid_height)));
-    }
+    // if (contains_ray_origin) {
+    //     rect_tile_space.min_x = 0;
+    //     rect_tile_space.max_x = cam.grid_width;
+    //     rect_tile_space.min_y = 0;
+    //     rect_tile_space.max_y = cam.grid_height;
+    //     out_vs.storeOnce(uint2(g_idx, 1), 1);
+    // } else {
+    //     rect_tile_space.min_x = int32_t(floor(clip(minv.x / cam.tile_width, 0, cam.grid_width)));
+    //     rect_tile_space.min_y = int32_t(floor(clip(minv.y / cam.tile_height, 0, cam.grid_height)));
+    //     rect_tile_space.max_x = int32_t(ceil(clip(maxv.x / cam.tile_width, 0, cam.grid_width)));
+    //     rect_tile_space.max_y = int32_t(ceil(clip(maxv.y / cam.tile_height, 0, cam.grid_height)));
+    // }
+    rect_tile_space.min_x = int32_t(floor(clip(minv.x / cam.tile_width, 0, cam.grid_width)));
+    rect_tile_space.min_y = int32_t(floor(clip(minv.y / cam.tile_height, 0, cam.grid_height)));
+    rect_tile_space.max_x = int32_t(ceil(clip(maxv.x / cam.tile_width, 0, cam.grid_width)));
+    rect_tile_space.max_y = int32_t(ceil(clip(maxv.y / cam.tile_height, 0, cam.grid_height)));
     int32_t n_tiles = (rect_tile_space.max_x - rect_tile_space.min_x) * (rect_tile_space.max_y - rect_tile_space.min_y);
+    // printf("%i: (%i, %i) - (%i, %i). %i\n", g_idx,
+    //     rect_tile_space.min_x,
+    //     rect_tile_space.min_y,
+    //     rect_tile_space.max_x,
+    //     rect_tile_space.max_y,
+    //     n_tiles);
 
     if (n_tiles == 0) {
         return;
     }
 
-    Ray center_ray = get_ray(cam, { cam.W / 2, cam.H / 2 });
-    Tetrahedra image_tet = camspace_tet2image_w_depth(camspace_tet, cam);
+    // Ray center_ray = get_ray(cam, { cam.W / 2, cam.H / 2 });
+    // Tetrahedra image_tet = camspace_tet2image_w_depth(camspace_tet, cam);
 
     // float3 circumcenter = calculate_circumcenter(tet);
     // float radius = length(circumcenter - tet.verts[0]);
diff --git a/delaunay_rasterization/internal/slang/verts_version.slang b/delaunay_rasterization/internal/slang/verts_version.slang
new file mode 100644
index 0000000..26a03c0
--- /dev/null
+++ b/delaunay_rasterization/internal/slang/verts_version.slang
@@ -0,0 +1,200 @@
+import intersect;
+import utils;
+import safe_math;
+
+struct SplatTetrahedra : IDifferentiable
+{
+    Tetrahedra tet;
+    float4x4 vertex_rgbs; // Per-vertex color to be barycentrically interpolated
+};
+
+// Given three vertex-colors (c0, c1, c2) and the barycentric coords (u,v)
+//   w0 = u, w1 = v, w2 = 1 - u - v
+// returns the final interpolated color:
+[Differentiable]
+float3 interpolate_triangle_color(
+    float3 c0, float3 c1, float3 c2,
+    float u, float v)
+{
+    float w0 = u;
+    float w1 = v;
+    float w2 = 1.0f - w0 - w1;
+    return c0 * w0 + c1 * w1 + c2 * w2;
+}
+
+[Differentiable]
+inline float4 mix_color(float4x4 vertex_color, float4 bary) {
+    return vertex_color[0] * bary.x + vertex_color[1] * bary.y + vertex_color[2] * bary.z + vertex_color[3] * bary.w;
+}
+
+[Differentiable]
+float4 tet_barycentric(float3 p, float4x3 vs)
+{
+    // Construct matrix T from tetrahedron vertices
+    float3x3 T = float3x3(vs[0] - vs[3], vs[1] - vs[3], vs[2] - vs[3]);
+
+    // Solve for (u, v, w)
+    float3 uvw = SolveLinearSystem(T, p - vs[3]);
+
+    // Compute t
+    float t = 1.0 - uvw.x - uvw.y - uvw.z;
+
+    return float4(uvw, t);
+}
+
+void print_verts(float4x3 verts) {
+    printf("%f, %f, %f, %f\n", verts[0].x, verts[1].x, verts[2].x, verts[3].x);
+}
+void print_vec(float3 vec) {
+    printf("(%f, %f, %f),", vec.x, vec.y, vec.z);
+}
+
+[Differentiable]
+float3 compute_integral(float3 c0, float3 c1, float dt, float d) {
+    float alpha = safe_exp(-d*dt);
+    float X = (-d*dt*alpha + 1 - alpha);
+    float Y = (d*dt-1) + alpha;
+    return safe_div(X*c0+Y*c1, d*dt);
+}
+
+[Differentiable]
+float compute_integral_1D(float c0, float c1, float d_dt) {
+    float alpha = safe_exp(-d_dt);
+    float X = (-d_dt*alpha + 1 - alpha);
+    float Y = (d_dt-1) + alpha;
+    return safe_div(X*c0+Y*c1, d_dt);
+}
+
+[Differentiable]
+inline float integrate_channel(
+    float t_n, float t_f,
+    float c_at_t0, float dc_dt, float density)
+{
+    // Find zero-crossing and adjust integration bounds to the positive segment
+    float t_zero = - safe_div(c_at_t0, dc_dt);
+    bool change_within = (t_n < t_zero) && (t_zero < t_f);
+    float c0 = max(c_at_t0 + dc_dt * t_n, 0.f);
+    float c1 = max(c_at_t0 + dc_dt * t_f, 0.f);
+    if (change_within) {
+        float cm = max(c_at_t0 + dc_dt * t_zero, 0.f);
+        float dt1 = max(t_zero - t_n, 0);
+        float dt2 = max(t_f - t_zero, 0);
+        float onemalpha = safe_clip(safe_exp(-density * dt1), 0.f, 1.f);
+        if (dc_dt < 0.f) {
+            return compute_integral_1D(cm, c0, dt1 * density);
+        }
+        if (dc_dt > 0.f) {
+            //return compute_integral_1D(0, 0, dt1, density) + onemalpha * compute_integral_1D(c1, cm, dt2, density);
+            return onemalpha * compute_integral_1D(c1, cm, dt2 * density);
+        }
+    }
+    float dt = max(t_f - t_n, 0.f);
+    return compute_integral_1D(c1, c0, dt * density);
+}
+
+[Differentiable]
+bool evaluate_tetra_interp(
+    in SplatTetrahedra g, 
+    in float3 ray_o,
+    in float3 ray_d,
+    no_diff in float t_min,
+    out CtrlPt ctrl_pt)
+{
+    float2 dist;
+
+    float4x3 verts = float4x3(g.tet.verts[0], g.tet.verts[1], g.tet.verts[2], g.tet.verts[3]);
+
+    bool hit = ray_tetrahedron_intersect_fused(
+        ray_o, ray_d, verts, dist);
+
+    float enter = dist.x;
+    float exit = max(dist.y, enter);
+    float dt = max(dist.y - enter, 0.0f);
+    float intersection_t = 0.5*(exit+enter);
+
+    if (!hit || dt < 1e-5f) {
+        ctrl_pt = { (float4)(0.f, 0.f, 0.f, 0.f), {0.f, 0.f}, g.density };
+        return false;
+    } else {
+        float3x3 T = transpose(float3x3(verts[0] - verts[3], verts[1] - verts[3], verts[2] - verts[3]));
+        float3 uvw_enter = SolveLinearSystem(T, enter * ray_d + ray_o - verts[3]);
+        float3 uvw_exit = SolveLinearSystem(T, dist.y * ray_d + ray_o - verts[3]);
+
+        float w_enter = safe_clip(1.0 - uvw_enter.x - uvw_enter.y - uvw_enter.z, 0, 1);
+        float w_exit = safe_clip(1.0 - uvw_exit.x - uvw_exit.y - uvw_exit.z, 0, 1);
+
+        float4 b_enter = float4(uvw_enter, w_enter);
+        float4 b_exit = float4(uvw_exit, w_exit);
+
+        float4 c_enter = mix_color(g.vertex_color, b_enter);
+        float4 c_exit = mix_color(g.vertex_color, b_exit);
+
+        float alpha = safe_clip(1.0f - safe_exp(-g.density * dt), 0.f, 1.f);
+        float3 final_color = compute_integral(c_exit, c_enter, dt, g.density);
+
+        float alpha = safe_clip(-safe_expm1(-g.density * dt), 0.f, 1.f);
+        float dc_dt = dot(g.grd, ray_d);
+        float offset = dot(ray_o - verts[0], g.grd);
+        float3 final_color = {
+            integrate_channel(enter, exit, g.base_color.r + offset, dc_dt, g.density),
+            integrate_channel(enter, exit, g.base_color.g + offset, dc_dt, g.density),
+            integrate_channel(enter, exit, g.base_color.b + offset, dc_dt, g.density)
+        };
+        //*/
+
+        float4 rgba = {final_color.x,
+                       final_color.y,
+                       final_color.z,
+                       alpha};
+        ctrl_pt = { rgba, {enter, exit}, g.density };
+        return true;
+    }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// 5) Loading functions for separate density & vertex colors
+//    We add "_interp" suffix to keep the old ones intact.
+////////////////////////////////////////////////////////////////////////////////
+
+[Differentiable]
+SplatTetrahedra load_tet_alphablend_interp(
+    int32_t        g_idx,
+    DiffTensorView vertices,
+    TensorView<int32_t> indices,
+    DiffTensorView vert_attr)
+{
+    // 4 ints
+    no_diff uint4 virtual_tet = load_virtual_tetrahedra(g_idx, indices);
+    Tetrahedra tet    = load_tetrahedra(vertices, virtual_tet); // 12 floats
+    float4x4 vert_rgbs = float4x4(
+        read_t4_float4(virtual_tet.x, vert_attr),
+        read_t4_float4(virtual_tet.y, vert_attr),
+        read_t4_float4(virtual_tet.z, vert_attr),
+        read_t4_float4(virtual_tet.w, vert_attr)
+    );
+    SplatTetrahedra result = {
+        tet, vert_rgbs
+    };
+    return result;
+}
+
+// Non-diff version (if needed)
+SplatTetrahedra load_tet_alphablend_interp(
+    int32_t        g_idx,
+    TensorView<float> vertices,
+    TensorView<int32_t> indices,
+    TensorView<float>  vert_attr)
+{
+    uint4 virtual_tet = load_virtual_tetrahedra(g_idx, indices);
+    Tetrahedra tet    = load_tetrahedra(vertices, virtual_tet);
+    float4x4 vert_rgbs = float4x4(
+        read_t4_float4(virtual_tet.x, vert_attr),
+        read_t4_float4(virtual_tet.y, vert_attr),
+        read_t4_float4(virtual_tet.z, vert_attr),
+        read_t4_float4(virtual_tet.w, vert_attr)
+    );
+    SplatTetrahedra result = {
+        tet, vert_rgbs
+    };
+    return result;
+}
diff --git a/delaunay_rasterization/internal/tile_shader_slang.py b/delaunay_rasterization/internal/tile_shader_slang.py
index c8bc904..f41923e 100644
--- a/delaunay_rasterization/internal/tile_shader_slang.py
+++ b/delaunay_rasterization/internal/tile_shader_slang.py
@@ -4,6 +4,7 @@ import math
 from delaunay_rasterization.internal.sort_by_keys import sort_by_keys_cub
 from icecream import ic
 from delaunay_rasterization.internal.util import recombine_tensors, split_tensors
+import time
 
 def augment(v):
     return torch.cat([v, torch.ones_like(v[:, :1])], dim=-1)
@@ -55,8 +56,10 @@ def vertex_and_tile_shader(indices,
         )
 
         highest_tile_id_msb = (render_grid.grid_width*render_grid.grid_height).bit_length()
+        # torch.cuda.synchronize()
         sorted_keys, sorted_tetra_idx = sort_by_keys_cub.sort_by_keys(
             unsorted_keys, unsorted_tetra_idx, highest_tile_id_msb)
+        # torch.cuda.synchronize()
 
         tile_ranges = torch.zeros((render_grid.grid_height*render_grid.grid_width, 2), 
                                   device="cuda",
@@ -102,14 +105,6 @@ class VertexShader(torch.autograd.Function):
                 gridSize=(ceil_div(n_tetra, 256), 1, 1)
         )
 
-        inds, = torch.where(vs_tetra[:, 1] == 1)
-        if len(inds) > 1:
-            j = vs_tetra[inds, 2].argmin()
-            mask = torch.tensor([ind for i, ind in enumerate(inds) if i != j], device=vs_tetra.device)
-            tiles_touched[mask] = 0
-            rect_tile_space[mask] = 0
-            vs_tetra[mask, 1] = 0
-
         tensors = [
             indices, vertices,
             tiles_touched, rect_tile_space, vs_tetra, circumcenter
diff --git a/evaluate.py b/evaluate.py
index 37e18bb..4021dcb 100644
--- a/evaluate.py
+++ b/evaluate.py
@@ -1,4 +1,4 @@
-from utils.train_util import render
+from delaunay_rasterization import render
 # from models.vertex_color import Model
 import pickle
 import torch
@@ -7,7 +7,7 @@ from pathlib import Path
 import imageio
 import numpy as np
 from data import loader
-from utils import test_util
+from utils import test_util, topo_utils
 from utils.args import Args
 from utils import cam_util
 import mediapy
@@ -18,10 +18,20 @@ args.tile_size = 16
 args.image_folder = "images_4"
 args.dataset_path = Path("/optane/nerf_datasets/360/bicycle")
 args.output_path = Path("output/test/")
-args.eval = True
+args.eval = False
 args.use_ply = False
 args.render_train = False
-args = Args.from_namespace(args.get_parser().parse_args())
+args.base_min_t = 0.2
+
+parser = args.get_parser()
+args = Args.from_namespace(parser.parse_args())
+
+# if a ckpt is loaded, load config, then override config with user specified flags
+if len(str(args.output_path)) > 0: 
+    config_path = Path(args.output_path) / "config.json"
+    config = Args.load_from_json(str(config_path))
+    parser.set_defaults(**config.as_dict())
+args = Args.from_namespace(parser.parse_args())
 
 device = torch.device('cuda')
 if args.use_ply:
@@ -29,29 +39,39 @@ if args.use_ply:
     model = Model.load_ply(args.output_path / "ckpt.ply", device)
 else:
     from models.ingp_color import Model
-    from models.frozen import FrozenTetModel
+    from models.frozen_features import FrozenTetModel
+    # from models.frozen import FrozenTetModel
     try:
         model = Model.load_ckpt(args.output_path, device)
     except:
         model = FrozenTetModel.load_ckpt(args.output_path, device)
 
+# from models.frozen import bake_from_model
+# model = bake_from_model(model, detach=True)
+vol = topo_utils.tet_volumes(model.vertices[model.indices])
+reverse_mask = (vol < 0)
+model.indices[reverse_mask] = model.indices[reverse_mask][:, [1, 0, 2, 3]]
+model.save2ply(Path('test.ply'))
+
 # model.light_offset = -1
 train_cameras, test_cameras, scene_info = loader.load_dataset(
-    args.dataset_path, args.image_folder, data_device="cuda", eval=args.eval)
+    args.dataset_path, args.image_folder, data_device="cpu", eval=args.eval)
+
+# model.min_t = args.min_t = args.base_min_t * model.scene_scaling.item()
+min_t = args.min_t = args.base_min_t# * model.scene_scaling.item()
 
-ic(model.min_t)
+ic(min_t)
 if args.render_train:
     splits = zip(['train', 'test'], [train_cameras, test_cameras])
 else:
     splits = zip(['test'], [test_cameras])
-test_util.evaluate_and_save(model, splits, args.output_path, args.tile_size, min_t=model.min_t)
-#model.save2ply(Path('test.ply'))
+test_util.evaluate_and_save(model, splits, args.output_path, args.tile_size, min_t=min_t)
 
 with torch.no_grad():
     epath = cam_util.generate_cam_path(train_cameras, 400)
     eimages = []
     for camera in tqdm(epath):
-        render_pkg = render(camera, model, tile_size=args.tile_size, tmin=model.min_t)
+        render_pkg = render(camera, model, tile_size=args.tile_size, min_t=min_t)
         image = render_pkg['render']
         image = image.permute(1, 2, 0)
         image = image.detach().cpu().numpy()
diff --git a/extract_mesh.py b/extract_mesh.py
index 02012fd..b4a7f85 100644
--- a/extract_mesh.py
+++ b/extract_mesh.py
@@ -1,37 +1,35 @@
-from utils.train_util import render
-import pickle
 import torch
-from tqdm import tqdm
 from pathlib import Path
-import imageio
-import numpy as np
 from data import loader
-from utils import test_util
 from utils.args import Args
-from utils import cam_util
-import mediapy
-from icecream import ic
+from models.ingp_color import Model, TetOptimizer
 
 args = Args()
 args.tile_size = 16
 args.image_folder = "images_4"
+args.dataset_path = Path("/optane/nerf_datasets/360/bicycle")
 args.output_path = Path("output/test/")
 args.eval = True
 args.use_ply = False
+args.contrib_threshold = 0.1
 args.density_threshold = 0.5
-args.alpha_threshold = 0.2
+args.alpha_threshold = 0.5
+args.freeze_features = True
 args = Args.from_namespace(args.get_parser().parse_args())
 
 device = torch.device('cuda')
-if args.use_ply:
-    # from models.tet_color import Model
-    from models.frozen import FrozenTetModel as Model
-    model = Model.load_ply(args.output_path / "ckpt.ply", device)
-else:
-    # from models.ingp_color import Model
-    from models.frozen import FrozenTetModel as Model
-    model = Model.load_ckpt(args.output_path, device)
-model.extract_mesh(args.output_path / "meshes", args.density_threshold, args.alpha_threshold)
+try:
+    model = Model.load_ckpt(args.output_path, device, args)
+except:
+    if args.freeze_features:
+        from models.frozen_features import FrozenTetModel, FrozenTetOptimizer
+    else:
+        from models.frozen import FrozenTetModel, FrozenTetOptimizer
+    model = FrozenTetModel.load_ckpt(args.output_path, device)
+
+train_cameras, test_cameras, scene_info = loader.load_dataset(
+    args.dataset_path, args.image_folder, data_device="cpu", eval=False)
+model.extract_mesh(train_cameras, args.output_path / "meshes", **args.as_dict())
 # eventually, generate a UV map for each mesh
 # organize onto a texture map
 # then, optimize the texture for each of these maps
diff --git a/hyperparam.py b/hyperparam.py
index a885d69..a112eec 100644
--- a/hyperparam.py
+++ b/hyperparam.py
@@ -9,6 +9,8 @@ from queue import Queue
 from concurrent.futures import ThreadPoolExecutor, as_completed
 from pathlib import Path
 
+blacklist = ['ckpt', 'output_path']
+
 def parse_args():
     parser = argparse.ArgumentParser(description="Run nerf_test.py experiments from a CSV queue.")
     parser.add_argument("--queue_csv", type=str, default="tests_queue.csv",
@@ -29,7 +31,7 @@ def generate_folder_name(test_params, args, base_dir="output"):
     parts = []
     # Iterate in the order provided by test_params (csv.DictReader preserves header order)
     for key, value in test_params.items():
-        if key == "output_path":
+        if key in blacklist or len(key.strip()) == 0:
             continue
         if key == "dataset_path":
             initials = key
@@ -62,7 +64,7 @@ def run_test(test_params, gpu_id, args):
     # Build command-line arguments. Override any CSV-specified output_path with our unique folder.
     cmd_args = []
     for arg, value in test_params.items():
-        if arg == "output_path":
+        if arg == "output_path" or len(arg.strip()) == 0:
             continue
         cmd_args.append(f"--{arg} {value}")
     cmd_args.append(f"--output_path {output_folder}")
@@ -74,7 +76,7 @@ def run_test(test_params, gpu_id, args):
     json_file = os.path.join(output_folder, "results.json")
     # Attempt to run the command. If it fails, still try to read the JSON output.
     try:
-        subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
+        subprocess.run(command, shell=True, check=True, text=True)
     except subprocess.CalledProcessError as e:
         print(f"Test on GPU {gpu_id} failed with error: {e}")
         print(f"STDOUT: {e.stdout}")
@@ -158,7 +160,20 @@ def main():
     tests = []
     try:
         with open(args.queue_csv, newline='') as f:
-            reader = csv.DictReader(f)
+            # Sniff the first line to detect the CSV dialect (including the separator).
+            try:
+                dialect = csv.Sniffer().sniff(f.readline())
+            except csv.Error:
+                # If sniffing fails (e.g., on an empty file), default to comma.
+                dialect = 'excel' # 'excel' is the standard comma-separated dialect.
+                print(f"Warning: Could not detect separator for {args.queue_csv}. Defaulting to comma.")
+
+            # Rewind the file to the beginning so the header is read correctly.
+            f.seek(0)
+            
+            # Use the detected dialect to create the reader.
+            reader = csv.DictReader(f, dialect=dialect)
+            
             for row in reader:
                 tests.append(row)
     except FileNotFoundError:
diff --git a/jaxutil/tetra_quad.py b/jaxutil/tetra_quad.py
index 3024a12..da5a079 100644
--- a/jaxutil/tetra_quad.py
+++ b/jaxutil/tetra_quad.py
@@ -189,10 +189,10 @@ def query_tetrahedra_kernel(t_samples, ray_origins, ray_directions,
         #             vertex_color[i, 2].reshape(*padding, 3) * coords[..., 2].reshape(-1, 1) + \
         #             vertex_color[i, 3].reshape(*padding, 3) * coords[..., 3].reshape(-1, 1)
 
-        tet_color = vertex_color[i, 0:3].reshape(*padding, 3) + \
-                    vertex_color[i, 4].reshape(*padding, 1) * coords[..., 0].reshape(-1, 1) + \
-                    vertex_color[i, 5].reshape(*padding, 1) * coords[..., 1].reshape(-1, 1) + \
-                    vertex_color[i, 6].reshape(*padding, 1) * coords[..., 2].reshape(-1, 1)
+        base_color = vertex_color[i, 0:3].reshape(1, 3)
+        raw_grd = vertex_color[i, 3:6].reshape(3, 1)
+        scale_factor = (jnp.dot(coords, raw_grd)).reshape(-1, 1)
+        tet_color = jnp.clip(base_color.reshape(1, 3) + scale_factor, 0, None)
         
         # Update density and color
         contrib_density = jnp.where(is_inside, tet_density, 0.0)
diff --git a/models/base_linear_model.py b/models/base_linear_model.py
deleted file mode 100644
index 48f707a..0000000
--- a/models/base_linear_model.py
+++ /dev/null
@@ -1,6 +0,0 @@
-import torch
-from torch import nn
-from models.base_model import BaseModel
-
-class BaseModel(nn.Module):
-
diff --git a/models/base_model.py b/models/base_model.py
index 767b114..a2c8cb2 100644
--- a/models/base_model.py
+++ b/models/base_model.py
@@ -1,71 +1,23 @@
 import torch
 from torch import nn
-from typing import Optional, Tuple
-import gc
 import tinyplypy
 import numpy as np
 from pathlib import Path
 
-from data.camera import Camera
-from utils.topo_utils import (
-    build_tv_struct, max_density_contrast
-)
-from utils.model_util import activate_output
-from utils import optim
 from utils.model_util import *
-from utils.safe_math import safe_log, safe_exp
-from utils.train_util import get_expon_lr_func, SpikingLR
 from utils import mesh_util
-from utils.args import Args
-
+from delaunay_rasterization.internal.render_err import render_err
+from sh_slang.eval_sh import eval_sh
+from utils import topo_utils
 
 class BaseModel(nn.Module):
 
-    def get_cell_values(
-        self,
-        camera: Camera,
-        mask: Optional[torch.Tensor] = None,
-        all_circumcenters: Optional[torch.Tensor] = None,
-        radii: Optional[torch.Tensor] = None,
-    ):
-        indices = self.indices[mask] if mask is not None else self.indices
-        vertices = self.vertices
-
-        if self.chunk_size is None:
-            cc, normalized, density, rgb, grd, sh = self.compute_batch_features(
-                vertices, indices, start, end, circumcenters=all_circumcenters
-            )
-            cell_output = activate_output(
-                camera.camera_center.to(self.device),
-                density, rgb, grd, sh, indices,
-                cc, vertices,
-                self.max_sh_deg, self.max_sh_deg,
-            )
-            return normalized, cell_output
-        else:
-            outputs = []
-            normed_cc = []
-            start = 0
-            for start in range(0, indices.shape[0], self.chunk_size):
-                end = min(start + self.chunk_size, indices.shape[0])
-                circumcenters, normalized, density, rgb, grd, sh = self.compute_batch_features(
-                    vertices, indices, start, end, circumcenters=all_circumcenters)
-                dvrgbs = activate_output(camera.camera_center.to(self.device),
-                                         density, rgb, grd, sh, indices[start:end],
-                                         circumcenters,
-                                         vertices, self.current_sh_deg, self.max_sh_deg)
-                normed_cc.append(normalized)
-                outputs.append(dvrgbs)
-            features = torch.cat(outputs, dim=0)
-            normed_cc = torch.cat(normed_cc, dim=0)
-            return normed_cc, features
-
     def __len__(self):
         return self.vertices.shape[0]
 
     @property
     def num_int_verts(self):
-        return self.contracted_vertices.shape[0]
+        return self.interior_vertices.shape[0]
 
     def get_circumcenters(self):
         circumcenter =  pre_calc_cell_values(
@@ -91,9 +43,6 @@ class BaseModel(nn.Module):
         return vertex_density
 
     def calc_tet_alpha(self, mode="min", density=None):
-        alpha_list = []
-        start = 0
-        
         verts = self.vertices
         inds = self.indices
         v0, v1, v2, v3 = verts[inds[:, 0]], verts[inds[:, 1]], verts[inds[:, 2]], verts[inds[:, 3]]
@@ -103,7 +52,9 @@ class BaseModel(nn.Module):
             torch.norm(v1 - v2, dim=1), torch.norm(v1 - v3, dim=1), torch.norm(v2 - v3, dim=1)
         ], dim=0)
         if mode == "min":
-            el = edge_lengths.min(dim=0)[0]
+            el = edge_lengths.min(dim=0)[0] / 4
+        elif mode == "second":
+            el = edge_lengths.sort(dim=0).values[1, :]
         elif mode == "max":
             el = edge_lengths.max(dim=0)[0]
         elif mode == "mean":
@@ -119,7 +70,8 @@ class BaseModel(nn.Module):
     def save2ply(self, path):
         path.parent.mkdir(exist_ok=True, parents=True)
 
-        xyz = self.vertices.detach().cpu().numpy().astype(np.float32)  # shape (num_vertices, 3)
+        vertices = self.vertices
+        xyz = vertices.detach().cpu().numpy().astype(np.float32)  # shape (num_vertices, 3)
 
         vertex_dict = {
             "x": xyz[:, 0],
@@ -130,9 +82,8 @@ class BaseModel(nn.Module):
         N = self.indices.shape[0]
         sh_dim = ((self.max_sh_deg+1)**2-1)
 
-        vertices = self.vertices
         indices = self.indices
-        circumcenters, density, base_color_v0_raw, normed_grd, sh = self.compute_features(offset=True)
+        circumcenters, density, base_color_v0_raw, normed_grd, sh = self.compute_features()
 
         base_color_v0_raw = base_color_v0_raw.cpu().numpy().astype(np.float32)
         grds = normed_grd.reshape(-1, 3).cpu().numpy().astype(np.float32)
@@ -140,7 +91,7 @@ class BaseModel(nn.Module):
         sh_coeffs = sh.reshape(-1, sh_dim, 3).cpu().numpy().astype(np.float32)
 
         tetra_dict = {}
-        tetra_dict["vertex_indices"] = self.indices.cpu().numpy().astype(np.int32)
+        tetra_dict["vertex_indices"] = indices.cpu().numpy().astype(np.int32)
         tetra_dict["s"] = np.ascontiguousarray(densities)
         for i, co in enumerate(["x", "y", "z"]):
             tetra_dict[f"grd_{co}"]         = np.ascontiguousarray(grds[:, i])
@@ -163,32 +114,99 @@ class BaseModel(nn.Module):
         tinyplypy.write_ply(str(path), data_dict, is_binary=True)
 
     @torch.no_grad
-    def extract_mesh(self, path, density_threshold=0.5, alpha_threshold=0.2):
+    def extract_mesh(self, cameras, path, tile_size=16, contrib_threshold=0.1, density_threshold=0.5, alpha_threshold=0.2, **kwargs):
         path.mkdir(exist_ok=True, parents=True)
+        n_tets = self.indices.shape[0]
+        top1 = torch.zeros(n_tets, device=self.device)  # highest seen so far
+        top2 = torch.zeros(n_tets, device=self.device)  # second-highest seen so far
+
+        for cam in cameras:
+            target = cam.original_image.cuda()
+            camera_center = cam.camera_center.to(self.device)
+
+            image_votes, extras = render_err(
+                target, cam, self,
+                scene_scaling=self.scene_scaling,
+                tile_size=tile_size,
+                lambda_ssim=0
+            )
+
+            tc = extras["tet_count"][..., 0]
+            max_T = extras["tet_count"][..., 1].float() / 65535
+            
+            # --- Create a single mask for valid updates ---
+            # Mask for tets that have a reasonable number of samples in the current view
+            # --- Moments (s-1: sum of T, s1: sum of err, s2: sum of err^2)
+            image_T, image_err, image_err1 = image_votes[:, 0], image_votes[:, 1], image_votes[:, 2]
+            # total_T_p, image_err, image_err1 = image_votes[:, 3], image_votes[:, 4], image_votes[:, 5]
+            _, image_Terr, image_ssim = image_votes[:, 2], image_votes[:, 4], image_votes[:, 5]
+            N = tc
+
+            # contrib = (image_T / N.clip(min=1)).reshape(-1)
+            # contrib = (image_T / N.clip(min=1)).reshape(-1)
+            contrib = max_T
+            prev_top1 = top1
+            top1 = torch.maximum(prev_top1, contrib)
+            top2 = torch.maximum(top2, torch.minimum(prev_top1, contrib))
+
+        inds = self.indices
         verts = self.vertices
-        tet_density = self.calc_tet_density()
-        tet_alpha = self.calc_tet_alpha(mode="min")
-        mask = (tet_density > density_threshold) | (tet_alpha > alpha_threshold)
+        tets = verts[inds]
+        circumcenters, tet_density, rgb, grd, sh = self.compute_features()
+        # tet_alpha = self.calc_tet_alpha(mode="min")
+        # tet_alpha = self.calc_tet_alpha(mode="min") * max_color.clip(min=1)
+
+        v0, v1, v2, v3 = verts[inds[:, 0]], verts[inds[:, 1]], verts[inds[:, 2]], verts[inds[:, 3]]
+        edge_lengths = torch.stack([
+            torch.norm(v0 - v1, dim=1), torch.norm(v0 - v2, dim=1), torch.norm(v0 - v3, dim=1),
+            torch.norm(v1 - v2, dim=1), torch.norm(v1 - v3, dim=1), torch.norm(v2 - v3, dim=1)
+        ], dim=0)
+        aspect = edge_lengths.min(dim=0).values / edge_lengths.max(dim=0).values 
+        vol = topo_utils.tet_volumes(tets).abs()
+        tet_alpha = 1 - torch.exp(-tet_density.reshape(-1) * vol.reshape(-1)**(1/3) * aspect)
+        # mask = (peak_contrib > contrib_threshold)# | (tet_density > density_threshold)
+        # mask = (peak_contrib > contrib_threshold) | (tet_alpha > alpha_threshold) | (tet_density > density_threshold)
+
+        # mask = (peak_contrib > contrib_threshold) | (tet_alpha > alpha_threshold)
+        mask = (top1.reshape(-1) > contrib_threshold) | (tet_density.reshape(-1) > density_threshold)
+        ic(top1.mean(), tet_density.mean(), mask.sum())
+        # mask = (tet_density > density_threshold) | (tet_alpha > alpha_threshold)
 
-        circumcenters, density, rgb, grd, sh = self.compute_features(offset=False)
         rgb = rgb[mask].detach()
-        tets = verts[self.indices[mask]]
+        tets = tets[mask]
         circumcenters, radius = calculate_circumcenters_torch(tets.double())
         grd = grd[mask].detach()
-        grd = grd.reshape(-1, 1, 3) * rgb.reshape(-1, 3, 1).mean(dim=1, keepdim=True).detach()
-        normed_grd = safe_div(grd, radius.reshape(-1, 1, 1))
+        sh = sh[mask].detach()
+        normed_grd = grd.reshape(-1, 1, 3) * rgb.reshape(-1, 3, 1).mean(dim=1, keepdim=True).detach()
+        tet_color_raw = eval_sh(
+            tets.mean(dim=1).detach(),
+            RGB2SH(rgb),
+            sh.reshape(-1, (self.max_sh_deg+1)**2 - 1, 3),
+            camera_center,
+            self.max_sh_deg).float()
+        tet_color = torch.nn.functional.softplus(tet_color_raw.reshape(-1, 3, 1), beta=10)
         vcolors = compute_vertex_colors_from_field(
-            tets.detach(), rgb.reshape(-1, 3), normed_grd.float(), circumcenters.float().detach())
-        vcolors = torch.nn.functional.softplus(vcolors, beta=10)
-
+            tets.detach(), tet_color.reshape(-1, 3), normed_grd.float(), circumcenters.float().detach()).clip(min=0)
+        # vcolors = torch.nn.functional.softplus(vcolors, beta=10)
+
+        # mesh_util.export_textured_meshes(path, 
+        #     verts=verts.detach().cpu().numpy(),
+        #     tets=self.indices[mask].cpu().numpy(),
+        #     tet_v_rgb=vcolors.detach().cpu().numpy(),
+        #     min_faces_for_export=10000,
+        #     texture_size=4096*2**0,
+        # )
+
+        # meshes = mesh_util.extract_meshes_per_face_color(
         meshes = mesh_util.extract_meshes(
             vcolors.detach().cpu().numpy(),
             verts.detach().cpu().numpy(),
             self.indices[mask].cpu().numpy())
         for i, mesh in enumerate(meshes):
             F = mesh['face']['vertex_indices'].shape[0]
-            if F > 1000:
+            if F > 1:
                 mpath = path / f"{i}.ply"
                 print(f"Saving #F:{F} to {mpath}")
                 tinyplypy.write_ply(str(mpath), mesh, is_binary=False)
 
+
diff --git a/models/frozen.py b/models/frozen.py
index 70e961c..21426b6 100644
--- a/models/frozen.py
+++ b/models/frozen.py
@@ -18,6 +18,7 @@ from utils.train_util import get_expon_lr_func, SpikingLR
 from utils import mesh_util
 from utils.args import Args
 from models.base_model import BaseModel
+from sh_slang.eval_sh import eval_sh
 
 
 class FrozenTetModel(BaseModel):
@@ -36,7 +37,6 @@ class FrozenTetModel(BaseModel):
     def __init__(
         self,
         int_vertices: torch.Tensor,          # (N_int, 3)
-        ext_vertices: torch.Tensor,          # (N_ext, 3)
         indices: torch.Tensor,               # (T, 4)
         density: torch.Tensor,               # (T, 1)
         rgb: torch.Tensor,                   # (T, 3)
@@ -45,7 +45,6 @@ class FrozenTetModel(BaseModel):
         center: torch.Tensor,                # (1, 3)
         scene_scaling: torch.Tensor | float,
         *,
-        density_offset: float = -1.0,
         max_sh_deg: int = 2,
         chunk_size: int = 408_576,
         **kwargs
@@ -54,7 +53,6 @@ class FrozenTetModel(BaseModel):
 
         # geometry ----------------------------------------------------------------
         self.register_buffer("interior_vertices", int_vertices)          # immutable
-        self.register_buffer("ext_vertices", ext_vertices)
         self.register_buffer("indices", indices.int())
         self.register_buffer("center", center.reshape(1, 3))
         self.register_buffer("scene_scaling", torch.as_tensor(scene_scaling))
@@ -62,13 +60,12 @@ class FrozenTetModel(BaseModel):
         # learnable pertet parameters -------------------------------------------
         # self.density   = nn.Parameter(safe_log(density))    # (T, 1)
         # self.gradient  = nn.Parameter(torch.atanh(gradient.clip(min=-0.99, max=0.99)))   # (T, 3, 3)
-        self.density   = nn.Parameter(density)    # (T, 1)
-        self.gradient  = nn.Parameter(gradient)   # (T, 3, 3)
-        self.rgb       = nn.Parameter(rgb)        # (T, 3)
-        self.sh        = nn.Parameter(sh.half())         # (T, SH, 3)
+        self.density   = nn.Parameter(density, requires_grad=True)    # (T, 1)
+        self.gradient  = nn.Parameter(gradient, requires_grad=True)   # (T, 3, 3)
+        self.rgb       = nn.Parameter(rgb, requires_grad=True)        # (T, 3)
+        self.sh        = nn.Parameter(sh, requires_grad=True)         # (T, SH, 3)
 
         # misc --------------------------------------------------------------------
-        self.density_offset  = density_offset
         self.max_sh_deg      = max_sh_deg
         self.chunk_size      = chunk_size
         self.device          = self.density.device
@@ -101,7 +98,6 @@ class FrozenTetModel(BaseModel):
         
         # Extract required parameters from checkpoint
         int_vertices = ckpt['interior_vertices']
-        ext_vertices = ckpt['ext_vertices']
         indices = ckpt['indices']
         density = ckpt['density']
         rgb = ckpt['rgb']
@@ -115,7 +111,6 @@ class FrozenTetModel(BaseModel):
         # Create model instance
         model = FrozenTetModel(
             int_vertices=int_vertices.to(device),
-            ext_vertices=ext_vertices.to(device),
             indices=indices.to(device),
             density=density.to(device),
             rgb=rgb.to(device),
@@ -123,7 +118,6 @@ class FrozenTetModel(BaseModel):
             sh=sh.to(device),
             center=center.to(device),
             scene_scaling=scene_scaling.to(device),
-            density_offset=config.density_offset,
             max_sh_deg=config.max_sh_deg,
             chunk_size=config.chunk_size if hasattr(config, 'chunk_size') else 408_576,
         )
@@ -134,78 +128,45 @@ class FrozenTetModel(BaseModel):
         
         return model
 
-    # ------------------------------------------------------------------
-    # convenience properties
-    # ------------------------------------------------------------------
     @property
     def vertices(self) -> torch.Tensor:
         """Concatenated vertex tensor (internal + exterior)."""
-        return torch.cat([self.interior_vertices, self.ext_vertices], dim=0)
-
-
-    # ------------------------------------------------------------------
-    # core helper (networkfree)
-    # ------------------------------------------------------------------
-    def compute_batch_features(
-        self,
-        vertices: torch.Tensor,
-        indices: torch.Tensor,
-        mask: Optional[torch.Tensor] = None,
-        circumcenters: Optional[torch.Tensor] = None,
-    ):
-        if circumcenters is None:
-            circumcenter = pre_calc_cell_values(
-                vertices, indices
-            )
-        else:
-            circumcenter = circumcenters
-        normalized = (circumcenter - self.center) / self.scene_scaling
-
-        if mask is not None:
-            density  = self.density[mask]
-            grd      = self.gradient[mask]
-            rgb      = self.rgb[mask]
-            sh       = self.sh[mask]
-        else:
-            density  = self.density
-            grd      = self.gradient
-            rgb      = self.rgb
-            sh       = self.sh
+        return self.interior_vertices
 
-        return circumcenter, normalized, density, rgb, grd, sh
 
-    def compute_features(self, offset=False):
+    def compute_features(self):
         vertices = self.vertices
         indices = self.indices
-        circumcenters, _, density, rgb, grd, sh = self.compute_batch_features(vertices, indices)
         tets = vertices[indices]
-        if offset:
-            base_color_v0_raw, normed_grd = offset_normalize(rgb, grd, circumcenters, tets)
-            return circumcenters, density, base_color_v0_raw, normed_grd, sh
-        else:
-            return circumcenters, density, rgb, grd, sh
+        circumcenters, radius = calculate_circumcenters_torch(tets.double())
+        return circumcenters, self.density, self.rgb, safe_div(self.gradient, radius.reshape(-1, 1, 1)), self.sh
 
-    # ------------------------------------------------------------------
-    # public renderer interface
-    # ------------------------------------------------------------------
-    def get_cell_values(
-        self,
-        camera: Camera,
-        mask: Optional[torch.Tensor] = None,
-        all_circumcenters: Optional[torch.Tensor] = None,
-        radii: Optional[torch.Tensor] = None,
-    ):
+    def get_cell_values(self, camera: Camera, mask=None,
+                        all_circumcenters=None, glo=None):
+        cam_center = camera.camera_center.to(self.device)
         indices = self.indices[mask] if mask is not None else self.indices
         vertices = self.vertices
-        cc, normalized, density, rgb, grd, sh = self.compute_batch_features(
-            vertices, indices, mask, circumcenters=all_circumcenters
-        )
-        cell_output = activate_output(
-            camera.camera_center.to(self.device),
-            density, rgb, grd, sh, indices,
-            cc, vertices,
-            self.max_sh_deg, self.max_sh_deg,
-        )
+        tets = vertices[indices]
+        if all_circumcenters is None:
+            all_circumcenters, radius = calculate_circumcenters_torch(tets.double())
+
+        rgb = self.rgb if mask is None else self.rgb[mask]
+        density = self.density if mask is None else self.density[mask]
+        grd = self.gradient if mask is None else self.gradient[mask]
+        sh = self.sh if mask is None else self.sh[mask]
+
+        tet_color_raw = eval_sh(
+            tets.mean(dim=1).detach(),
+            RGB2SH(rgb),
+            sh.reshape(-1, (self.max_sh_deg+1)**2 - 1, 3),
+            cam_center,
+            self.max_sh_deg).float()
+        cell_output = activate_output(cam_center, tet_color_raw,
+            density, grd,
+            all_circumcenters,
+            tets)
+
+        normalized = (all_circumcenters - self.center) / self.scene_scaling
         return normalized, cell_output
 
     # ------------------------------------------------------------------
@@ -218,69 +179,15 @@ class FrozenTetModel(BaseModel):
     def num_int_verts(self):
         return self.interior_vertices.shape[0]
 
+    def sh_up(self):
+        pass
+
     def calc_tet_density(self):
         densities = []
         verts = self.vertices
         _, _, densities, _, _, _ = self.compute_batch_features(verts, self.indices)
         return densities.reshape(-1)
 
-
-# =============================================================================
-# 2.  BAKING UTILITY                                                         |
-# =============================================================================
-
-@torch.no_grad()
-def bake_from_model(base_model, *, detach: bool = True, chunk_size: int = 408_576) -> FrozenTetModel:
-    """Convert an existing neuralfield `Model` into a parameteronly
-    `FrozenTetModel`.  All pertet features are *evaluated once* through the
-    network and stored explicitly so that no backbone is needed afterwards."""
-    device = base_model.device
-
-    vertices_full = base_model.vertices.detach() if detach else base_model.vertices
-    int_vertices  = vertices_full[: base_model.num_int_verts]
-    ext_vertices  = base_model.ext_vertices.detach() if detach else base_model.ext_vertices
-    indices       = base_model.indices.detach() if detach else base_model.indices
-
-    d_list, rgb_list, grd_list, sh_list = [], [], [], []
-    for start in range(0, indices.shape[0], chunk_size):
-        end = min(start + chunk_size, indices.shape[0])
-        _, _, density, rgb, grd, sh = base_model.compute_batch_features(
-            vertices_full, indices, start, end
-        )
-        d_list.append(density)
-        rgb_list.append(rgb)
-        grd_list.append(grd)
-        sh_list.append(sh)
-
-    density  = torch.cat(d_list, 0)
-    rgb      = torch.cat(rgb_list, 0)
-    gradient = torch.cat(grd_list, 0)
-    sh       = torch.cat(sh_list, 0)
-
-    if detach:
-        density, rgb, gradient, sh = (x.clone().detach() for x in (density, rgb, gradient, sh))
-
-    return FrozenTetModel(
-        int_vertices=int_vertices.to(device),
-        ext_vertices=ext_vertices.to(device),
-        indices=indices.to(device),
-        density=density.to(device),
-        rgb=rgb.to(device),
-        gradient=gradient.to(device),
-        sh=sh.to(device),
-        center=base_model.center.detach().to(device),
-        scene_scaling=base_model.scene_scaling.detach().to(device),
-        density_offset=base_model.density_offset,
-        current_sh_deg=base_model.current_sh_deg,
-        max_sh_deg=base_model.max_sh_deg,
-        chunk_size=chunk_size,
-    )
-
-
-# =============================================================================
-# 3.  OPTIMISER FOR FROZEN MODEL                                             |
-# =============================================================================
-
 class FrozenTetOptimizer:
     """Lightweight optimiser tailored to `FrozenTetModel`.
 
@@ -312,15 +219,16 @@ class FrozenTetOptimizer:
         # ------------------------------------------------------------------
         # single optimiser with four parameter groups
         # ------------------------------------------------------------------
-        self.optim = torch.optim.RMSprop([
-        # self.optim = torch.optim.Adam([
+        # self.optim = torch.optim.RMSprop([
+        self.optim = torch.optim.Adam([
             {"params": [model.density],  "lr": freeze_lr,  "name": "density"},
             {"params": [model.rgb],      "lr": freeze_lr,    "name": "color"},
             {"params": [model.gradient], "lr": freeze_lr, "name": "gradient"},
         ])
-        self.sh_optim = torch.optim.RMSprop([
-            {"params": [model.sh],       "lr": freeze_lr,       "name": "sh"},
-        ], eps=1e-4)
+        # self.sh_optim = torch.optim.RMSprop([
+        self.sh_optim = torch.optim.Adam([
+            {"params": [model.sh],       "lr": freeze_lr / 20,       "name": "sh"},
+        ])#, eps=1e-6)
         self.freeze_start = freeze_start
         self.scheduler = get_expon_lr_func(lr_init=freeze_lr,
                                            lr_final=final_freeze_lr,
@@ -332,22 +240,15 @@ class FrozenTetOptimizer:
         self.net_optim   = self.optim
         self.vertex_optim = None  # geometry is frozen
 
-        # TV structure (pairs & face areas) for regulariser ----------------
-        if self.lambda_tv > 0:
-            self.pairs, self.face_area = build_tv_struct(
-                self.model.vertices.detach(), self.model.indices, device=model.device
-            )
+    def update_triangulation(self, *_, **__):
+        return None
 
-    # ------------------------------------------------------------------
-    # public helpers ----------------------------------------------------
-    # ------------------------------------------------------------------
     def step(self):
         self.optim.step()
 
     def zero_grad(self):
         self.optim.zero_grad()
 
-    # compatibility shims ------------------------------------------------
     def main_step(self):
         self.step()
 
@@ -365,20 +266,7 @@ class FrozenTetOptimizer:
     def split(self, clone_indices, split_point, split_mode):
         print("Split called on frozen optimizer")
 
-    # ------------------------------------------------------------------
-    # regularisers ------------------------------------------------------
-    # ------------------------------------------------------------------
     def regularizer(self, *_):
-        # wd_loss = self.weight_decay * sum((p ** 2).mean() for p in [
-        #     self.model.density, self.model.rgb, self.model.gradient, self.model.sh
-        # ])
-
-        if self.lambda_density > 0:
-            density = self.model.density.squeeze(-1)
-            density_loss = density.mean()
-        else:
-            density_loss = 0.0
-
         if self.lambda_tv > 0:
             # simple TV on densities as example
             diff = (self.model.density[self.pairs[:, 0]] - self.model.density[self.pairs[:, 1]]).abs()
@@ -386,7 +274,49 @@ class FrozenTetOptimizer:
         else:
             tv_loss = 0.0
 
-        return self.lambda_density * density_loss + self.lambda_tv * tv_loss
+        return self.lambda_tv * tv_loss
+
+def bake_from_model(base_model, mask, chunk_size: int = 408_576) -> FrozenTetModel:
+    """Convert an existing neuralfield `Model` into a parameteronly
+    `FrozenTetModel`.  All pertet features are *evaluated once* through the
+    network and stored explicitly so that no backbone is needed afterwards."""
+    device = base_model.device
+
+    vertices_full = base_model.vertices.detach()
+    int_vertices  = vertices_full[: base_model.num_int_verts]
+    indices       = base_model.indices[mask].detach()
+
+    d_list, rgb_list, grd_list, sh_list = [], [], [], []
+    for start in range(0, indices.shape[0], chunk_size):
+        end = min(start + chunk_size, indices.shape[0])
+        _, _, density, rgb, grd, sh = base_model.compute_batch_features(
+            vertices_full, indices, start, end
+        )
+        d_list.append(density)
+        rgb_list.append(rgb)
+        grd_list.append(grd)
+        sh_list.append(sh)
+
+    density  = torch.cat(d_list, 0)
+    rgb      = torch.cat(rgb_list, 0)
+    gradient = torch.cat(grd_list, 0)
+    sh       = torch.cat(sh_list, 0)
+
+    density, rgb, gradient, sh = (x.clone().detach() for x in (density, rgb, gradient, sh))
+
+    return FrozenTetModel(
+        int_vertices=int_vertices.to(device),
+        indices=indices.to(device),
+        density=density.to(device),
+        rgb=rgb.to(device),
+        gradient=gradient.to(device),
+        sh=sh.to(device),
+        center=base_model.center.detach().to(device),
+        scene_scaling=base_model.scene_scaling.detach().to(device),
+        max_sh_deg=base_model.max_sh_deg,
+        chunk_size=chunk_size,
+    )
+
 
 def _offload_model_to_cpu(model: nn.Module):
     """Move every parameter & buffer to CPU and drop gradients to free GPU VRAM."""
@@ -399,14 +329,10 @@ def _offload_model_to_cpu(model: nn.Module):
         b.data = b.data.cpu()
     torch.cuda.empty_cache()
 
-@torch.no_grad()
 def freeze_model(
     base_model,
-    *,
-    weight_decay: float = 1e-10,
-    lambda_tv:    float = 0.0,
-    lambda_density: float = 0.0,
-    detach: bool = True,
+    mask,
+    args,
     chunk_size: int = 408_576,
     **kwargs
 ) -> Tuple[FrozenTetModel, FrozenTetOptimizer]:
@@ -422,14 +348,11 @@ def freeze_model(
         Optimiser bound to the frozen model.
     """
     print("Freezing model")
-    frozen_model = bake_from_model(base_model, detach=detach, chunk_size=chunk_size)
+    frozen_model = bake_from_model(base_model, mask, chunk_size=chunk_size)
 
     frozen_optim = FrozenTetOptimizer(
         frozen_model,
-        weight_decay=weight_decay,
-        lambda_tv=lambda_tv,
-        lambda_density=lambda_density,
-        **kwargs
+        **args.as_dict()
     )
 
     # free GPU memory used by the big backbone (optional but handy)
diff --git a/models/frozen_features.py b/models/frozen_features.py
new file mode 100644
index 0000000..acc91f4
--- /dev/null
+++ b/models/frozen_features.py
@@ -0,0 +1,403 @@
+import torch
+from utils.contraction import contract_mean_std
+from utils.contraction import contract_points, inv_contract_points
+from models.base_model import BaseModel
+from muon import SingleDeviceMuonWithAuxAdam
+import math
+from data.camera import Camera
+from torch import nn
+from typing import Optional, Tuple
+from torch.utils.checkpoint import checkpoint
+import gc
+
+from utils.topo_utils import calculate_circumcenters_torch
+from utils.contraction import contract_mean_std
+from utils.contraction import contract_points, inv_contract_points
+
+from utils.train_util import get_expon_lr_func
+from pathlib import Path
+import numpy as np
+from utils.args import Args
+from utils.model_util import *
+from utils import safe_math
+from sh_slang.eval_sh import eval_sh
+
+class FrozenTetModel(BaseModel):
+    def __init__(self,
+                 int_vertices: torch.Tensor,
+                 indices: torch.Tensor,
+                 center: torch.Tensor,
+                 features: torch.Tensor,
+                 scene_scaling: float,
+                 max_sh_deg=2,
+                 glo_dim=0,
+                 **kwargs):
+        super().__init__()
+        self.device = int_vertices.device
+        self.max_sh_deg = max_sh_deg
+        self.dir_offset = torch.tensor([
+            [0, 0],
+            [math.pi, 0],
+        ], device=self.device)
+        sh_dim = ((1+max_sh_deg)**2-1)*3
+        self.backbone = torch.compile(Heads(
+            features.shape[1],
+            sh_dim,
+            glo_dim=glo_dim,
+            **kwargs)).to(self.device)
+        self.features = nn.Parameter(features, requires_grad=True)
+        self.default_glo = None if glo_dim == 0 else torch.zeros((1, glo_dim), device=self.device)
+        self.chunk_size = 408576
+        self.mask_values = True
+        self.frozen = True
+        self.alpha = 0
+        self.linear = False
+        self.feature_dim = 7
+
+        self.register_buffer("interior_vertices", int_vertices)          # immutable
+        self.register_buffer('indices', indices.to(self.device))
+        self.register_buffer('center', center.reshape(1, 3))
+        self.register_buffer('scene_scaling', torch.tensor(float(scene_scaling), device=self.device))
+        self.update_triangulation()
+
+    def get_circumcenters(self):
+        circumcenter =  pre_calc_cell_values(
+            self.vertices, self.indices, self.center, self.scene_scaling)
+        return circumcenter
+
+    def get_cell_values(self, camera: Camera, mask=None,
+                        all_circumcenters=None, glo=None):
+        indices = self.indices[mask] if mask is not None else self.indices
+        vertices = self.vertices
+        glo = glo if glo is not None else self.default_glo
+
+        outputs = []
+        normed_cc = []
+        start = 0
+        features = self.features[mask] if mask is not None else self.features
+        cam_center = camera.camera_center.to(self.device)
+        for start in range(0, indices.shape[0], self.chunk_size):
+            end = min(start + self.chunk_size, indices.shape[0])
+            circumcenters, normalized, density, rgb, grd, sh = self.compute_batch_features(
+                vertices, indices, start, end, features=features, circumcenters=all_circumcenters, glo=glo)
+            tets = vertices[indices[start:end]]
+            tet_color_raw = eval_sh(
+                tets.mean(dim=1).detach(),
+                RGB2SH(rgb),
+                sh.reshape(-1, (self.max_sh_deg+1)**2 - 1, 3),
+                cam_center,
+                self.max_sh_deg).float()
+            dvrgbs = activate_output(cam_center, tet_color_raw,
+                                     density, grd,
+                                     circumcenters,
+                                     tets)
+            normed_cc.append(normalized)
+            outputs.append(dvrgbs)
+        features = torch.cat(outputs, dim=0)
+        normed_cc = torch.cat(normed_cc, dim=0)
+        return normed_cc, features
+
+    def compute_batch_features(self, vertices, indices, start, end, features=None, circumcenters=None, glo=None):
+        if circumcenters is None:
+            tets = vertices[indices[start:end]]
+            circumcenter, radius = calculate_circumcenters_torch(tets.double())
+        else:
+            circumcenter = circumcenters[start:end]
+        if self.training:
+            circumcenter += self.alpha*torch.rand_like(circumcenter)
+        normalized = (circumcenter - self.center) / self.scene_scaling
+
+        glo = glo if glo is not None else self.default_glo
+
+        if features is not None:
+            output = checkpoint(self.backbone, features[start:end], glo, use_reentrant=True)
+        else:
+            output = checkpoint(self.backbone, self.features[start:end], glo, use_reentrant=True)
+        return circumcenter, normalized, *output
+
+    def compute_features(self):
+        vertices = self.vertices
+        indices = self.indices
+        features = self.features
+        cs, ds, rs, gs, ss = [], [], [], [], []
+        for start in range(0, indices.shape[0], self.chunk_size):
+            end = min(start + self.chunk_size, indices.shape[0])
+
+            circumcenters, _, density, rgb, grd, sh = self.compute_batch_features(vertices, indices, start, end, features=features, glo=self.default_glo)
+            tets = vertices[indices[start:end]]
+            radius = torch.linalg.norm(tets[:, :1] - circumcenters[:, None, :], dim=-1, keepdim=True)
+            cs.append(circumcenters)
+            ds.append(density)
+            ss.append(sh)
+            rs.append(rgb)
+            gs.append(safe_math.safe_div(grd, radius))
+        cs = torch.cat(cs, dim=0)
+        ds = torch.cat(ds, dim=0)
+        rs = torch.cat(rs, dim=0)
+        gs = torch.cat(gs, dim=0)
+        ss = torch.cat(ss, dim=0)
+        return cs, ds, rs, gs, ss
+
+
+    @staticmethod
+    def load_ckpt(path: Path, device, **overrides):
+        ckpt_path = path / "ckpt.pth"
+        config_path = path / "config.json"
+        config = Args.load_from_json(str(config_path))
+        for k, v in overrides.items():
+            config[k] = v
+        ckpt = torch.load(ckpt_path)
+        vertices = ckpt['interior_vertices']
+        indices = ckpt["indices"]  # shape (N,4)
+        print(f"Loaded {vertices.shape[0]} vertices")
+        model = FrozenTetModel(
+            int_vertices=vertices.to(device),
+            features=ckpt['features'],
+            indices=indices,
+            center=ckpt['center'],
+            scene_scaling=ckpt['scene_scaling'],
+            **config.as_dict())
+        model.load_state_dict(ckpt)
+        # model.min_t = model.scene_scaling * config.base_min_t
+        model.min_t = config.base_min_t
+        model.indices = torch.as_tensor(indices).cuda()
+        return model
+
+    def calc_tet_density(self):
+        densities = []
+        verts = self.vertices
+        for start in range(0, self.indices.shape[0], self.chunk_size):
+            end = min(start + self.chunk_size, self.indices.shape[0])
+            
+            _, _, density, _, _, _ = self.compute_batch_features(verts, self.indices, start, end, glo=self.default_glo)
+
+            densities.append(density.reshape(-1))
+        return torch.cat(densities)
+
+    def inv_contract(self, points):
+        return inv_contract_points(points) * self.scene_scaling + self.center
+
+    def contract(self, points):
+        return contract_points((points - self.center) / self.scene_scaling)
+
+    @property
+    def vertices(self):
+        verts = self.interior_vertices
+        return verts
+
+    def sh_up(self):
+        pass
+
+    @torch.no_grad()
+    def update_triangulation(self, high_precision=False, density_threshold=0.0, alpha_threshold=0.0):
+        pass
+
+    def __len__(self):
+        return self.vertices.shape[0]
+
+
+class FrozenTetOptimizer:
+    def __init__(self,
+                 model: FrozenTetModel,
+                 feature_lr: float=1e-3,
+                 final_feature_lr: float=1e-4,
+                 fnetwork_lr: float=1e-3,
+                 final_fnetwork_lr: float=1e-3,
+
+                 weight_decay=1e-10,
+                 lr_delay: int = 500,
+                 lambda_tv: float = 0.0,
+                 lambda_density: float = 0.0,
+                 sh_weight_decay: float = 1e-5,
+
+                 glo_net_decay: float = 0,
+                 glo_network_lr: float = 1e-3,
+
+                 freeze_start: int = 15000,
+                 iterations: int = 30000,
+                 sh_lr_div: int = 20,
+
+                 **kwargs):
+        self.weight_decay = weight_decay
+        self.lambda_tv = lambda_tv
+        self.lambda_density = lambda_density
+        def process(body, lr, weight_decay=0):
+            hidden_weights = [p for p in body.parameters() if p.ndim >= 2]
+            hidden_gains_biases = [p for p in body.parameters() if p.ndim < 2]
+            a = dict(
+                params=hidden_weights,
+                use_muon = True,
+                momentum=0.95,
+                lr=lr,
+                weight_decay=weight_decay,
+            )
+            b = dict(
+                params=hidden_gains_biases,
+                use_muon = False,
+                betas=(0.9, 0.999),
+                eps=1e-15,
+                weight_decay=weight_decay,
+            )
+            return [a, b]
+        glo_p = process(model.backbone.glo_net, glo_network_lr, weight_decay=glo_net_decay) if model.backbone.glo_dim > 0 else []
+        self.net_optim = SingleDeviceMuonWithAuxAdam(
+            process(model.backbone.density_net, fnetwork_lr) + \
+            process(model.backbone.color_net, fnetwork_lr) + \
+            process(model.backbone.gradient_net, fnetwork_lr) + \
+            glo_p
+        )
+        self.sh_lr_div = sh_lr_div
+        self.sh_net_optim = SingleDeviceMuonWithAuxAdam(
+            process(model.backbone.sh_net, fnetwork_lr/self.sh_lr_div, weight_decay=sh_weight_decay)
+        )
+        self.feature_optim = torch.optim.Adam([
+            {"params": [model.features],       "lr": feature_lr,       "name": "sh"},
+        ])
+        self.sh_optim = None
+        self.model = model
+        self.freeze_start = freeze_start
+        self.net_scheduler = get_expon_lr_func(lr_init=fnetwork_lr,
+                                                lr_final=final_fnetwork_lr,
+                                                lr_delay_mult=1e-8,
+                                                lr_delay_steps=lr_delay,
+                                                max_steps=iterations - self.freeze_start)
+
+        self.feature_scheduler = get_expon_lr_func(lr_init=feature_lr,
+                                                lr_final=final_feature_lr,
+                                                lr_delay_mult=1e-8,
+                                                lr_delay_steps=lr_delay,
+                                                max_steps=iterations - self.freeze_start)
+        self.iteration = 0
+
+    def update_learning_rate(self, iteration):
+        ''' Learning rate scheduling per step '''
+        self.iteration = iteration
+        for param_group in self.net_optim.param_groups:
+            lr = self.net_scheduler(iteration - self.freeze_start)
+            param_group['lr'] = lr
+        for param_group in self.sh_net_optim.param_groups:
+            lr = self.net_scheduler(iteration)
+            param_group['lr'] = lr/self.sh_lr_div
+        for param_group in self.feature_optim.param_groups:
+            lr = self.feature_scheduler(iteration - self.freeze_start)
+            param_group['lr'] = lr
+
+    def step(self):
+        self.net_optim.step()
+        self.feature_optim.step()
+
+    def zero_grad(self):
+        self.net_optim.zero_grad()
+        self.feature_optim.zero_grad()
+
+    # compatibility shims ------------------------------------------------
+    def main_step(self):
+        self.step()
+
+    def main_zero_grad(self):
+        self.zero_grad()
+
+    @torch.no_grad()
+    def split(self, clone_indices, split_point, split_mode):
+        print("Split called on frozen optimizer")
+
+    def regularizer(self, *_):
+        if self.lambda_tv > 0:
+            # simple TV on densities as example
+            diff = (self.model.density[self.pairs[:, 0]] - self.model.density[self.pairs[:, 1]]).abs()
+            tv_loss = (self.face_area * diff).sum() / self.face_area.sum()
+        else:
+            tv_loss = 0.0
+
+        return self.lambda_tv * tv_loss
+
+    def update_triangulation(self, *_, **__):
+        return None
+
+
+def bake_from_model(base_model, mask, args, chunk_size: int = 408_576) -> FrozenTetModel:
+    """Convert an existing neuralfield `Model` into a parameteronly
+    `FrozenTetModel`.  All pertet features are *evaluated once* through the
+    network and stored explicitly so that no backbone is needed afterwards."""
+    device = base_model.device
+
+    vertices = base_model.vertices.detach()
+    int_vertices  = vertices[: base_model.num_int_verts]
+    indices       = base_model.indices[mask].detach()
+
+    features = []
+    for start in range(0, indices.shape[0], chunk_size):
+        end = min(start + chunk_size, indices.shape[0])
+
+        tets = vertices[indices[start:end]]
+        circumcenter, radius = calculate_circumcenters_torch(tets.double())
+        normalized = (circumcenter - base_model.center) / base_model.scene_scaling
+        radius = torch.linalg.norm(circumcenter - vertices[indices[start:end, 0]], dim=-1)
+        cv, cr = contract_mean_std(normalized, radius / base_model.scene_scaling)
+        x = (cv/2 + 1)/2
+        output = base_model.backbone.encode(x, cr)
+        features.append(output)
+
+    features  = torch.cat(features, 0)
+
+    fmodel = FrozenTetModel(
+        int_vertices=int_vertices.to(device),
+        indices=indices.to(device),
+        features=features.to(device),
+        center=base_model.center.detach().to(device),
+        scene_scaling=base_model.scene_scaling.detach().to(device),
+        density_offset=base_model.density_offset,
+        max_sh_deg=base_model.max_sh_deg,
+        chunk_size=chunk_size,
+        sh_hidden_dim=base_model.backbone.sh_hidden_dim,
+        hidden_dim=base_model.backbone.hidden_dim,
+        glo_dim=base_model.glo_dim,
+    )
+    fmodel.backbone.load_state_dict(base_model.backbone.state_dict(), strict=False)
+    return fmodel
+
+def _offload_model_to_cpu(model: nn.Module):
+    """Move every parameter & buffer to CPU and drop gradients to free GPU VRAM."""
+    if model is None:
+        return
+    for p in model.parameters(recurse=True):
+        p.grad = None
+        p.data = p.data.cpu()
+    for b in model.buffers(recurse=True):
+        b.data = b.data.cpu()
+    torch.cuda.empty_cache()
+
+
+@torch.no_grad()
+def freeze_model(
+    base_model,
+    mask,
+    args,
+    chunk_size: int = 408_576,
+) -> Tuple[FrozenTetModel, FrozenTetOptimizer]:
+    """Utility wrapper to *freeze* a trained neuralfield `Model`, produce the
+    corresponding `FrozenTetModel`, and return a readytouse
+    `FrozenTetOptimizer` so training can continue seamlessly.
+
+    Returns
+    -------
+    FrozenTetModel
+        Parameteronly representation of the field.
+    FrozenTetOptimizer
+        Optimiser bound to the frozen model.
+    """
+    print("Freezing model")
+    frozen_model = bake_from_model(base_model, mask, args, chunk_size=chunk_size)
+
+    frozen_optim = FrozenTetOptimizer(
+        frozen_model,
+        **args.as_dict()
+    )
+
+    # free GPU memory used by the big backbone (optional but handy)
+    _offload_model_to_cpu(base_model)
+    del base_model
+    gc.collect()
+    torch.cuda.empty_cache()
+
+    return frozen_model, frozen_optim
diff --git a/models/ingp_color.py b/models/ingp_color.py
index 75f5165..faebdb9 100644
--- a/models/ingp_color.py
+++ b/models/ingp_color.py
@@ -2,32 +2,25 @@ import torch
 import math
 from data.camera import Camera
 from utils import optim
-from sh_slang.eval_sh import eval_sh
-from gDel3D.build.gdel3d import Del
+from gdel3d import Del
 from torch import nn
 from icecream import ic
 
-from utils.topo_utils import calculate_circumcenters_torch, fibonacci_spiral_on_sphere, calc_barycentric, sample_uniform_in_sphere, project_points_to_tetrahedra, contraction_jacobian_d_in_chunks
 from utils import topo_utils
-from utils.safe_math import safe_exp, safe_div, safe_sqrt
 from utils.contraction import contract_mean_std
-from utils.contraction import contract_points, inv_contract_points
 
-from utils.train_util import get_expon_lr_func, SpikingLR
+from utils.train_util import get_expon_lr_func, SpikingLR, TwoPhaseLR
 from utils.graphics_utils import l2_normalize_th
 from torch.utils.checkpoint import checkpoint
 from pathlib import Path
 import numpy as np
 from utils.args import Args
-import tinyplypy
-from scipy.spatial import ConvexHull
 from scipy.spatial import  Delaunay
-import open3d as o3d
-from data.types import BasicPointCloud
-from simple_knn._C import distCUDA2
-from utils import mesh_util
+# import open3d as o3d
+from sh_slang.eval_sh import eval_sh
 from utils.model_util import *
 from models.base_model import BaseModel
+from muon import SingleDeviceMuonWithAuxAdam
 
 torch.set_float32_matmul_precision('high')
 
@@ -35,45 +28,39 @@ torch.set_float32_matmul_precision('high')
 class Model(BaseModel):
     def __init__(self,
                  vertices: torch.Tensor,
-                 ext_vertices: torch.Tensor,
                  center: torch.Tensor,
                  scene_scaling: float,
-                 contract_vertices=True,
                  density_offset=-1,
                  current_sh_deg=2,
                  max_sh_deg=2,
+                 glo_dim=0,
+                 ablate_circumsphere=False,
                  **kwargs):
         super().__init__()
         self.device = vertices.device
         self.density_offset = density_offset
         self.max_sh_deg = max_sh_deg
-        self.current_sh_deg = current_sh_deg
-        self.dir_offset = torch.tensor([
-            [0, 0],
-            [math.pi, 0],
-        ], device=self.device)
-        sh_dim = ((1+max_sh_deg)**2-1)*3
-        self.backbone = torch.compile(iNGPDW(sh_dim, **kwargs)).to(self.device)
+        self.sh_dim = ((1+max_sh_deg)**2-1)*3
+        self.backbone = torch.compile(iNGPDW(self.sh_dim, glo_dim=glo_dim, **kwargs)).to(self.device)
+        self.default_glo = None if glo_dim == 0 else torch.zeros((1, glo_dim), device=self.device)
+        self.glo_dim = glo_dim
         self.chunk_size = 408576
         self.mask_values = True
         self.frozen = False
         self.alpha = 0
         self.linear = False
         self.feature_dim = 7
+        self.current_sh_deg = current_sh_deg
+        self.ablate_circumsphere = ablate_circumsphere
 
-        self.register_buffer('ext_vertices', ext_vertices.to(self.device))
         self.register_buffer('center', center.reshape(1, 3))
         self.register_buffer('scene_scaling', torch.tensor(float(scene_scaling), device=self.device))
-        self.contract_vertices = contract_vertices
-        if self.contract_vertices:
-            self.contracted_vertices = nn.Parameter(self.contract(vertices.detach()))
-        else:
-            self.contracted_vertices = nn.Parameter(vertices.detach())
+        self.interior_vertices = nn.Parameter(vertices.detach())
         self.update_triangulation()
 
     @property
     def num_int_verts(self):
-        return self.contracted_vertices.shape[0]
+        return self.interior_vertices.shape[0]
 
     def get_circumcenters(self):
         circumcenter =  pre_calc_cell_values(
@@ -81,120 +68,96 @@ class Model(BaseModel):
         return circumcenter
 
     def get_cell_values(self, camera: Camera, mask=None,
-                        all_circumcenters=None, radii=None):
+                        all_circumcenters=None, glo=None):
         indices = self.indices[mask] if mask is not None else self.indices
         vertices = self.vertices
-
-        # vertex_color_raw = eval_sh(
-        #     vertices,
-        #     torch.zeros((vertices.shape[0], 3), device=vertices.device),
-        #     self.vertex_lights.reshape(-1, (self.max_sh_deg+1)**2 - 1, 3),
-        #     camera.camera_center.to(self.device),
-        #     self.current_sh_deg) - 0.5
+        glo = glo if glo is not None else self.default_glo
 
         outputs = []
         normed_cc = []
         start = 0
+        cam_center = camera.camera_center.to(self.device)
         for start in range(0, indices.shape[0], self.chunk_size):
             end = min(start + self.chunk_size, indices.shape[0])
+            tets = vertices[indices[start:end]]
             circumcenters, normalized, density, rgb, grd, sh = self.compute_batch_features(
-                vertices, indices, start, end, circumcenters=all_circumcenters)
-            dvrgbs = activate_output(camera.camera_center.to(self.device),
-                                     density, rgb, grd, sh, indices[start:end],
+                vertices, indices, start, end, circumcenters=all_circumcenters, glo=glo)
+            tet_color_raw = eval_sh(
+                tets.mean(dim=1).detach(),
+                RGB2SH(rgb),
+                sh.reshape(-1, (self.max_sh_deg+1)**2 - 1, 3),
+                cam_center,
+                self.current_sh_deg).float()
+            dvrgbs = activate_output(cam_center, tet_color_raw,
+                                     density, grd,
                                      circumcenters,
-                                     vertices, self.current_sh_deg, self.max_sh_deg)
+                                     tets)
             normed_cc.append(normalized)
             outputs.append(dvrgbs)
         features = torch.cat(outputs, dim=0)
         normed_cc = torch.cat(normed_cc, dim=0)
         return normed_cc, features
 
-    @staticmethod
-    def init_from_pcd(point_cloud, cameras, device, max_sh_deg,
-                      voxel_size=0.00, **kwargs):
-        torch.manual_seed(2)
-
-        ccenters = torch.stack([c.camera_center.reshape(3) for c in cameras], dim=0).to(device)
-        center = ccenters.mean(dim=0)
-        scaling = torch.linalg.norm(ccenters - center.reshape(1, 3), dim=1, ord=torch.inf).max()
-        print(f"Scene scaling: {scaling}. Center: {center}")
-
-        vertices = torch.as_tensor(point_cloud.points).float()
-
-        dist = torch.clamp_min(distCUDA2(vertices.cuda()), 0.0000001).sqrt().cpu()
-
-        # vertices = vertices.reshape(-1, 1, 3).expand(-1, init_repeat, 3)
-        # vertices = vertices + torch.randn(*vertices.shape) * dist.reshape(-1, 1, 1).clip(min=0.01)
-        # vertices = vertices.reshape(-1, 3)
-
-        # Convert BasicPointCloud to Open3D PointCloud
-        o3d_pcd = o3d.geometry.PointCloud()
-        o3d_pcd.points = o3d.utility.Vector3dVector(vertices.numpy())
-
-        # Perform voxel downsampling
-        if voxel_size > 0:
-            o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size=voxel_size)
-
-        N = point_cloud.points.shape[0]
-        vertices = torch.as_tensor(np.asarray(o3d_pcd.points)).float()
-        vertices = vertices + torch.randn(*vertices.shape) * 1e-3
-
-        # add sphere
-        pcd_scaling = torch.linalg.norm(vertices - center.cpu().reshape(1, 3), dim=1, ord=2).max()
-        new_radius = pcd_scaling.cpu().item()
-
-        # vertices = sample_uniform_in_sphere(10000, 3, base_radius=0, radius=new_radius, device='cpu') + center.reshape(1, 3).cpu()
-
-        # vertices = vertices + torch.randn(*vertices.shape) * 1e-3
-        # v = Del(vertices.shape[0])
-        # indices_np, prev = v.compute(vertices.detach().cpu().double())
-        # indices_np = indices_np.numpy()
-        # indices_np = indices_np[(indices_np < vertices.shape[0]).all(axis=1)]
-        # vertices = vertices[indices_np].mean(dim=1)
-        # vertices = vertices + torch.randn(*vertices.shape) * 1e-3
+    def compute_features(self):
+        vertices = self.vertices
+        indices = self.indices
+        cs, ds, rs, gs, ss = [], [], [], [], []
+        for start in range(0, indices.shape[0], self.chunk_size):
+            end = min(start + self.chunk_size, indices.shape[0])
 
-        # within_sphere = sample_uniform_in_sphere(10000, 3, base_radius=new_radius, radius=new_radius, device='cpu') + center.reshape(1, 3).cpu()
-        # vertices = torch.cat([vertices, within_sphere], dim=0)
-        num_ext = 1000
-        ext_vertices = fibonacci_spiral_on_sphere(num_ext, new_radius, device='cpu') + center.reshape(1, 3).cpu()
-        num_ext = ext_vertices.shape[0]
+            circumcenters, _, density, rgb, grd, sh = self.compute_batch_features(vertices, indices, start, end, glo=self.default_glo)
+            tets = vertices[indices[start:end]]
+            radius = torch.linalg.norm(tets[:, :1] - circumcenters[:, None, :], dim=-1, keepdim=True)
+            cs.append(circumcenters)
+            ds.append(density)
+            ss.append(sh)
+            rs.append(rgb)
+            gs.append(grd)
+            gs.append(safe_div(grd, radius))
+        cs = torch.cat(cs, dim=0)
+        ds = torch.cat(ds, dim=0)
+        rs = torch.cat(rs, dim=0)
+        gs = torch.cat(gs, dim=0)
+        ss = torch.cat(ss, dim=0)
+        return cs, ds, rs, gs, ss
 
-        model = Model(vertices.cuda(), ext_vertices, center, scaling,
-                      max_sh_deg=max_sh_deg, **kwargs)
-        return model
 
-    def compute_batch_features(self, vertices, indices, start, end, circumcenters=None):
+    def compute_batch_features(self, vertices, indices, start, end, circumcenters=None, glo=None):
+        tets = vertices[indices[start:end]]
         if circumcenters is None:
-            tets = vertices[indices[start:end]]
-            circumcenter, radius = calculate_circumcenters_torch(tets.double())
+            circumcenter, radius = topo_utils.calculate_circumcenters_torch(tets.double())
         else:
             circumcenter = circumcenters[start:end]
-        if self.training:
-            circumcenter += self.alpha*torch.rand_like(circumcenter)
+        if self.ablate_circumsphere:
+            circumcenter = tets.mean(dim=1)
         normalized = (circumcenter - self.center) / self.scene_scaling
         radius = torch.linalg.norm(circumcenter - vertices[indices[start:end, 0]], dim=-1)
         cv, cr = contract_mean_std(normalized, radius / self.scene_scaling)
         x = (cv/2 + 1)/2
-        output = checkpoint(self.backbone, x, cr, use_reentrant=True)
-        return circumcenter, normalized, *output
+
+        glo = glo if glo is not None else self.default_glo
+
+        density, rgb, grd, sh = checkpoint(self.backbone, x, cr, glo, use_reentrant=True)
+        # density = safe_div(density, radius.reshape(-1, 1).detach())
+        # vol = topo_utils.tet_volumes(tets).clip(min=1, max=1000)
+        # density = safe_div(density, vol.reshape(-1, 1).detach())
+        return circumcenter, normalized, density, rgb, grd, sh
 
     @staticmethod
-    def load_ckpt(path: Path, device):
+    def load_ckpt(path: Path, device, config=None):
         ckpt_path = path / "ckpt.pth"
-        config_path = path / "config.json"
-        config = Args.load_from_json(str(config_path))
+        if config is None:
+            config_path = path / "config.json"
+            config = Args.load_from_json(str(config_path))
         ckpt = torch.load(ckpt_path)
-        vertices = ckpt['contracted_vertices']
+        vertices = ckpt['interior_vertices']
         indices = ckpt["indices"]  # shape (N,4)
         del ckpt["indices"]
         print(f"Loaded {vertices.shape[0]} vertices")
-        temp = config.contract_vertices
-        config.contract_vertices = False
-        ext_vertices = ckpt['ext_vertices']
-        model = Model(vertices.to(device), ext_vertices, ckpt['center'], ckpt['scene_scaling'], **config.as_dict())
+        model = Model(vertices.to(device), ckpt['center'], ckpt['scene_scaling'], **config.as_dict())
         model.load_state_dict(ckpt)
-        model.contract_vertices = temp
-        model.min_t = model.scene_scaling * config.base_min_t
+        # model.min_t = model.scene_scaling * config.base_min_t
+        model.min_t = config.base_min_t
         model.indices = torch.as_tensor(indices).cuda()
         return model
 
@@ -204,53 +167,17 @@ class Model(BaseModel):
         for start in range(0, self.indices.shape[0], self.chunk_size):
             end = min(start + self.chunk_size, self.indices.shape[0])
             
-            _, _, density, _, _, _ = self.compute_batch_features(verts, self.indices, start, end)
+            _, _, density, _, _, _ = self.compute_batch_features(verts, self.indices, start, end, glo=self.default_glo)
 
             densities.append(density.reshape(-1))
         return torch.cat(densities)
 
-    def compute_features(self, offset=False):
-        vertices = self.vertices
-        indices = self.indices
-        cs, ds, rs, gs, ss = [], [], [], [], []
-        for start in range(0, indices.shape[0], self.chunk_size):
-            end = min(start + self.chunk_size, indices.shape[0])
-
-            circumcenters, _, density, rgb, grd, sh = self.compute_batch_features(vertices, indices, start, end)
-            tets = vertices[indices[start:end]]
-            cs.append(circumcenters)
-            ds.append(density)
-            ss.append(sh)
-            if offset:
-                base_color_v0_raw, normed_grd = offset_normalize(rgb, grd, circumcenters, tets)
-                rs.append(base_color_v0_raw)
-                gs.append(normed_grd)
-            else:
-                rs.append(rgb)
-                gs.append(grd)
-        cs = torch.cat(cs, dim=0)
-        ds = torch.cat(ds, dim=0)
-        rs = torch.cat(rs, dim=0)
-        gs = torch.cat(gs, dim=0)
-        ss = torch.cat(ss, dim=0)
-        return cs, ds, rs, gs, ss
-
-    def inv_contract(self, points):
-        return inv_contract_points(points) * self.scene_scaling + self.center
-
-    def contract(self, points):
-        return contract_points((points - self.center) / self.scene_scaling)
-
     @property
     def vertices(self):
-        if self.contract_vertices:
-            verts = self.inv_contract(self.contracted_vertices)
-        else:
-            verts = self.contracted_vertices
-        return torch.cat([verts, self.ext_vertices])
+        return self.interior_vertices
 
     def sh_up(self):
-        self.current_sh_deg = min(self.max_sh_deg, self.current_sh_deg+1)
+        self.current_sh_deg = min(self.current_sh_deg + 1, self.max_sh_deg)
 
     @torch.no_grad()
     def update_triangulation(self, high_precision=False, density_threshold=0.0, alpha_threshold=0.0):
@@ -266,11 +193,23 @@ class Model(BaseModel):
             indices_np = indices_np[(indices_np < verts.shape[0]).all(axis=1)]
             del prev
         
-        self.indices = torch.as_tensor(indices_np).cuda()
+        # Ensure volume is positive
+        indices = torch.as_tensor(indices_np).cuda()
+        vols = topo_utils.tet_volumes(verts[indices])
+        reverse_mask = vols < 0
+        if reverse_mask.sum() > 0:
+            indices[reverse_mask] = indices[reverse_mask][:, [1, 0, 2, 3]]
+
+        # Cull tets with low density
+        self.indices = indices
+        denom = topo_utils.tet_denom(self.vertices.detach()[self.indices]).detach()
         if density_threshold > 0 or alpha_threshold > 0:
             tet_density = self.calc_tet_density()
             tet_alpha = self.calc_tet_alpha(mode="min", density=tet_density)
-            mask = (tet_density > density_threshold) | (tet_alpha > alpha_threshold)
+            mask = ((tet_density > density_threshold) | (tet_alpha > alpha_threshold)) & (denom > 1e-10)
+            self.indices = self.indices[mask]
+        else:
+            mask = (denom > 1e-10)
             self.indices = self.indices[mask]
             
         torch.cuda.empty_cache()
@@ -279,6 +218,73 @@ class Model(BaseModel):
         return self.vertices.shape[0]
         
 
+    @staticmethod
+    def init_from_pcd(point_cloud, cameras, device, max_sh_deg,
+                      ext_convex_hull, voxel_size=0.00, **kwargs):
+        torch.manual_seed(2)
+
+        ccenters = torch.stack([c.camera_center.reshape(3) for c in cameras], dim=0).to(device)
+        center = ccenters.mean(dim=0)
+        scaling = torch.linalg.norm(ccenters - center.reshape(1, 3), dim=1, ord=torch.inf).max()
+        print(f"Scene scaling: {scaling}. Center: {center}")
+
+        vertices = torch.as_tensor(point_cloud.points).float()
+
+        # dist = torch.clamp_min(distCUDA2(vertices.cuda()), 0.0000001).sqrt().cpu()
+
+        # vertices = vertices.reshape(-1, 1, 3).expand(-1, init_repeat, 3)
+        # vertices = vertices + torch.randn(*vertices.shape) * dist.reshape(-1, 1, 1).clip(min=0.01)
+        # vertices = vertices.reshape(-1, 3)
+
+        # Convert BasicPointCloud to Open3D PointCloud
+        # o3d_pcd = o3d.geometry.PointCloud()
+        # o3d_pcd.points = o3d.utility.Vector3dVector(vertices.numpy())
+        #
+        # # Perform voxel downsampling
+        # if voxel_size > 0:
+        #     o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size=voxel_size)
+        #
+        # N = point_cloud.points.shape[0]
+        # vertices = torch.as_tensor(np.asarray(o3d_pcd.points)).float()
+
+        vertices = vertices + torch.randn(*vertices.shape) * 1e-3
+
+        # add sphere
+        pcd_scaling = torch.linalg.norm(vertices - center.cpu().reshape(1, 3), dim=1, ord=2)
+        pcd_scaling = (vertices - vertices.mean(dim=0, keepdim=True)).abs().max(dim=0).values
+        new_radius = math.sqrt(2) * pcd_scaling.cpu()
+
+        # vertices = topo_util.sample_uniform_in_sphere(10000, 3, base_radius=0, radius=new_radius, device='cpu') + center.reshape(1, 3).cpu()
+
+        # vertices = vertices + torch.randn(*vertices.shape) * 1e-3
+        # v = Del(vertices.shape[0])
+        # indices_np, prev = v.compute(vertices.detach().cpu().double())
+        # indices_np = indices_np.numpy()
+        # indices_np = indices_np[(indices_np < vertices.shape[0]).all(axis=1)]
+        # vertices = vertices[indices_np].mean(dim=1)
+        # vertices = vertices + torch.randn(*vertices.shape) * 1e-3
+
+        # within_sphere = topo_util.sample_uniform_in_sphere(10000, 3, base_radius=new_radius, radius=new_radius, device='cpu') + center.reshape(1, 3).cpu()
+        # vertices = torch.cat([vertices, within_sphere], dim=0)
+        if ext_convex_hull:
+            num_ext = 5000
+            ext_vertices = topo_utils.expand_convex_hull(vertices, 1, device=vertices.device)
+            if ext_vertices.shape[0] > num_ext:
+                inds = np.random.default_rng().permutation(ext_vertices.shape[0])[:num_ext]
+                ext_vertices = ext_vertices[inds]
+            else:
+                num_ext = ext_vertices.shape[0]
+        else:
+            num_ext = 5000
+            ext_vertices = topo_utils.fibonacci_spiral_on_sphere(num_ext, new_radius.reshape(1, 3), device='cpu') + center.reshape(1, 3).cpu()
+            # ext_vertices = torch.empty((0, 3), device='cpu')
+            num_ext = ext_vertices.shape[0]
+        vertices = torch.cat([vertices, ext_vertices], dim=0)
+
+        model = Model(vertices.cuda(), center, scaling,
+                      max_sh_deg=max_sh_deg, **kwargs)
+        return model
+
 class TetOptimizer:
     def __init__(self,
                  model: Model,
@@ -289,13 +295,12 @@ class TetOptimizer:
                  vertices_lr: float=4e-4,
                  final_vertices_lr: float=4e-7,
                  vertices_lr_delay_multi: float=0.01,
+
                  weight_decay=1e-10,
-                 lambda_color=1e-10,
                  split_std: float = 0.5,
                  lr_delay: int = 500,
-                 freeze_start: int = 10000,
+                 final_iter: int = 10000,
                  vert_lr_delay: int = 500,
-                 sh_interval: int = 1000,
                  lambda_tv: float = 0.0,
                  lambda_density: float = 0.0,
 
@@ -304,83 +309,107 @@ class TetOptimizer:
                  densify_interval: int = 500,
                  densify_end: int = 15000,
                  midpoint: int = 2000,
+                 sh_lr_div: int = 20,
 
-                 density_lr:  float = 1e-3,
-                 color_lr:    float = 1e-3,
-                 gradient_lr: float = 1e-3,
-                 sh_lr:       float = 1e-3,
-
-                 lambda_dist: float = 1e-5,
+                 glo_net_decay: float = 0,
+                 glo_network_lr: float = 1e-3,
                  percent_alpha: float = 0.02,
-                 dist_delay: int = 2000,
-
+                 sh_weight_decay: float = 1e-5,
                  **kwargs):
-        self.weight_decay = weight_decay
-        self.lambda_color = lambda_color
-        self.lambda_tv = lambda_tv
-        self.lambda_density = lambda_density
         self.optim = optim.CustomAdam([
             {"params": model.backbone.encoding.parameters(), "lr": encoding_lr, "name": "encoding"},
         ], ignore_param_list=["encoding", "network"], betas=[0.9, 0.999], eps=1e-15)
-        self.net_optim = optim.CustomAdam([
-            {"params": model.backbone.network.parameters(), "lr": network_lr, "name": "network"},
-            {"params": model.backbone.density_net.parameters(),   "lr": network_lr,  "name": "density"},
-            {"params": model.backbone.color_net.parameters(),     "lr": network_lr,    "name": "color"},
-            {"params": model.backbone.gradient_net.parameters(),  "lr": network_lr, "name": "gradient"},
-            {"params": model.backbone.sh_net.parameters(),        "lr": network_lr,       "name": "sh"},
-        ], ignore_param_list=[], betas=[0.9, 0.999])
-        self.vert_lr_multi = 1 if model.contract_vertices else float(model.scene_scaling.cpu())
+        def process(body, lr, weight_decay=0):
+            hidden_weights = [p for p in body.parameters() if p.ndim >= 2]
+            hidden_gains_biases = [p for p in body.parameters() if p.ndim < 2]
+            a = dict(
+                params=hidden_weights,
+                use_muon = True,
+                momentum=0.95,
+                lr=lr,
+                weight_decay=weight_decay,
+            )
+            b = dict(
+                params=hidden_gains_biases,
+                use_muon = False,
+                betas=(0.9, 0.999),
+                eps=1e-15,
+                weight_decay=weight_decay,
+            )
+            return [a, b]
+        glo_p = process(model.backbone.glo_net, glo_network_lr, weight_decay=glo_net_decay) if model.backbone.glo_dim > 0 else []
+        self.sh_lr_div = sh_lr_div
+        self.net_optim = SingleDeviceMuonWithAuxAdam(
+            process(model.backbone.density_net, network_lr) + \
+            process(model.backbone.color_net, network_lr) + \
+            process(model.backbone.gradient_net, network_lr) + \
+            glo_p
+        )
+        self.sh_net_optim = SingleDeviceMuonWithAuxAdam(
+            process(model.backbone.sh_net, network_lr/self.sh_lr_div, weight_decay=sh_weight_decay)
+        )
+        self.vert_lr_multi = float(model.scene_scaling.cpu())
         self.vertex_optim = optim.CustomAdam([
-            {"params": [model.contracted_vertices], "lr": self.vert_lr_multi*vertices_lr, "name": "contracted_vertices"},
+            {"params": [model.interior_vertices], "lr": self.vert_lr_multi*vertices_lr, "name": "interior_vertices"},
         ])
         self.model = model
         self.vertex_rgbs_param_grad = None
         self.vertex_grad = None
         self.split_std = split_std
+        def make_spiking(init_lr, final_peak_lr, final_lr):
+            return TwoPhaseLR(
+                final_iter, densify_start, densify_interval, densify_end, init_lr, final_peak_lr, (init_lr + final_lr) / 2, final_lr)
 
         self.alpha_sched = get_expon_lr_func(lr_init=percent_alpha*float(model.scene_scaling.cpu()),
                                                 lr_final=1e-20,
                                                 lr_delay_mult=1e-8,
                                                 lr_delay_steps=0,
-                                                max_steps=freeze_start//3)
+                                                max_steps=final_iter//3)
 
         base_net_scheduler = get_expon_lr_func(lr_init=network_lr,
                                                 lr_final=final_network_lr,
                                                 lr_delay_mult=1e-8,
                                                 lr_delay_steps=lr_delay,
-                                                max_steps=freeze_start)
+                                                max_steps=final_iter)
 
         self.net_scheduler_args = SpikingLR(
-            spike_duration, freeze_start, base_net_scheduler,
+            spike_duration, final_iter, base_net_scheduler,
             midpoint, densify_interval, densify_end,
             network_lr, network_lr)
+        # self.net_scheduler_args = make_spiking(
+        #     network_lr, network_lr, final_network_lr)
             # network_lr, final_network_lr)
 
         base_encoder_scheduler = get_expon_lr_func(lr_init=encoding_lr,
                                                 lr_final=final_encoding_lr,
                                                 lr_delay_mult=1e-8,
                                                 lr_delay_steps=lr_delay,
-                                                max_steps=freeze_start)
+                                                max_steps=final_iter)
 
         self.encoder_scheduler_args = SpikingLR(
-            spike_duration, freeze_start, base_encoder_scheduler,
+            spike_duration, final_iter, base_encoder_scheduler,
             midpoint, densify_interval, densify_end,
             encoding_lr, encoding_lr)
             # encoding_lr, final_encoding_lr)
+        # self.encoder_scheduler_args = make_spiking(
+        #     encoding_lr, encoding_lr, final_encoding_lr)
 
         self.vertex_lr = self.vert_lr_multi*vertices_lr
         base_vertex_scheduler = get_expon_lr_func(lr_init=self.vertex_lr,
                                                 lr_final=self.vert_lr_multi*final_vertices_lr,
                                                 lr_delay_mult=vertices_lr_delay_multi,
-                                                max_steps=freeze_start,
+                                                max_steps=final_iter,
                                                 lr_delay_steps=vert_lr_delay)
 
         self.vertex_scheduler_args = base_vertex_scheduler
         self.vertex_scheduler_args = SpikingLR(
-            spike_duration, freeze_start, base_vertex_scheduler,
+            spike_duration, final_iter, base_vertex_scheduler,
             midpoint, densify_interval, densify_end,
             # self.vertex_lr, self.vert_lr_multi*final_vertices_lr)
             self.vertex_lr, self.vertex_lr)
+
+        # self.vertex_scheduler_args = make_spiking(
+        #     self.vertex_lr, self.vertex_lr, self.vert_lr_multi*final_vertices_lr)
         self.iteration = 0
 
     def update_learning_rate(self, iteration):
@@ -390,120 +419,55 @@ class TetOptimizer:
         for param_group in self.net_optim.param_groups:
             lr = self.net_scheduler_args(iteration)
             param_group['lr'] = lr
+        for param_group in self.sh_net_optim.param_groups:
+            lr = self.net_scheduler_args(iteration)
+            param_group['lr'] = lr/self.sh_lr_div
         for param_group in self.optim.param_groups:
             if param_group["name"] == "encoding":
                 lr = self.encoder_scheduler_args(iteration)
                 param_group['lr'] = lr
         for param_group in self.vertex_optim.param_groups:
-            if param_group["name"] == "contracted_vertices":
+            if param_group["name"] == "interior_vertices":
                 lr = self.vertex_scheduler_args(iteration)
                 self.vertex_lr = lr
                 param_group['lr'] = lr
 
     def add_points(self, new_verts: torch.Tensor, raw_verts=False):
-        if self.model.contract_vertices and not raw_verts:
-            new_verts = self.model.contract(new_verts)
-        self.model.contracted_vertices = self.vertex_optim.cat_tensors_to_optimizer(dict(
-            contracted_vertices = new_verts
-        ))['contracted_vertices']
+        self.model.interior_vertices = self.vertex_optim.cat_tensors_to_optimizer(dict(
+            interior_vertices = new_verts
+        ))['interior_vertices']
         self.model.update_triangulation()
 
     def remove_points(self, keep_mask: torch.Tensor):
-        keep_mask = keep_mask[:self.model.contracted_vertices.shape[0]]
-        self.model.contracted_vertices = self.vertex_optim.prune_optimizer(keep_mask)['contracted_vertices']
+        keep_mask = keep_mask[:self.model.interior_vertices.shape[0]]
+        self.model.interior_vertices = self.vertex_optim.prune_optimizer(keep_mask)['interior_vertices']
         self.model.update_triangulation()
 
     @torch.no_grad()
-    def split(self, clone_indices, split_point, split_mode, split_std, **kwargs):
+    def split(self, clone_indices, split_point, split_std, **kwargs):
         device = self.model.device
-        clone_vertices = self.model.vertices[clone_indices]
-
-        if split_mode == "circumcenter":
-            circumcenters, radius = calculate_circumcenters_torch(clone_vertices)
-            radius = radius.reshape(-1, 1)
-            circumcenters = circumcenters.reshape(-1, 3)
-            sphere_loc = sample_uniform_in_sphere(circumcenters.shape[0], 3).to(device)
-            r = torch.randn((clone_indices.shape[0], 1), device=self.model.device)
-            r[r.abs() < 1e-2] = 1e-2
-            sampled_radius = (r * self.split_std + 1) * radius
-            new_vertex_location = l2_normalize_th(sphere_loc) * sampled_radius + circumcenters
-        elif split_mode == "barycenter":
-            barycentric_weights = 0.25*torch.ones((clone_indices.shape[0], clone_indices.shape[1], 1), device=device).clip(min=0.01, max=0.99)
-            new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights).sum(dim=1)
-        elif split_mode == "barycentric":
-            barycentric = torch.rand((clone_indices.shape[0], clone_indices.shape[1], 1), device=device).clip(min=0.01, max=0.99)
-            barycentric_weights = barycentric / (1e-3+barycentric.sum(dim=1, keepdim=True))
-            new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights).sum(dim=1)
-        elif split_mode == "split_point":
-            _, radius = calculate_circumcenters_torch(self.model.vertices[clone_indices])
-            split_point += (split_std * radius.reshape(-1, 1)).clip(min=1e-3, max=3) * torch.randn(*split_point.shape, device=self.model.device)
-            new_vertex_location = split_point
-            # new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights.unsqueeze(-1)).sum(dim=1)
-        elif split_mode == "split_point_c":
-            barycentric_weights = calc_barycentric(split_point, clone_vertices).clip(min=0)
-            barycentric_weights = barycentric_weights / (1e-3+barycentric_weights.sum(dim=1, keepdim=True))
-            barycentric_weights += 1e-4*torch.randn(*barycentric_weights.shape, device=self.model.device)
-            new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights.unsqueeze(-1)).sum(dim=1)
-        else:
-            raise Exception(f"Split mode: {split_mode} not supported")
-        self.add_points(new_vertex_location)
+        _, radius = topo_utils.calculate_circumcenters_torch(self.model.vertices[clone_indices])
+        split_point += (split_std * radius.reshape(-1, 1)).clip(min=1e-3, max=3) * torch.randn(*split_point.shape, device=self.model.device)
+        self.add_points(split_point)
 
     def main_step(self):
         self.optim.step()
         self.net_optim.step()
+        self.sh_net_optim.step()
 
     def main_zero_grad(self):
         self.optim.zero_grad(set_to_none=True)
         self.net_optim.zero_grad(set_to_none=True)
+        self.sh_net_optim.zero_grad(set_to_none=True)
 
     @property
     def sh_optim(self):
         return None
 
-    def regularizer(self, render_pkg):
-        weight_decay = self.weight_decay * sum([(embed.weight**2).mean() for embed in self.model.backbone.encoding.embeddings])
-
-        if self.lambda_density > 0 or self.lambda_tv > 0:
-            density = self.model.calc_tet_density()
-            density_loss = (self.model.calc_tet_area().detach() * density).sum()
-            if self.lambda_tv > 0:
-                diff  = density[self.pairs[:,0]] - density[self.pairs[:,1]]
-                tv_loss  = (self.face_area * diff.abs())
-                tv_loss  = tv_loss.sum() / self.face_area.sum()
-            else:
-                tv_loss = 0
-        else:
-            density_loss = 0
-            tv_loss = 0
+    def regularizer(self, render_pkg, weight_decay, lambda_tv, **kwargs):
+        weight_decay = weight_decay * sum([(embed.weight**2).mean() for embed in self.model.backbone.encoding.embeddings])
 
-        return weight_decay + self.lambda_tv * tv_loss + self.lambda_density * density_loss
+        return weight_decay
 
     def update_triangulation(self, **kwargs):
         self.model.update_triangulation(**kwargs)
-        if self.lambda_tv > 0:
-            self.build_tv()
-
-    def build_tv(self):
-        self.pairs, self.face_area = topo_utils.build_tv_struct(
-            self.model.vertices.detach(), self.model.indices, device='cuda')
-
-    def prune(self, diff_threshold, **kwargs):
-        if diff_threshold <= 0:
-            return
-        density = self.model.calc_tet_density()
-        self.build_tv()
-        diff  = density[self.pairs[:,0]] - density[self.pairs[:,1]]
-        tet_diff  = (self.face_area * diff.abs()).reshape(-1)
-
-        indices = self.model.indices.long()
-        device = indices.device
-        vert_diff = torch.zeros((self.model.vertices.shape[0],), device=device)
-
-        reduce_type = "amax"
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 0], src=tet_diff, reduce=reduce_type)
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 1], src=tet_diff, reduce=reduce_type)
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 2], src=tet_diff, reduce=reduce_type)
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 3], src=tet_diff, reduce=reduce_type)
-        keep_mask = vert_diff > diff_threshold
-        print(f"Pruned {(~keep_mask).sum()} points. VD: {vert_diff.mean()} TD: {tet_diff.mean()}")
-        self.remove_points(keep_mask)
diff --git a/models/ingp_linear.py b/models/ingp_linear.py
deleted file mode 100644
index 589767b..0000000
--- a/models/ingp_linear.py
+++ /dev/null
@@ -1,523 +0,0 @@
-import torch
-import math
-from data.camera import Camera
-from utils import optim
-from sh_slang.eval_sh import eval_sh
-from gDel3D.build.gdel3d import Del
-from torch import nn
-from icecream import ic
-
-from utils.topo_utils import calculate_circumcenters_torch, fibonacci_spiral_on_sphere, calc_barycentric, sample_uniform_in_sphere, project_points_to_tetrahedra, contraction_jacobian_d_in_chunks
-from utils import topo_utils
-from utils.safe_math import safe_exp, safe_div, safe_sqrt
-from utils.contraction import contract_mean_std
-from utils.contraction import contract_points, inv_contract_points
-
-from utils.train_util import get_expon_lr_func, SpikingLR
-from utils.graphics_utils import l2_normalize_th
-from torch.utils.checkpoint import checkpoint
-from pathlib import Path
-import numpy as np
-from utils.args import Args
-import tinyplypy
-from scipy.spatial import ConvexHull
-from scipy.spatial import  Delaunay
-import open3d as o3d
-from data.types import BasicPointCloud
-from simple_knn._C import distCUDA2
-from utils import mesh_util
-from utils.lmodel_util import *
-from models.base_model import BaseModel
-
-torch.set_float32_matmul_precision('high')
-
-
-class Model(BaseModel):
-    def __init__(self,
-                 vertices: torch.Tensor,
-                 ext_vertices: torch.Tensor,
-                 center: torch.Tensor,
-                 scene_scaling: float,
-                 contract_vertices=True,
-                 density_offset=-1,
-                 current_sh_deg=2,
-                 max_sh_deg=2,
-                 **kwargs):
-        super().__init__()
-        self.device = vertices.device
-        self.density_offset = density_offset
-        self.max_sh_deg = max_sh_deg
-        self.current_sh_deg = current_sh_deg
-        self.dir_offset = torch.tensor([
-            [0, 0],
-            [math.pi, 0],
-        ], device=self.device)
-        sh_dim = ((1+max_sh_deg)**2-1)*3
-        self.backbone = torch.compile(iNGPDWL(sh_dim, **kwargs)).to(self.device)
-        self.chunk_size = 408576
-        self.mask_values = True
-        self.frozen = False
-        self.alpha = 0
-        self.linear = True
-        self.feature_dim = 10
-
-        self.register_buffer('ext_vertices', ext_vertices.to(self.device))
-        self.register_buffer('center', center.reshape(1, 3))
-        self.register_buffer('scene_scaling', torch.tensor(float(scene_scaling), device=self.device))
-        self.contract_vertices = contract_vertices
-        if self.contract_vertices:
-            self.contracted_vertices = nn.Parameter(self.contract(vertices.detach()))
-        else:
-            self.contracted_vertices = nn.Parameter(vertices.detach())
-        self.update_triangulation()
-
-    @property
-    def num_int_verts(self):
-        return self.contracted_vertices.shape[0]
-
-    def get_circumcenters(self):
-        circumcenter =  pre_calc_cell_values(
-            self.vertices, self.indices, self.center, self.scene_scaling)
-        return circumcenter
-
-    def get_cell_values(self, camera: Camera, mask=None,
-                        all_circumcenters=None, radii=None):
-        indices = self.indices[mask] if mask is not None else self.indices
-        vertices = self.vertices
-
-        # vertex_color_raw = eval_sh(
-        #     vertices,
-        #     torch.zeros((vertices.shape[0], 3), device=vertices.device),
-        #     self.vertex_lights.reshape(-1, (self.max_sh_deg+1)**2 - 1, 3),
-        #     camera.camera_center.to(self.device),
-        #     self.current_sh_deg) - 0.5
-
-        outputs = []
-        normed_cc = []
-        start = 0
-        for start in range(0, indices.shape[0], self.chunk_size):
-            end = min(start + self.chunk_size, indices.shape[0])
-            circumcenters, normalized, base_density, rgb, grd, d_grd, sh = self.compute_batch_features(
-                vertices, indices, start, end, circumcenters=all_circumcenters)
-            dvrgbs = activate_output(camera.camera_center.to(self.device),
-                                     rgb, grd, base_density, d_grd, sh, indices[start:end],
-                                     circumcenters,
-                                     vertices, self.current_sh_deg, self.max_sh_deg)
-            normed_cc.append(normalized)
-            outputs.append(dvrgbs)
-        features = torch.cat(outputs, dim=0)
-        normed_cc = torch.cat(normed_cc, dim=0)
-        return normed_cc, features
-
-    @staticmethod
-    def init_from_pcd(point_cloud, cameras, device, max_sh_deg,
-                      voxel_size=0.00, init_repeat=3, **kwargs):
-        torch.manual_seed(2)
-
-        vertices = torch.as_tensor(point_cloud.points).float()
-
-        dist = torch.clamp_min(distCUDA2(vertices.cuda()), 0.0000001).sqrt().cpu()
-
-        vertices = vertices.reshape(-1, 1, 3).expand(-1, init_repeat, 3)
-        vertices = vertices + torch.randn(*vertices.shape) * dist.reshape(-1, 1, 1).clip(min=0.01)
-        vertices = vertices.reshape(-1, 3)
-
-        # Convert BasicPointCloud to Open3D PointCloud
-        o3d_pcd = o3d.geometry.PointCloud()
-        o3d_pcd.points = o3d.utility.Vector3dVector(vertices.numpy())
-
-        # Perform voxel downsampling
-        if voxel_size > 0:
-            o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size=voxel_size)
-
-        N = point_cloud.points.shape[0]
-        vertices = torch.as_tensor(np.asarray(o3d_pcd.points)).float()
-        vertices = vertices + torch.randn(*vertices.shape) * 1e-3
-
-        ccenters = torch.stack([c.camera_center.reshape(3) for c in cameras], dim=0).to(device)
-        center = ccenters.mean(dim=0)
-        scaling = torch.linalg.norm(ccenters - center.reshape(1, 3), dim=1, ord=torch.inf).max()
-
-        # add sphere
-        pcd_scaling = torch.linalg.norm(vertices - center.cpu().reshape(1, 3), dim=1, ord=2).max()
-        new_radius = pcd_scaling.cpu().item()
-
-        # vertices = sample_uniform_in_sphere(10000, 3, base_radius=0, radius=new_radius, device='cpu') + center.reshape(1, 3).cpu()
-
-        # vertices = vertices + torch.randn(*vertices.shape) * 1e-3
-        # v = Del(vertices.shape[0])
-        # indices_np, prev = v.compute(vertices.detach().cpu().double())
-        # indices_np = indices_np.numpy()
-        # indices_np = indices_np[(indices_np < vertices.shape[0]).all(axis=1)]
-        # vertices = vertices[indices_np].mean(dim=1)
-        # vertices = vertices + torch.randn(*vertices.shape) * 1e-3
-
-        # within_sphere = sample_uniform_in_sphere(10000, 3, base_radius=new_radius, radius=new_radius, device='cpu') + center.reshape(1, 3).cpu()
-        # vertices = torch.cat([vertices, within_sphere], dim=0)
-        num_ext = 1000
-        ext_vertices = fibonacci_spiral_on_sphere(num_ext, new_radius, device='cpu') + center.reshape(1, 3).cpu()
-        num_ext = ext_vertices.shape[0]
-
-        model = Model(vertices.cuda(), ext_vertices, center, scaling,
-                      max_sh_deg=max_sh_deg, **kwargs)
-        return model
-
-    def compute_batch_features(self, vertices, indices, start, end, circumcenters=None):
-        if circumcenters is None:
-            tets = vertices[indices[start:end]]
-            circumcenter, radius = calculate_circumcenters_torch(tets.double())
-        else:
-            circumcenter = circumcenters[start:end]
-        if self.training:
-            circumcenter += self.alpha*torch.rand_like(circumcenter)
-        normalized = (circumcenter - self.center) / self.scene_scaling
-        radius = torch.linalg.norm(circumcenter - vertices[indices[start:end, 0]], dim=-1)
-        cv, cr = contract_mean_std(normalized, radius / self.scene_scaling)
-        x = (cv/2 + 1)/2
-        output = checkpoint(self.backbone, x, cr, use_reentrant=True)
-        return circumcenter, normalized, *output
-
-    @staticmethod
-    def load_ckpt(path: Path, device):
-        ckpt_path = path / "ckpt.pth"
-        config_path = path / "config.json"
-        config = Args.load_from_json(str(config_path))
-        ckpt = torch.load(ckpt_path)
-        vertices = ckpt['contracted_vertices']
-        indices = ckpt["indices"]  # shape (N,4)
-        del ckpt["indices"]
-        print(f"Loaded {vertices.shape[0]} vertices")
-        temp = config.contract_vertices
-        config.contract_vertices = False
-        ext_vertices = ckpt['ext_vertices']
-        model = Model(vertices.to(device), ext_vertices, ckpt['center'], ckpt['scene_scaling'], **config.as_dict())
-        model.load_state_dict(ckpt)
-        model.contract_vertices = temp
-        model.min_t = model.scene_scaling * config.base_min_t
-        model.indices = torch.as_tensor(indices).cuda()
-        return model
-
-    def calc_tet_density(self):
-        densities = []
-        verts = self.vertices
-        for start in range(0, self.indices.shape[0], self.chunk_size):
-            end = min(start + self.chunk_size, self.indices.shape[0])
-            bf = self.compute_batch_features(verts, self.indices, start, end)
-            density = bf[2]
-
-            densities.append(density.reshape(-1))
-        return torch.cat(densities)
-
-    def compute_features(self, offset=False):
-        vertices = self.vertices
-        indices = self.indices
-        cs, ds, rs, gs, ss, dgs = [], [], [], [], [], []
-        for start in range(0, indices.shape[0], self.chunk_size):
-            end = min(start + self.chunk_size, indices.shape[0])
-
-            circumcenters, _, density, rgb, grd, d_grd, sh = self.compute_batch_features(vertices, indices, start, end)
-            tets = vertices[indices[start:end]]
-            cs.append(circumcenters)
-            ds.append(density)
-            ss.append(sh)
-            if offset:
-                base_color_v0_raw, normed_grd = offset_normalize(
-                    rgb, grd, density, d_grd, circumcenters, tets)
-                rs.append(base_color_v0_raw)
-                gs.append(normed_grd)
-                dgs.append(d_grd)
-            else:
-                rs.append(rgb)
-                gs.append(grd)
-                dgs.append(d_grd)
-        cs = torch.cat(cs, dim=0)
-        ds = torch.cat(ds, dim=0)
-        rs = torch.cat(rs, dim=0)
-        gs = torch.cat(gs, dim=0)
-        ss = torch.cat(ss, dim=0)
-        dgs = torch.cat(dgs, dim=0)
-        return cs, ds, rs, gs, ss, dgs
-
-    def inv_contract(self, points):
-        return inv_contract_points(points) * self.scene_scaling + self.center
-
-    def contract(self, points):
-        return contract_points((points - self.center) / self.scene_scaling)
-
-    @property
-    def vertices(self):
-        if self.contract_vertices:
-            verts = self.inv_contract(self.contracted_vertices)
-        else:
-            verts = self.contracted_vertices
-        return torch.cat([verts, self.ext_vertices])
-
-    def sh_up(self):
-        self.current_sh_deg = min(self.max_sh_deg, self.current_sh_deg+1)
-
-    @torch.no_grad()
-    def update_triangulation(self, high_precision=False, density_threshold=0.0, alpha_threshold=0.0):
-        torch.cuda.empty_cache()
-        verts = self.vertices
-        if high_precision:
-            indices_np = Delaunay(verts.detach().cpu().numpy()).simplices.astype(np.int32)
-            # self.indices = torch.tensor(indices_np, device=verts.device).int().cuda()
-        else:
-            v = Del(verts.shape[0])
-            indices_np, prev = v.compute(verts.detach().cpu().double())
-            indices_np = indices_np.numpy()
-            indices_np = indices_np[(indices_np < verts.shape[0]).all(axis=1)]
-            del prev
-        
-        self.indices = torch.as_tensor(indices_np).cuda()
-        if density_threshold > 0 or alpha_threshold > 0:
-            tet_density = self.calc_tet_density()
-            tet_alpha = self.calc_tet_alpha(mode="min", density=tet_density)
-            mask = (tet_density > density_threshold) | (tet_alpha > alpha_threshold)
-            self.indices = self.indices[mask]
-            
-        torch.cuda.empty_cache()
-
-    def __len__(self):
-        return self.vertices.shape[0]
-        
-
-class TetOptimizer:
-    def __init__(self,
-                 model: Model,
-                 encoding_lr: float=1e-2,
-                 final_encoding_lr: float=1e-2,
-                 network_lr: float=1e-3,
-                 final_network_lr: float=1e-3,
-                 vertices_lr: float=4e-4,
-                 final_vertices_lr: float=4e-7,
-                 vertices_lr_delay_multi: float=0.01,
-                 weight_decay=1e-10,
-                 lambda_color=1e-10,
-                 split_std: float = 0.5,
-                 lr_delay: int = 500,
-                 max_steps: int = 10000,
-                 vert_lr_delay: int = 500,
-                 sh_interval: int = 1000,
-                 lambda_tv: float = 0.0,
-                 lambda_density: float = 0.0,
-
-                 spike_duration: int = 20,
-                 densify_start: int = 2500,
-                 densify_interval: int = 500,
-                 densify_end: int = 15000,
-                 midpoint: int = 2000,
-
-                 density_lr:  float = 1e-3,
-                 color_lr:    float = 1e-3,
-                 gradient_lr: float = 1e-3,
-                 sh_lr:       float = 1e-3,
-
-                 lambda_dist: float = 1e-5,
-                 percent_alpha: float = 0.02,
-                 dist_delay: int = 2000,
-
-                 **kwargs):
-        self.weight_decay = weight_decay
-        self.lambda_color = lambda_color
-        self.lambda_tv = lambda_tv
-        self.lambda_density = lambda_density
-        self.optim = optim.CustomAdam([
-            {"params": model.backbone.encoding.parameters(), "lr": encoding_lr, "name": "encoding"},
-        ], ignore_param_list=["encoding", "network"], betas=[0.9, 0.99], eps=1e-15)
-        self.net_optim = optim.CustomAdam([
-            {"params": model.backbone.network.parameters(), "lr": network_lr, "name": "network"},
-            {"params": model.backbone.density_net.parameters(),   "lr": density_lr,  "name": "density"},
-            {"params": model.backbone.color_net.parameters(),     "lr": color_lr,    "name": "color"},
-            {"params": model.backbone.gradient_net.parameters(),  "lr": gradient_lr, "name": "gradient"},
-            {"params": model.backbone.d_gradient_net.parameters(),  "lr": gradient_lr, "name": "d_gradient"},
-            {"params": model.backbone.sh_net.parameters(),        "lr": sh_lr,       "name": "sh"},
-        ], ignore_param_list=[], betas=[0.9, 0.99])
-        self.ratios = dict(
-            network = 1,
-            density = density_lr / network_lr,
-            color = color_lr / network_lr,
-            gradient = gradient_lr / network_lr,
-            d_gradient = gradient_lr / network_lr,
-            sh = sh_lr / network_lr,
-        )
-        self.vert_lr_multi = 1 if model.contract_vertices else float(model.scene_scaling.cpu())
-        self.vertex_optim = optim.CustomAdam([
-            {"params": [model.contracted_vertices], "lr": self.vert_lr_multi*vertices_lr, "name": "contracted_vertices"},
-        ])
-        self.model = model
-        self.vertex_rgbs_param_grad = None
-        self.vertex_grad = None
-        self.split_std = split_std
-
-        self.alpha_sched = get_expon_lr_func(lr_init=percent_alpha*float(model.scene_scaling.cpu()),
-                                                lr_final=1e-20,
-                                                lr_delay_mult=1e-8,
-                                                lr_delay_steps=0,
-                                                max_steps=max_steps//3)
-
-        base_net_scheduler = get_expon_lr_func(lr_init=network_lr,
-                                                lr_final=final_network_lr,
-                                                lr_delay_mult=1e-8,
-                                                lr_delay_steps=lr_delay,
-                                                max_steps=max_steps)
-
-        self.net_scheduler_args = SpikingLR(
-            spike_duration, max_steps, base_net_scheduler,
-            midpoint, densify_interval, densify_end,
-            network_lr, network_lr)
-            # network_lr, final_network_lr)
-
-        base_encoder_scheduler = get_expon_lr_func(lr_init=encoding_lr,
-                                                lr_final=final_encoding_lr,
-                                                lr_delay_mult=1e-8,
-                                                lr_delay_steps=lr_delay,
-                                                max_steps=max_steps)
-
-        self.encoder_scheduler_args = SpikingLR(
-            spike_duration, max_steps, base_encoder_scheduler,
-            midpoint, densify_interval, densify_end,
-            encoding_lr, encoding_lr)
-            # encoding_lr, final_encoding_lr)
-
-        self.vertex_lr = self.vert_lr_multi*vertices_lr
-        base_vertex_scheduler = get_expon_lr_func(lr_init=self.vertex_lr,
-                                                lr_final=self.vert_lr_multi*final_vertices_lr,
-                                                lr_delay_mult=vertices_lr_delay_multi,
-                                                max_steps=max_steps,
-                                                lr_delay_steps=vert_lr_delay)
-
-        self.vertex_scheduler_args = base_vertex_scheduler
-        self.vertex_scheduler_args = SpikingLR(
-            spike_duration, max_steps, base_vertex_scheduler,
-            midpoint, densify_interval, densify_end,
-            # self.vertex_lr, self.vert_lr_multi*final_vertices_lr)
-            self.vertex_lr, self.vertex_lr)
-        self.iteration = 0
-
-    def update_learning_rate(self, iteration):
-        ''' Learning rate scheduling per step '''
-        self.iteration = iteration
-        self.model.alpha = self.alpha_sched(iteration)
-        for param_group in self.net_optim.param_groups:
-            ratio = self.ratios[param_group["name"]]
-            lr = self.net_scheduler_args(iteration)
-            param_group['lr'] = ratio * lr
-        for param_group in self.optim.param_groups:
-            if param_group["name"] == "encoding":
-                lr = self.encoder_scheduler_args(iteration)
-                param_group['lr'] = lr
-        for param_group in self.vertex_optim.param_groups:
-            if param_group["name"] == "contracted_vertices":
-                lr = self.vertex_scheduler_args(iteration)
-                self.vertex_lr = lr
-                param_group['lr'] = lr
-
-    def add_points(self, new_verts: torch.Tensor, raw_verts=False):
-        if self.model.contract_vertices and not raw_verts:
-            new_verts = self.model.contract(new_verts)
-        self.model.contracted_vertices = self.vertex_optim.cat_tensors_to_optimizer(dict(
-            contracted_vertices = new_verts
-        ))['contracted_vertices']
-        self.model.update_triangulation()
-
-    def remove_points(self, keep_mask: torch.Tensor):
-        keep_mask = keep_mask[:self.model.contracted_vertices.shape[0]]
-        self.model.contracted_vertices = self.vertex_optim.prune_optimizer(keep_mask)['contracted_vertices']
-        self.model.update_triangulation()
-
-    @torch.no_grad()
-    def split(self, clone_indices, split_point, split_mode, split_std, **kwargs):
-        device = self.model.device
-        clone_vertices = self.model.vertices[clone_indices]
-
-        if split_mode == "circumcenter":
-            circumcenters, radius = calculate_circumcenters_torch(clone_vertices)
-            radius = radius.reshape(-1, 1)
-            circumcenters = circumcenters.reshape(-1, 3)
-            sphere_loc = sample_uniform_in_sphere(circumcenters.shape[0], 3).to(device)
-            r = torch.randn((clone_indices.shape[0], 1), device=self.model.device)
-            r[r.abs() < 1e-2] = 1e-2
-            sampled_radius = (r * self.split_std + 1) * radius
-            new_vertex_location = l2_normalize_th(sphere_loc) * sampled_radius + circumcenters
-        elif split_mode == "barycenter":
-            barycentric_weights = 0.25*torch.ones((clone_indices.shape[0], clone_indices.shape[1], 1), device=device).clip(min=0.01, max=0.99)
-            new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights).sum(dim=1)
-        elif split_mode == "barycentric":
-            barycentric = torch.rand((clone_indices.shape[0], clone_indices.shape[1], 1), device=device).clip(min=0.01, max=0.99)
-            barycentric_weights = barycentric / (1e-3+barycentric.sum(dim=1, keepdim=True))
-            new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights).sum(dim=1)
-        elif split_mode == "split_point":
-            _, radius = calculate_circumcenters_torch(self.model.vertices[clone_indices])
-            split_point += (split_std * radius.reshape(-1, 1)).clip(min=1e-3, max=3) * torch.randn(*split_point.shape, device=self.model.device)
-            new_vertex_location = split_point
-            # new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights.unsqueeze(-1)).sum(dim=1)
-        elif split_mode == "split_point_c":
-            barycentric_weights = calc_barycentric(split_point, clone_vertices).clip(min=0)
-            barycentric_weights = barycentric_weights / (1e-3+barycentric_weights.sum(dim=1, keepdim=True))
-            barycentric_weights += 1e-4*torch.randn(*barycentric_weights.shape, device=self.model.device)
-            new_vertex_location = (self.model.vertices[clone_indices] * barycentric_weights.unsqueeze(-1)).sum(dim=1)
-        else:
-            raise Exception(f"Split mode: {split_mode} not supported")
-        self.add_points(new_vertex_location)
-
-    def main_step(self):
-        self.optim.step()
-        self.net_optim.step()
-
-    def main_zero_grad(self):
-        self.optim.zero_grad(set_to_none=True)
-        self.net_optim.zero_grad(set_to_none=True)
-
-    @property
-    def sh_optim(self):
-        return None
-
-    def regularizer(self, render_pkg):
-        weight_decay = self.weight_decay * sum([(embed.weight**2).mean() for embed in self.model.backbone.encoding.embeddings])
-
-        if self.lambda_density > 0 or self.lambda_tv > 0:
-            density = self.model.calc_tet_density()
-            density_loss = (self.model.calc_tet_area().detach() * density).sum()
-            if self.lambda_tv > 0:
-                diff  = density[self.pairs[:,0]] - density[self.pairs[:,1]]
-                tv_loss  = (self.face_area * diff.abs())
-                tv_loss  = tv_loss.sum() / self.face_area.sum()
-            else:
-                tv_loss = 0
-        else:
-            density_loss = 0
-            tv_loss = 0
-
-        return weight_decay + self.lambda_tv * tv_loss + self.lambda_density * density_loss
-
-    def update_triangulation(self, **kwargs):
-        self.model.update_triangulation(**kwargs)
-        if self.lambda_tv > 0:
-            self.build_tv()
-
-    def build_tv(self):
-        self.pairs, self.face_area = topo_utils.build_tv_struct(
-            self.model.vertices.detach(), self.model.indices, device='cuda')
-
-    def prune(self, diff_threshold, **kwargs):
-        if diff_threshold <= 0:
-            return
-        density = self.model.calc_tet_density()
-        self.build_tv()
-        diff  = density[self.pairs[:,0]] - density[self.pairs[:,1]]
-        tet_diff  = (self.face_area * diff.abs()).reshape(-1)
-
-        indices = self.model.indices.long()
-        device = indices.device
-        vert_diff = torch.zeros((self.model.vertices.shape[0],), device=device)
-
-        reduce_type = "amax"
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 0], src=tet_diff, reduce=reduce_type)
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 1], src=tet_diff, reduce=reduce_type)
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 2], src=tet_diff, reduce=reduce_type)
-        vert_diff.scatter_reduce_(dim=0, index=indices[..., 3], src=tet_diff, reduce=reduce_type)
-        keep_mask = vert_diff > diff_threshold
-        print(f"Pruned {(~keep_mask).sum()} points. VD: {vert_diff.mean()} TD: {tet_diff.mean()}")
-        self.remove_points(keep_mask)
-
diff --git a/models/vertex_model.py b/models/vertex_model.py
new file mode 100644
index 0000000..c0d5dad
--- /dev/null
+++ b/models/vertex_model.py
@@ -0,0 +1,97 @@
+import torch
+from torch import nn
+from utils.train_util import get_expon_lr_func
+
+class Model(nn.Module):
+    def __init__(self,
+                 vertices: torch.Tensor,
+                 indices: torch.Tensor,
+                 max_sh_deg: int = 2):
+        super().__init__()
+        self.sh_dim = ((1+max_sh_deg)**2-1)*3
+        self.register_buffer("vertices", vertices)          # immutable
+        self.register_buffer("indices", indices.int())
+
+        self.colors = nn.Parameter(0.5*torch.ones((len(self), 3)))
+        self.alpha = nn.Parameter(0.5*torch.ones((len(self), 1)))
+        self.sh_features = nn.Parameter(torch.zeros((len(self), self.sh_dim)))
+
+    def __len__(self):
+        return self.vertices.shape[0]
+
+class TetOptimizer:
+    def __init__(self,
+                 model: Model,
+                 alpha_lr: float = 1e-3,
+                 final_alpha_lr: float = 1e-4,
+                 color_lr: float = 1e-3,
+                 final_color_lr: float = 1e-4,
+                 sh_lr: float = 1e-3,
+                 final_sh_lr: float = 1e-4,
+                 freeze_start: int = 15000,
+                 iterations: int = 30000,
+                 **kwargs
+    ) -> None:
+        self.model = model
+        self.alpha_optim = torch.optim.Adam([
+            {"params": [model.colors],  "lr": color_lr,  "name": "colors"},
+        ])
+        self.color_optim = torch.optim.Adam([
+            {"params": [model.alpha],      "lr": alpha_lr,    "name": "alpha"},
+        ])
+        self.sh_optim = torch.optim.Adam([
+            {"params": [model.sh_features], "lr": sh_lr, "name": "sh"},
+        ])
+        self.freeze_start = freeze_start
+        self.alpha_scheduler = get_expon_lr_func(
+            lr_init=alpha_lr,
+            lr_final=final_alpha_lr,
+            lr_delay_mult=1,
+            max_steps=iterations - self.freeze_start,
+            lr_delay_steps=0)
+        self.color_scheduler = get_expon_lr_func(
+            lr_init=color_lr,
+            lr_final=final_color_lr,
+            lr_delay_mult=1,
+            max_steps=iterations - self.freeze_start,
+            lr_delay_steps=0)
+        self.sh_scheduler = get_expon_lr_func(
+            lr_init=sh_lr,
+            lr_final=final_sh_lr,
+            lr_delay_mult=1,
+            max_steps=iterations - self.freeze_start,
+            lr_delay_steps=0)
+        self.vertex_optim = None  # geometry is frozen
+
+    def update_triangulation(self, *_, **__):
+        return None
+
+    def step(self):
+        self.alpha_optim.step()
+        self.color_optim.step()
+        self.sh_optim.step()
+
+    def zero_grad(self):
+        self.alpha_optim.zero_grad()
+        self.color_optim.zero_grad()
+        self.sh_optim.zero_grad()
+
+    def main_step(self):
+        self.step()
+
+    def main_zero_grad(self):
+        self.zero_grad()
+
+
+    def update_learning_rate(self, iteration):
+        ''' Learning rate scheduling per step '''
+        self.iteration = iteration
+        for param_group in self.alpha_optim.param_groups:
+            lr = self.alpha_scheduler(iteration - self.freeze_start)
+            param_group['lr'] = lr
+        for param_group in self.color_optim.param_groups:
+            lr = self.color_scheduler(iteration - self.freeze_start)
+            param_group['lr'] = lr
+        for param_group in self.sh_optim.param_groups:
+            lr = self.sh_scheduler(iteration - self.freeze_start)
+            param_group['lr'] = lr
diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000..8ad0ba8
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,33 @@
+[project]
+name = "dsplat"
+version = "0.1.0"
+description = "Delaunay splatting boooii"
+readme = "README.md"
+requires-python = ">=3.10"
+dependencies = [
+    "fused-ssim",
+    "gdel3d",
+    "icecream>=2.1.5",
+    "imageio>=2.37.0",
+    "jax[cuda12]>=0.5.3",
+    "matplotlib>=3.10.3",
+    "mediapy>=1.2.4",
+    "muon-optimizer",
+    "nvdiffrast",
+    "opencv-contrib-python>=4.12.0.88",
+    "plyfile>=1.1.2",
+    "pybind11>=3.0.0",
+    "slangtorch>=1.3.11",
+    "tinyplypy",
+    "torch>=2.7.1",
+    "torchaudio>=2.7.1",
+    "torchvision>=0.22.1",
+    "tqdm>=4.67.1",
+]
+
+[tool.uv.sources]
+tinyplypy = { git = "https://github.com/half-potato/tinyplypy" }
+muon-optimizer = { git = "https://github.com/KellerJordan/Muon" }
+fused-ssim = { git = "https://github.com/rahul-goel/fused-ssim/" }
+gdel3d = { git = "https://github.com/half-potato/pyGDel3D.git" }
+nvdiffrast = { git = "https://github.com/NVlabs/nvdiffrast" }
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..054a90c
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,5 @@
+slangtorch
+opencv-python
+tqdm
+scipy
+jax
diff --git a/results/ablations_in.csv b/results/ablations_in.csv
new file mode 100644
index 0000000..f81166d
--- /dev/null
+++ b/results/ablations_in.csv
@@ -0,0 +1,13 @@
+dataset_path	image_folder	lambda_cost	total_thresh	within_thresh		
+/data/nerf_datasets/360/bicycle	images_4	1.00E-03	1000	0.4		
+/data/nerf_datasets/360/garden	images_4	1.00E-03	1000	0.4		
+/data/nerf_datasets/360/room	images_2	1.00E-03	1000	0.4		
+/data/nerf_datasets/360/bicycle	images_4	1.00E-03	0.025	1000		
+/data/nerf_datasets/360/garden	images_4	1.00E-03	0.025	1000		
+/data/nerf_datasets/360/room	images_2	1.00E-03	0.025	1000		
+/data/nerf_datasets/360/bicycle	images_4	1.00E-03	0.025	0.4	--ablate_gradient	
+/data/nerf_datasets/360/garden	images_4	1.00E-03	0.025	0.4	--ablate_gradient	
+/data/nerf_datasets/360/room	images_2	1.00E-03	0.025	0.4	--ablate_gradient	
+/data/nerf_datasets/360/bicycle	images_4	1.00E-03	0.025	0.4		--ablate_circumsphere
+/data/nerf_datasets/360/garden	images_4	1.00E-03	0.025	0.4		--ablate_circumsphere
+/data/nerf_datasets/360/room	images_2	1.00E-03	0.025	0.4		--ablate_circumsphere
diff --git a/results/ablations_out.csv b/results/ablations_out.csv
new file mode 100644
index 0000000..fcfd28a
--- /dev/null
+++ b/results/ablations_out.csv
@@ -0,0 +1,4 @@
+,dataset_path,gpu_id,image_folder,lambda_cost,output_folder,psnr,test_LPIPS,test_PSNR,test_SSIM,total_thresh,within_thresh
+,/data/nerf_datasets/360/bicycle,0,images_4,0,output/bicycle_ifimages_4_lc0_tt0.025_wt0.4,23.368182385787286,0.27138441801071167,24.632314682006836,0.7436779737472534,0.025,0.4
+,/data/nerf_datasets/360/garden,0,images_4,0,output/garden_ifimages_4_lc0_tt0.025_wt0.4,27.0262312530957,0.15084882080554962,26.696916580200195,0.8459108471870422,0.025,0.4
+,/data/nerf_datasets/360/room,0,images_2,0,output/room_ifimages_2_lc0_tt0.025_wt0.4,31.993436158652077,0.27150964736938477,30.59916877746582,0.920693039894104,0.025,0.4
diff --git a/results/ablations_out2.csv b/results/ablations_out2.csv
new file mode 100644
index 0000000..52442e6
--- /dev/null
+++ b/results/ablations_out2.csv
@@ -0,0 +1,10 @@
+,dataset_path,error,gpu_id,image_folder,lambda_cost,output_folder,total_thresh,within_thresh
+,/optane/nerf_datasets/360/bicycle,Process failed and no JSON output file found,0,images_4,1.00E-03,output/bicycle_ifimages_4_lc1.00E-03_tt1000_wt0.4,1000,0.4
+,/optane/nerf_datasets/360/garden,Process failed and no JSON output file found,0,images_4,1.00E-03,output/garden_ifimages_4_lc1.00E-03_tt1000_wt0.4,1000,0.4
+,/optane/nerf_datasets/360/room,Process failed and no JSON output file found,0,images_2,1.00E-03,output/room_ifimages_2_lc1.00E-03_tt1000_wt0.4,1000,0.4
+,/optane/nerf_datasets/360/bicycle,Process failed and no JSON output file found,0,images_4,1.00E-03,output/bicycle_ifimages_4_lc1.00E-03_tt0.025_wt1000,0.025,1000
+,/optane/nerf_datasets/360/garden,Process failed and no JSON output file found,0,images_4,1.00E-03,output/garden_ifimages_4_lc1.00E-03_tt0.025_wt1000,0.025,1000
+,/optane/nerf_datasets/360/room,Process failed and no JSON output file found,0,images_2,1.00E-03,output/room_ifimages_2_lc1.00E-03_tt0.025_wt1000,0.025,1000
+,/optane/nerf_datasets/360/bicycle,Process failed and no JSON output file found,0,images_4,1.00E-03,output/bicycle_ifimages_4_lc1.00E-03_tt0.025_wt0.4,0.025,0.4
+,/optane/nerf_datasets/360/garden,Process failed and no JSON output file found,0,images_4,1.00E-03,output/garden_ifimages_4_lc1.00E-03_tt0.025_wt0.4,0.025,0.4
+,/optane/nerf_datasets/360/room,Process failed and no JSON output file found,0,images_2,1.00E-03,output/room_ifimages_2_lc1.00E-03_tt0.025_wt0.4,0.025,0.4
diff --git a/sh_slang/eval_sh.py b/sh_slang/eval_sh.py
index 16df80b..589b024 100644
--- a/sh_slang/eval_sh.py
+++ b/sh_slang/eval_sh.py
@@ -42,6 +42,10 @@ class EvalSH(Function):
         sh0 = sh0.contiguous()
         features = features.contiguous()
         color = torch.zeros(means.shape, device=means.device, dtype=features.dtype)
+        assert(means.dtype == torch.float32)
+        assert(sh0.dtype == torch.float32)
+        assert(features.dtype == torch.float32)
+        assert(rayo.dtype == torch.float32)
         assert(means.shape[0] == sh0.shape[0])
         assert(features.shape[0] == sh0.shape[0])
         sh_dim = ((sh_degree+1)**2-1)
diff --git a/sh_slang/slang/safe-math.slang b/sh_slang/slang/safe-math.slang
index e18a4eb..7716bdc 100644
--- a/sh_slang/slang/safe-math.slang
+++ b/sh_slang/slang/safe-math.slang
@@ -314,3 +314,15 @@ float4 half2float(half4 v) {
         half2float(v.w)
     };
 }
+
+[Differentiable]
+float tukey_power_ladder(float x, float p) {
+  // Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)
+  if (p == 1) {
+    return x;
+  }
+  let xp = abs(x);
+  let xs = xp / max(TINY_VAL, abs(p - 1));
+  let y = sign(x) * abs(p - 1) / p * (pow(xs + 1, p) - 1);
+  return y;
+}
diff --git a/sh_slang/slang/sh.slang b/sh_slang/slang/sh.slang
index de9d0e8..801724b 100644
--- a/sh_slang/slang/sh.slang
+++ b/sh_slang/slang/sh.slang
@@ -12,7 +12,6 @@
 // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 // See the License for the specific language governing permissions and
 // limitations under the License.
-import safe_math;
 
 // Spherical harmonics coefficients
 static const float SH_C0 = 0.28209479177387814f;
diff --git a/sh_slang/slang/sh_kernel.slang b/sh_slang/slang/sh_kernel.slang
index 305c45b..92c1241 100644
--- a/sh_slang/slang/sh_kernel.slang
+++ b/sh_slang/slang/sh_kernel.slang
@@ -12,7 +12,6 @@
 // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 // See the License for the specific language governing permissions and
 // limitations under the License.
-import safe_math;
 import tensor_util;
 import sh;
 /*
diff --git a/tests/multi_tet_test.py b/tests/multi_tet_test.py
index b387535..a496800 100644
--- a/tests/multi_tet_test.py
+++ b/tests/multi_tet_test.py
@@ -45,8 +45,8 @@ def create_view_matrix(camera_pos, look_at_point):
 class DelaunayRenderTest(parameterized.TestCase):
     def setUp(self):
         torch.manual_seed(189710234)
-        self.height = 16
-        self.width = 16
+        self.height = 32
+        self.width = 32
         
     def run_test(self, points, viewmat, tile_size):
         """Run rendering test with different sample counts and compare results."""
@@ -81,7 +81,7 @@ class DelaunayRenderTest(parameterized.TestCase):
     @parameterized.product(
         n_points=[10, 20],
         radius=[10, 100],
-        tile_size=[8]
+        tile_size=[16]
     )
     def test_center_view(self, n_points, radius, tile_size, N=5):
         """Test rendering from center of point cloud with random rotation."""
@@ -105,7 +105,7 @@ class DelaunayRenderTest(parameterized.TestCase):
         n_points=[10, 20],
         radius=[10, 100],
         offset_mag=[1, 10, 100],
-        tile_size=[8]
+        tile_size=[16]
     )
     def test_outside_view(self, n_points, radius, offset_mag, tile_size, N=5):
         """Test rendering from outside the point cloud looking in."""
diff --git a/train.py b/train.py
index 4d84285..4fe9622 100644
--- a/train.py
+++ b/train.py
@@ -17,85 +17,76 @@ from tqdm import tqdm
 import numpy as np
 from utils import cam_util
 from utils.train_util import *
+from delaunay_rasterization import render
 # from models.vertex_color import Model, TetOptimizer
 from models.ingp_color import Model, TetOptimizer
 # from models.ingp_linear import Model, TetOptimizer
-from models.frozen import freeze_model
 from fused_ssim import fused_ssim
 from pathlib import Path, PosixPath
 from utils.args import Args
-import json
-import imageio
 from utils import test_util
-import termplotlib as tpl
 from utils.lib_bilagrid import BilateralGrid, total_variation_loss, slice
 from torch.optim.lr_scheduler import ExponentialLR, LinearLR, ChainedScheduler
 import gc
-from utils.densification import collect_render_stats, apply_densification
+from utils.densification import collect_render_stats, apply_densification, determine_cull_mask
 import mediapy
+from torch import nn
 
-torch.set_num_threads(1)
 
-class CustomEncoder(json.JSONEncoder):
-    def default(self, o):
-        if isinstance(o, PosixPath):
-            return str(o)
-        return super().default(o)
-
-class SimpleSampler:
-    def __init__(self, total_num_samples, batch_size):
-        self.total_num_samples = total_num_samples
-        self.batch_size = batch_size
-        self.curr = total_num_samples
-        self.ids = None
-
-    def nextids(self, batch_size=None):
-        batch_size = self.batch_size if batch_size is None else batch_size
-        self.curr += batch_size
-        if self.curr + batch_size > self.total_num_samples:
-            # self.ids = torch.LongTensor(np.random.permutation(self.total_num_samples))
-            self.ids = torch.randperm(self.total_num_samples, dtype=torch.long, device=device)
-            self.curr = 0
-        ids = self.ids[self.curr : self.curr + batch_size]
-        return ids
+torch.set_num_threads(1)
 
 eps = torch.finfo(torch.float).eps
 args = Args()
-args.tile_size = 4
+args.tile_size = 16
 args.image_folder = "images_4"
 args.eval = False
-args.dataset_path = Path("/data/nerf_datasets/360/bicycle")
+args.dataset_path = Path("/optane/nerf_datasets/360/garden")
 args.output_path = Path("output/test/")
 args.iterations = 30000
 args.ckpt = ""
 args.render_train = False
+args.delaunay_interval = 10
+args.orient_scene = True
+args.freeze_features = True
 
 # Light Settings
 args.max_sh_deg = 3
-args.sh_interval = 0
+args.sh_interval = 2000
 args.sh_step = 1
+args.bake_model = True
+
+args.glo_dim = 0
+args.glo_lr = 1e-3
+args.glo_network_lr = 5e-5
+args.glo_weight_decay = 1e-1
+args.glo_net_decay = 1e-6
 
 # iNGP Settings
+args.base_resolution = 64
 args.encoding_lr = 3e-3
 args.final_encoding_lr = 3e-4
 args.network_lr = 1e-3
 args.final_network_lr = 1e-4
-args.hidden_dim = 64
 args.scale_multi = 0.35 # chosen such that 96% of the distribution is within the sphere 
-args.log2_hashmap_size = 22
+args.log2_hashmap_size = 23
 args.per_level_scale = 2
-args.L = 10
-args.density_offset = -4
-args.weight_decay = 0.01
-args.hashmap_dim = 16
-args.percent_alpha = 0.02 # preconditioning
-args.spike_duration = 500
-
-args.dg_init=1e-4
-args.g_init=1.0
-args.s_init=1e-4
+args.L = 6
+args.density_offset = -3
+args.weight_decay = 0.1
+args.final_weight_decay = 0.1
+args.hashmap_dim = 8
+args.percent_alpha = 0.04 # preconditioning
+args.spike_duration = 0
+args.hidden_dim = 64
+args.sh_hidden_dim = 256
+args.sh_weight_decay = 1e-5
+args.sh_lr_div = 1
+
+args.dg_init=0.1
+args.g_init=0.1
+args.s_init=0.1
 args.d_init=0.1
-args.c_init=0.8
+args.c_init=0.1
 
 # Vertex Settings
 args.lr_delay = 0
@@ -103,79 +94,104 @@ args.vert_lr_delay = 0
 args.vertices_lr = 1e-4
 args.final_vertices_lr = 1e-6
 args.vertices_lr_delay_multi = 1e-8
-args.vertices_beta = [0.9, 0.99]
-args.contract_vertices = False
 args.clip_multi = 0
 args.delaunay_start = 30000
 
-args.freeze_start = 22500
-args.freeze_lr = 1e-3
+args.freeze_start = 16000
+args.freeze_lr = 5e-3
 args.final_freeze_lr = 1e-4
+args.feature_lr = 1e-3
+args.final_feature_lr = 1e-4
+args.fnetwork_lr = 1e-3
+args.final_fnetwork_lr = 1e-4
 
 # Distortion Settings
 args.lambda_dist = 1e-4
+args.lambda_density = 0.0
+args.lambda_cost = 0.0
+args.lambda_color = 0.0
 
 # Clone Settings
 args.num_samples = 200
-args.clone_lambda_ssim = 0.0
 args.split_std = 1e-1
-args.split_mode = "split_point"
-args.clone_schedule = "quadratic"
 args.min_tet_count = 9
 args.densify_start = 2000
-args.densify_end = 20000
+args.densify_end = 16000
 args.densify_interval = 500
 args.budget = 2_000_000
-args.clone_velocity = 0.0
-args.speed_mul = 10
-args.percent_within = 0.70
-args.percent_total = 0.30
-args.diff_threshold = 0.0
 args.clone_min_alpha = 0.025
 args.clone_min_density = 0.025
-args.clone_min_ratio = 1.0
 
+args.lambda_alpha = 1e-4
 args.lambda_ssim = 0.2
 args.base_min_t = 0.2
-args.sample_cam = 8
+args.sample_cam = 1
 args.data_device = 'cpu'
 args.lambda_tv = 0.0
-args.density_threshold = 0.001
-args.alpha_threshold = 0.001
-
+args.contrib_threshold = 0.01
+args.density_threshold = 0.1
+args.alpha_threshold = 0.1
+args.total_thresh = 0.1#025
+args.within_thresh = 0.4
+args.density_intercept = 0.2
 args.voxel_size = 0.01
+args.start_threshold = 10000000
+args.ext_convex_hull = True
 
 args.use_bilateral_grid = False
 args.bilateral_grid_shape = [16, 16, 8]
 args.bilateral_grid_lr = 0.003
 args.lambda_tv_grid = 0.0
-
+args.record_training = False
 args.checkpoint_iterations = []
 
-args = Args.from_namespace(args.get_parser().parse_args())
+args.ablate_gradient = False
+args.ablate_circumsphere = False
+
+parser = args.get_parser()
+args = Args.from_namespace(parser.parse_args())
+
+# if a ckpt is loaded, load config, then override config with user specified flags
+if len(args.ckpt) > 0: 
+    config_path = Path(args.ckpt) / "config.json"
+    config = Args.load_from_json(str(config_path))
+    parser.set_defaults(**config.as_dict())
+args = Args.from_namespace(parser.parse_args())
 
 args.output_path.mkdir(exist_ok=True, parents=True)
-# args.checkpoint_iterations.append(args.freeze_start-1)
+args.checkpoint_iterations = [int(i) for i in args.checkpoint_iterations]
+print(args.checkpoint_iterations)
 
 train_cameras, test_cameras, scene_info = loader.load_dataset(
     args.dataset_path, args.image_folder, data_device=args.data_device, eval=args.eval)
 
+np.savetxt(str(args.output_path / "transform.txt"), scene_info.transform)
 
 args.num_samples = min(len(train_cameras), args.num_samples)
 
 with (args.output_path / "config.json").open("w") as f:
     json.dump(args.as_dict(), f, cls=CustomEncoder)
 
+final_iter = args.freeze_start if args.bake_model else args.iterations
 device = torch.device('cuda')
 if len(args.ckpt) > 0: 
-    model = Model.load_ckpt(Path(args.ckpt), device)
+    try:
+        model = Model.load_ckpt(Path(args.ckpt), device, args)
+        tet_optim = TetOptimizer(model, final_iter=final_iter, **args.as_dict())
+    except:
+        if args.freeze_features:
+            from models.frozen_features import FrozenTetModel, FrozenTetOptimizer
+        else:
+            from models.frozen import FrozenTetModel, FrozenTetOptimizer
+        model = FrozenTetModel.load_ckpt(Path(args.ckpt), device)
+        tet_optim = FrozenTetOptimizer(model, final_iter=final_iter, **args.as_dict())
 else:
     model = Model.init_from_pcd(scene_info.point_cloud, train_cameras, device,
                                 current_sh_deg = args.max_sh_deg if args.sh_interval <= 0 else 0,
                                 **args.as_dict())
-min_t = args.min_t = args.base_min_t * model.scene_scaling.item()
+    tet_optim = TetOptimizer(model, final_iter=final_iter, **args.as_dict())
 
-tet_optim = TetOptimizer(model, **args.as_dict())
+min_t = args.min_t = args.base_min_t# * model.scene_scaling.item()
 if args.eval:
     sample_camera = test_cameras[args.sample_cam]
     # sample_camera = train_cameras[args.sample_cam]
@@ -196,69 +212,17 @@ num_densify_iter = args.densify_end - args.densify_start
 N = num_densify_iter // args.densify_interval + 1
 S = model.vertices.shape[0]
 
-def target_num(x):
-    if args.clone_schedule == "linear":
-        k = (args.budget - S) // N
-        return k * x + S
-    elif args.clone_schedule == "quadratic":
-        k = 2 * (args.budget - S) // N
-        a = (args.budget - S - k * N) // N**2
-        return a * x**2 + k * x + S
-    else:
-        raise Exception(f"Clone Schedule: {args.clone_schedule} is not supported")
-
-# ----------------------------------------------------------------
-# new helper: frontloaded (highfrequencythenslowdown) schedule
-def densify_schedule(start: int,
-                     end: int,
-                     n_events: int,
-                     mode: str = "sqrt"):
-    """
-    Generate `n_events` iteration indices between `start` and `end`
-    with decreasing frequency.  Modes:
-         'sqrt'     spacing  t   (simple, monotone)
-         'exp'      exponential easing
-         'logistic' Scurve
-    """
-    t = np.linspace(0.0, 1.0, n_events)
-    if mode == "linear":
-        w = t
-    elif mode == "sqrt":
-        w = t**2                         # lots of points early, sparse later
-    elif mode == "exp":
-        g = 4.0
-        w = (np.exp(g*t) - 1) / (np.exp(g) - 1)
-    elif mode == "logistic":
-        k = 10.0
-        w = 1 / (1 + np.exp(-k*(t-0.5)))
-        w = (w - w.min()) / (w.max() - w.min())
-    else:
-        raise ValueError("mode must be 'sqrt', 'exp', or 'logistic'")
-    iters = np.round(start + w * (end - start)).astype(int)
-    iters[0] = start                     # make sure start & end are included
-    iters[-1] = end
-    return list(np.unique(iters))
-
-# dschedule = densify_schedule(args.densify_start,
-#                             args.densify_end,
-#                             N,
-#                             mode="linear")
 dschedule = list(range(args.densify_start, args.densify_end, args.densify_interval))
-targets = [target_num((i - args.densify_start) / num_densify_iter * N+1) for i in dschedule]
-fig = tpl.figure()
-fig.plot(dschedule, targets, width=100, height=20)
-fig.show()
-
-print("Encoding LR")
-xs = list(range(args.iterations))
-ys = [tet_optim.encoder_scheduler_args(x) for x in xs]
-fig = tpl.figure()
-fig.plot(xs, ys, width=150, height=20)
-fig.show()
+
+# print("Encoding LR")
+# xs = list(range(args.iterations))
+# ys = [tet_optim.encoder_scheduler_args(x) for x in xs]
+# fig = tpl.figure()
+# fig.plot(xs, ys, width=150, height=20)
+# fig.show()
 
 densification_sampler = SimpleSampler(len(train_cameras), args.num_samples)
 
-# ----- Initialize bilateral grid if enabled -----
 bil_grids = None
 bil_optimizer = None
 if args.use_bilateral_grid:
@@ -274,9 +238,6 @@ if args.use_bilateral_grid:
     ).to("cuda")
     bil_optimizer = torch.optim.Adam([bil_grids.grids], lr=args.bilateral_grid_lr, eps=1e-15)
     
-    # Create a chained scheduler with warmup like in gsplat
-    # First 1000 iterations: linear warmup from 1% to 100% of learning rate
-    # Then exponential decay to 1% of initial learning rate by the end of training
     bil_warmup = LinearLR(bil_optimizer, start_factor=0.01, total_iters=1000)
     bil_decay = ExponentialLR(bil_optimizer, gamma=0.01**(1.0/args.iterations))
     bil_scheduler = ChainedScheduler([bil_warmup, bil_decay])
@@ -284,28 +245,62 @@ if args.use_bilateral_grid:
     print(f"- Number of grids: {len(train_cameras)}")
     print("- Using LinearLR warmup + ExponentialLR decay scheduler")
     print("Bilateral Grid initialized successfully!\n")
-# ------------------------------------------------
 
-video_writer = cv2.VideoWriter(str(args.output_path / "training.mp4"), cv2.CAP_FFMPEG, cv2.VideoWriter_fourcc(*'avc1'), 30,
+glo_list = lambda x: None
+glo_optim = None
+if args.glo_dim > 0:
+    glo_list = nn.Embedding(len(train_cameras), args.glo_dim)
+    with torch.no_grad():
+        glo_list.weight *= 0
+    glo_list = glo_list.cuda()
+    glo_optim = torch.optim.Adam(glo_list.parameters(), lr=args.glo_lr)
+
+if args.record_training:
+    video_writer = cv2.VideoWriter(str(args.output_path / "training.mp4"), cv2.VideoWriter_fourcc(*'mp4v'), 30,
                                pad_hw2even(sample_camera.image_width, sample_camera.image_height))
 
-tet_optim.build_tv()
 progress_bar = tqdm(range(args.iterations))
 torch.cuda.empty_cache()
+
+weight_decay_fn = get_expon_lr_func(
+    lr_init=args.weight_decay,
+    lr_final=args.final_weight_decay,
+    lr_delay_mult=1e-8,
+    lr_delay_steps=0,
+    max_steps=args.freeze_start)
+
+densification_cam_buffer = []
+
 for iteration in progress_bar:
-    delaunay_interval = 10 if iteration < args.delaunay_start else 100
+    delaunay_interval = args.delaunay_interval if iteration < args.delaunay_start else 100
     do_delaunay = iteration % delaunay_interval == 0 and iteration < args.freeze_start
     do_freeze = iteration == args.freeze_start
-    do_cloning = iteration in dschedule
+    do_cloning = iteration in dschedule and iteration < args.freeze_start
     do_sh_up = not args.sh_interval == 0 and iteration % args.sh_interval == 0 and iteration > 0
     do_sh_step = iteration % args.sh_step == 0
 
     if do_delaunay or do_freeze:
         st = time.time()
-        tet_optim.update_triangulation(density_threshold=args.density_threshold, alpha_threshold=args.alpha_threshold, high_precision=do_freeze)
-        if do_freeze:
+        dt = args.density_threshold if iteration > args.start_threshold else 0
+        at = args.alpha_threshold if iteration > args.start_threshold else 0
+        tet_optim.update_triangulation(
+            density_threshold=dt, alpha_threshold=at, high_precision=do_freeze)
+        if do_freeze and args.bake_model and not model.frozen:
+            # model.save2ply(args.output_path / "ckpt_prefreeze.ply")
+            sd = model.state_dict()
+            sd['indices'] = model.indices
+            torch.save(sd, args.output_path / "ckpt_prefreeze.pth")
+            if args.freeze_features:
+                from models.frozen_features import freeze_model
+            else:
+                from models.frozen import freeze_model
             del tet_optim
-            model, tet_optim = freeze_model(model, **args.as_dict())
+            model.eval()
+            mask = determine_cull_mask(train_cameras, model, glo_list, args, device)
+            model.train()
+            print(f"Kept {mask.sum()} tets")
+            model, tet_optim = freeze_model(model, mask, args)
+            del mask
             gc.collect()
             torch.cuda.empty_cache()
 
@@ -314,12 +309,15 @@ for iteration in progress_bar:
         random.shuffle(inds)
         psnrs.append([])
     ind = inds.pop()
+    densification_cam_buffer.append(ind)
     camera = train_cameras[ind]
     target = camera.original_image.cuda()
 
     st = time.time()
     ray_jitter = torch.rand((camera.image_height, camera.image_width, 2), device=device)
-    render_pkg = render(camera, model, scene_scaling=model.scene_scaling, ray_jitter=ray_jitter, **args.as_dict())
+    tid = torch.LongTensor([camera.uid]).cuda()
+    render_pkg = render(camera, model, scene_scaling=model.scene_scaling,
+                        ray_jitter=ray_jitter, glo=glo_list(tid), **args.as_dict())
     image = render_pkg['render']
 
     if args.use_bilateral_grid:
@@ -339,18 +337,37 @@ for iteration in progress_bar:
 
     l1_loss = (target - image).abs().mean()
     l2_loss = ((target - image)**2).mean()
-    reg = tet_optim.regularizer(render_pkg)
+    reg = tet_optim.regularizer(render_pkg, weight_decay_fn(iteration), args.lambda_tv)
     ssim_loss = (1-fused_ssim(image.unsqueeze(0), target.unsqueeze(0))).clip(min=0, max=1)
     dl_loss = render_pkg['distortion_loss']
+    a = args.density_intercept
+    mask = render_pkg['mask']
+    area = topo_utils.tet_surface_areas(model.vertices[model.indices])
+    density = render_pkg['density'].reshape(-1)
+
+    # density_loss = ((-(density - a)**2 / a**2 + 1).clip(min=0) * area)[mask].mean()
+    density_loss = density[mask].mean()
+    lambda_dist = args.lambda_dist if iteration > 5000 else 0
+    lambda_density = lambda_dist * args.lambda_density if iteration > 5000 else 0
+    # area = topo_utils.tet_volumes(model.vertices[model.indices])
+    render_cost = (density.clip(max=2*args.density_threshold) * area.reshape(-1))[mask].mean()
+    lambda_alpha = args.lambda_alpha if iteration > 5000 else 0
     loss = (1-args.lambda_ssim)*l1_loss + \
            args.lambda_ssim*ssim_loss + \
            reg + \
-           args.lambda_dist * dl_loss
+           lambda_dist * dl_loss + \
+           lambda_density * density_loss + \
+           lambda_alpha * (1-render_pkg['alpha']).mean() + \
+           args.lambda_color * (render_pkg['color']).abs().mean() + \
+           args.lambda_cost * render_cost
 
     if args.use_bilateral_grid:
         tvloss = args.lambda_tv_grid * total_variation_loss(bil_grids.grids)
         loss += tvloss
 
+    if args.glo_dim > 0:
+        loss += args.glo_weight_decay * (glo_list.weight**2).mean()
+
     mask = render_pkg['mask']
     st = time.time()
 
@@ -367,6 +384,10 @@ for iteration in progress_bar:
         tet_optim.vertex_optim.step()
         tet_optim.vertex_optim.zero_grad()
 
+    if glo_optim:
+        glo_optim.step()
+        glo_optim.zero_grad()
+
     if args.use_bilateral_grid:
         bil_optimizer.step()
         bil_optimizer.zero_grad(set_to_none=True)
@@ -374,11 +395,8 @@ for iteration in progress_bar:
 
     tet_optim.update_learning_rate(iteration)
 
-    if do_sh_up:
-        model.sh_up()
-
     with torch.no_grad():
-        if iteration % 10 == 0:
+        if iteration % 10 == 0 and args.record_training:
             render_pkg = render(sample_camera, model, min_t=min_t, tile_size=args.tile_size)
             sample_image = render_pkg['render']
             sample_image = sample_image.permute(1, 2, 0)
@@ -388,12 +406,17 @@ for iteration in progress_bar:
 
     if do_cloning and not model.frozen:
         with torch.no_grad():
-            sampled_cams = [train_cameras[i] for i in densification_sampler.nextids()]
+            # sampled_cams = [train_cameras[i] for i in densification_sampler.nextids()]
+            sampled_cams = [train_cameras[i] for i in np.unique(densification_cam_buffer)]
 
             model.eval()
-            stats = collect_render_stats(sampled_cams, model, args, device)
+            stats = collect_render_stats(sampled_cams, model, glo_list, args, device)
             model.train()
-            target_addition = targets[dschedule.index(iteration)] - model.vertices.shape[0]
+            render_pkg = render(sample_camera, model, min_t=min_t, tile_size=args.tile_size)
+            sample_image = render_pkg['render']
+            sample_image = sample_image.permute(1, 2, 0)
+            sample_image = (sample_image.detach().cpu().numpy()*255).clip(min=0, max=255).astype(np.uint8)
+            sample_image = cv2.cvtColor(sample_image, cv2.COLOR_RGB2BGR)
 
             apply_densification(
                 stats,
@@ -404,25 +427,19 @@ for iteration in progress_bar:
                 device      = device,
                 sample_cam  = sample_camera,
                 sample_image= sample_image,     # whatever RGB debug frame you use
-                target_addition= target_addition
-
+                budget      = max(args.budget - model.vertices.shape[0], 0)
             )
-            # tet_optim.prune(**args.as_dict())
             gc.collect()
             torch.cuda.empty_cache()
+            densification_cam_buffer = []
 
     # Save checkpoints at specified iterations
     if iteration in args.checkpoint_iterations:
-        checkpoint_dir = args.output_path / f"checkpoint_{iteration}"
-        checkpoint_dir.mkdir(exist_ok=True, parents=True)
-        model.save2ply(checkpoint_dir / "ckpt.ply")
-        sd = model.state_dict()
-        sd['indices'] = model.indices
-        torch.save(sd, checkpoint_dir / "ckpt.pth")
+        model.save2ply(args.output_path / f"ckpt_{iteration}.ply")
         print(f"Saved checkpoint at iteration {iteration}")
 
-        with (checkpoint_dir / "config.json").open("w") as f:
-            json.dump(args.as_dict(), f, cls=CustomEncoder)
+    if do_sh_up:
+        model.sh_up()
 
     psnr = 20 * math.log10(1.0 / math.sqrt(l2_loss.detach().cpu().item()))
     psnrs[-1].append(psnr)
@@ -435,10 +452,14 @@ for iteration in progress_bar:
         "#V": len(model),
         "#T": model.indices.shape[0],
         "DL": repr(f"{dl_loss:>5.2f}"),
+        "Density": repr(f"{density_loss.item():.3f}")
     })
 
 avged_psnrs = [sum(v)/len(v) for v in psnrs if len(v) == len(train_cameras)]
-video_writer.release()
+if args.record_training:
+    video_writer.release()
+
+model.save2ply(args.output_path / "ckpt.ply")
 
 torch.cuda.synchronize()
 torch.cuda.empty_cache()
@@ -457,17 +478,22 @@ with (args.output_path / "results.json").open("w") as f:
     json.dump(all_data, f, cls=CustomEncoder)
 
 with torch.no_grad():
+    if args.glo_dim > 0:
+        mean_glo = glo_list.weight.data.mean(dim=0)
+    else:
+        mean_glo = None
     epath = cam_util.generate_cam_path(train_cameras, 400)
     eimages = []
     for camera in tqdm(epath):
-        render_pkg = render(camera, model, min_t=min_t, tile_size=args.tile_size)
+        render_pkg = render(camera, model, glo=mean_glo, min_t=min_t, tile_size=args.tile_size)
         image = render_pkg['render']
         image = image.permute(1, 2, 0)
         image = image.detach().cpu().numpy()
         eimages.append(pad_image2even(image))
 mediapy.write_video(args.output_path / "rotating.mp4", eimages)
 
-model.save2ply(args.output_path / "ckpt.ply")
 sd = model.state_dict()
 sd['indices'] = model.indices
 torch.save(sd, args.output_path / "ckpt.pth")
+if args.glo_dim > 0:
+    torch.save(glo_list.state_dict(), args.output_path / "glo.pth")
diff --git a/utils/args.py b/utils/args.py
index c4482f7..bc10265 100644
--- a/utils/args.py
+++ b/utils/args.py
@@ -6,6 +6,12 @@ class Args:
     def __init__(self):
         self._data = {}
 
+    def __setitem__(self, key, value):
+        if key == "_data":
+            super().__setattr__(key, value)
+        else:
+            self._data[key] = value
+
     def __setattr__(self, key, value):
         if key == "_data":
             super().__setattr__(key, value)
@@ -25,19 +31,30 @@ class Args:
         """Generate an ArgumentParser with stored values as defaults."""
         parser = ArgumentParser(description="Argument parser for the script")
         for key, value in self._data.items():
-            arg_type = type(value)
-            if isinstance(value, bool):  # Special handling for boolean flags
-                parser.add_argument(f"--{key}", action="store_true" if not value else "store_false")
+            if isinstance(value, bool):
+                # Special handling for boolean flags
+                parser.add_argument(
+                    f"--{key}",
+                    action="store_true" if not value else "store_false",
+                    default=value
+                )
+            elif isinstance(value, list):
+                # --- NEW: Handling for list arguments ---
+                parser.add_argument(
+                    f"--{key}",
+                    nargs='+', # Accepts one or more arguments
+                    default=value
+                )
             else:
-                parser.add_argument(f"--{key}", type=arg_type, default=value)
+                # Handling for other types (int, float, str, etc.)
+                parser.add_argument(f"--{key}", type=type(value), default=value)
         return parser
 
     @classmethod
     def from_namespace(cls, namespace: Namespace):
         """Convert a parsed Namespace back into an Args object."""
         obj = cls()
-        for key, value in vars(namespace).items():
-            obj._data[key] = value
+        obj._data = vars(namespace)
         return obj
 
     @classmethod
@@ -46,19 +63,20 @@ class Args:
         path = Path(json_path)
         if not path.exists():
             raise FileNotFoundError(f"JSON file not found: {json_path}")
-        
+
         with path.open("r", encoding="utf-8") as f:
             data = json.load(f)
-        
+
         obj = cls()
         obj._data = data
         return obj
 
     def __add__(self, other):
-        """Merges two Args objects, creating a shared dictionary."""
+        """Merges two Args objects, updating the first with the second."""
         if not isinstance(other, Args):
             raise TypeError("Can only add Args objects together.")
 
         new_args = Args()
-        new_args._data = {**self._data, **other._data}  # Shared dictionary
-        return new_args
\ No newline at end of file
+        # Other's values overwrite self's values in case of conflict
+        new_args._data = {**self._data, **other._data}
+        return new_args
diff --git a/utils/contraction.py b/utils/contraction.py
index d7cdfd1..7f91545 100644
--- a/utils/contraction.py
+++ b/utils/contraction.py
@@ -3,6 +3,7 @@ import numpy as np
 from utils import safe_math
 from icecream import ic
 
+@torch.jit.script
 def contract_mean_std(x, std, eps:float = 1.1920928955078125e-07):#torch.finfo(torch.float).eps):
     # eps = 1e-3
     # Clamping to eps prevents non-finite gradients when x == 0.
diff --git a/utils/densification.py b/utils/densification.py
index 707527d..311dd4c 100644
--- a/utils/densification.py
+++ b/utils/densification.py
@@ -4,65 +4,154 @@ from utils import safe_math
 from typing import NamedTuple, List
 import gc
 from delaunay_rasterization.internal.render_err import render_err
+from delaunay_rasterization import render_debug
 import torch
-from utils.train_util import *
-from utils import topo_utils
+from icecream import ic
+
+def get_approx_ray_intersections(split_rays_data, epsilon=1e-7):
+    """
+    Calculates the approximate intersection point for pairs of line segments.
+
+    The intersection is defined as the midpoint of the shortest segment
+    connecting the two input line segments.
+
+    Args:
+        split_rays_data (torch.Tensor): Tensor of shape (N, 2, 6).
+            - N: Number of segment pairs.
+            - 2: Represents the two segments in a pair.
+            - 6: Contains [Ax, Ay, Az, Bx, By, Bz] for each segment,
+                 where A and B are the segment endpoints.
+                 Based on current Python code:
+                 A = average_P_exit, B = average_P_entry
+        epsilon (float): Small value to handle parallel lines and avoid
+                         division by zero if a segment has zero length.
+
+    Returns:
+        torch.Tensor: Tensor of shape (N, 3) representing the approximate
+                      "intersection" points (midpoints of closest approach).
+    """
+    # Segment 1 endpoints
+    p1_a = split_rays_data[:, 0, 0:3]  # Endpoint A of first segments (N, 3)
+    p1_b = split_rays_data[:, 0, 3:6]  # Endpoint B of first segments (N, 3)
+    # Segment 2 endpoints
+    p2_a = split_rays_data[:, 1, 0:3]  # Endpoint A of second segments (N, 3)
+    p2_b = split_rays_data[:, 1, 3:6]  # Endpoint B of second segments (N, 3)
+
+    # Define segment origins and direction vectors
+    # Segment S1: o1 + s * d1, for s in [0, 1]
+    # Segment S2: o2 + t * d2, for t in [0, 1]
+    o1 = p1_a
+    d1 = p1_b - p1_a  # Direction vector for segment 1 (from A to B)
+    o2 = p2_a
+    d2 = p2_b - p2_a  # Direction vector for segment 2 (from A to B)
+
+    # Calculate terms for finding closest points on the infinite lines
+    # containing the segments (based on standard formulas, e.g., Christer Ericson's "Real-Time Collision Detection")
+    v_o = o1 - o2 # Vector from origin of line 2 to origin of line 1
+
+    a = torch.sum(d1 * d1, dim=1)  # Squared length of d1
+    b = torch.sum(d1 * d2, dim=1)  # Dot product of d1 and d2
+    c = torch.sum(d2 * d2, dim=1)  # Squared length of d2
+    d = torch.sum(d1 * v_o, dim=1) # d1 dot (o1 - o2)
+    e = torch.sum(d2 * v_o, dim=1) # d2 dot (o1 - o2)
+
+    denom = a * c - b * b
+    s_line_num = (b * e) - (c * d)
+    t_line_num = (a * e) - (b * d) # This corresponds to t_c = (a*e - b*d)/denom from previous thoughts for P(t) = O2 + tD2
+
+    # Handle near-zero denominator (lines are parallel or one segment is a point)
+    # We compute with a safe denominator, then clamp. Clamping is key for segments.
+    denom_safe = torch.where(denom.abs() < epsilon, torch.ones_like(denom), denom)
+    
+    s_line = s_line_num / denom_safe
+    t_line = t_line_num / denom_safe # Note: This t_line is for the parameter of d2 (from o2)
+
+    # Clamp parameters to [0, 1] to stay within the segments
+    bad_intersect = (s_line < 0) | (t_line < 0) | (s_line > 1) | (t_line > 1)
+    s_seg = torch.clamp(s_line, 0.0, 1.0)
+    t_seg = torch.clamp(t_line, 0.0, 1.0)
 
+    # Points of closest approach on the segments
+    pc1 = o1 + s_seg.unsqueeze(1) * d1
+    pc2 = o2 + t_seg.unsqueeze(1) * d2
+    
+    p_int = (pc1 + pc2) / 2.0
+                        
+    return p_int, bad_intersect
 
 # -----------------------------------------------------------------------------
 # 1.  Aggregation helper
 # -----------------------------------------------------------------------------
 class RenderStats(NamedTuple):
-    total_within_var_votes: torch.Tensor    # (T, 2)
-    total_within_var: torch.Tensor    # (T, 2)
-    within_var_rays: torch.Tensor         # (T, 2, 6)
-    total_var_moments: torch.Tensor     # (T, 3)
-    between_var_moments: torch.Tensor   # (T, 3)
-    tet_moments: torch.Tensor           # (T, 4)
-    tet_view_count: torch.Tensor             # (T,)
-    total_var_count: torch.Tensor         # (T,)
-    tet_size: torch.Tensor              # (T,)
-    peak_contrib: torch.Tensor              # (T,)
-    total_T: torch.Tensor
+    within_var_rays: torch.Tensor
+    total_var_moments: torch.Tensor
+    tet_moments: torch.Tensor
+    tet_view_count: torch.Tensor
     total_err: torch.Tensor
-    total_ssim: torch.Tensor
-    max_ssim: torch.Tensor
     top_ssim: torch.Tensor
     top_size: torch.Tensor
+    total_count: torch.Tensor
+    peak_contrib: torch.Tensor
+    alphas: torch.Tensor
+    density: torch.Tensor
 
+@torch.no_grad()
+def determine_cull_mask(
+    sampled_cameras: List["Camera"],
+    model,
+    glo_list,
+    args,
+    device: torch.device,
+):
+    """Accumulate densification statistics for one iteration."""
+    n_tets = model.indices.shape[0]
+    peak_contrib = torch.zeros((n_tets), device=device)
+
+    for cam in sampled_cameras:
+        target = cam.original_image.cuda()
+
+        image_votes, extras = render_err(
+            target, cam, model,
+            scene_scaling=model.scene_scaling,
+            tile_size=args.tile_size,
+            lambda_ssim=0,
+            glo=glo_list(torch.LongTensor([cam.uid]).to(device))
+        )
+
+        tc = extras["tet_count"][..., 0]
+        max_T = extras["tet_count"][..., 1].float() / 65535
+        image_T, image_err, image_err2 = image_votes[:, 0], image_votes[:, 1], image_votes[:, 2]
+        _, image_Terr, image_ssim = image_votes[:, 3], image_votes[:, 4], image_votes[:, 5]
+        # peak_contrib = torch.maximum(image_T / tc.clip(min=1), peak_contrib)
+        peak_contrib = torch.maximum(max_T, peak_contrib)
+
+    tet_density = model.calc_tet_density()
+    alphas = model.calc_tet_alpha(mode="min", density=tet_density)
+    mask = ((peak_contrib > args.contrib_threshold))
+    return mask
 
 @torch.no_grad()
 def collect_render_stats(
     sampled_cameras: List["Camera"],
     model,
+    glo_list,
     args,
     device: torch.device,
 ):
     """Accumulate densification statistics for one iteration."""
     n_tets = model.indices.shape[0]
-
-    # Pre-allocate accumulators ------------------------------------------------
     tet_moments = torch.zeros((n_tets, 4), device=device)
     tet_view_count = torch.zeros((n_tets,), device=device)
-    tet_size = torch.zeros_like(tet_view_count)
-    total_var_count = torch.zeros((n_tets,), device=device)
 
-    total_within_var = torch.zeros((n_tets), device=device)
-    max_ssim = torch.zeros((n_tets), device=device)
-    total_ssim = torch.zeros((n_tets), device=device)
+    peak_contrib = torch.zeros((n_tets), device=device)
     top_ssim = torch.zeros((n_tets, 2), device=device)
     top_size = torch.zeros((n_tets, 2), device=device)
-    total_T = torch.zeros((n_tets), device=device)
     total_err = torch.zeros((n_tets), device=device)
-    peak_contrib = torch.zeros((n_tets), device=device)
-    total_within_var_votes = torch.zeros((n_tets, 2), device=device)
+    total_count = torch.zeros((n_tets), device=device, dtype=int)
     within_var_rays = torch.zeros((n_tets, 2, 6), device=device)
     total_var_moments = torch.zeros((n_tets, 3), device=device)
-    between_var_moments = torch.zeros((n_tets, 3), device=device)
     top_moments = torch.zeros((n_tets, 2, 4), device=device)
 
-
-    # Main per-camera loop -----------------------------------------------------
     for cam in sampled_cameras:
         target = cam.original_image.cuda()
 
@@ -70,45 +159,28 @@ def collect_render_stats(
             target, cam, model,
             scene_scaling=model.scene_scaling,
             tile_size=args.tile_size,
-            lambda_ssim=args.clone_lambda_ssim
+            lambda_ssim=0,
+            glo=glo_list(torch.LongTensor([cam.uid]).to(device))
         )
 
-        tc = extras["tet_count"]
+        tc = extras["tet_count"][..., 0]
+        max_T = extras["tet_count"][..., 1].float() / 65535
         
-        # --- Create a single mask for valid updates ---
-        # Mask for tets that have a reasonable number of samples in the current view
         update_mask = (tc >= args.min_tet_count) & (tc < 8000)
 
         # --- Moments (s0: sum of T, s1: sum of err, s2: sum of err^2)
         image_T, image_err, image_err2 = image_votes[:, 0], image_votes[:, 1], image_votes[:, 2]
-        # total_T_p, image_err, image_err2 = image_votes[:, 3], image_votes[:, 4], image_votes[:, 5]
         _, image_Terr, image_ssim = image_votes[:, 3], image_votes[:, 4], image_votes[:, 5]
-        N = tc
-        peak_contrib = torch.maximum(image_T, peak_contrib)
-        total_T += image_T
         total_err += image_Terr
-        total_ssim += image_ssim
-        max_ssim = torch.maximum(image_ssim, max_ssim)
-
-        # -------- Within-Image Variance (Top-2 per tet) -----------------------
-        within_var_mu = safe_math.safe_div(image_err, N)
-        within_var_std = (safe_math.safe_div(image_err2, N) - within_var_mu**2).clip(min=0)
-        within_var_std[N < 10] = 0
-        within_var_std[~update_mask] = 0 # Use the unified mask
-
-        within_var_votes = image_T * within_var_std
+        # peak_contrib = torch.maximum(image_T / tc.clip(min=1), peak_contrib)
+        peak_contrib = torch.maximum(max_T, peak_contrib)
 
         # ray buffer: (enter | exit)  (N, 6)
         w = image_votes[:, 12:13]
         seg_exit = safe_math.safe_div(image_votes[:, 9:12], w)
         seg_enter = safe_math.safe_div(image_votes[:, 6:9], w)
 
-        # keep top-2 candidates per tet across all views
-        total_within_var += within_var_votes
-        votes_3 = torch.cat([total_within_var_votes, within_var_votes[:, None]], dim=1)
-        votes_sorted, idx_sorted = votes_3.sort(1, descending=True)
-        total_within_var_votes = votes_sorted[:, :2]
-
+        image_ssim[~update_mask] = 0
         top_ssim, idx_sorted = torch.cat([top_ssim[:, :2], image_ssim.reshape(-1, 1)], dim=1).sort(1, descending=True)
         top_size = torch.gather(
             torch.cat([top_size, tc.reshape(-1, 1)], dim=1), 1,
@@ -136,47 +208,34 @@ def collect_render_stats(
         )
 
         # -------- Total Variance (accumulated across images) ------------------
-        total_var_moments[update_mask, 0] += N[update_mask]
-        # total_var_moments[update_mask, 0] += image_T[update_mask]
-        total_var_moments[update_mask, 1] += image_err[update_mask]
-        total_var_moments[update_mask, 2] += image_err2[update_mask]
-        total_var_count[update_mask] += N[update_mask]
-
-        # -------- Between-Image Variance (accumulated across images) ----------
-        # We compute the variance of the mean error across different views
-        mean_err_per_view = within_var_mu
-        mean_err_per_view[N < 10] = 0
-
-        between_var_moments[update_mask, 0] += image_T[update_mask] # Use summed image_T as weight
-        between_var_moments[update_mask, 1] += mean_err_per_view[update_mask]
-        between_var_moments[update_mask, 2] += (mean_err_per_view[update_mask])**2
+        # total_var_moments[update_mask, 0] += tc[update_mask]
+        total_var_moments[update_mask, 0] += image_T[update_mask]
+        total_var_moments[update_mask, 1] += image_T[update_mask] * image_err[update_mask]
+        total_var_moments[update_mask, 2] += image_T[update_mask] * image_err2[update_mask]
+        # total_count += N
+        total_count[update_mask] += 1
 
         # -------- Other stats -------------------------------------------------
         tet_moments[update_mask, :3] += image_votes[update_mask, 13:16]
         tet_moments[update_mask, 3] += w[update_mask].reshape(-1)
 
         tet_view_count[update_mask] += 1 # Count views per tet
-        tet_size += tc
 
+    tet_density = model.calc_tet_density()
+    alphas = model.calc_tet_alpha(mode="max", density=tet_density)
     # done
     return RenderStats(
-        total_within_var_votes = total_within_var_votes,
-        total_within_var = total_within_var,
         within_var_rays = within_var_rays,
         total_var_moments = total_var_moments,
-        between_var_moments = between_var_moments,
         tet_moments = tet_moments,
-        # tet_moments = top_moments.sum(dim=1),
         tet_view_count = tet_view_count,
-        total_var_count = total_var_count,
-        tet_size = tet_size,
-        peak_contrib = peak_contrib,
-        total_T = total_T,
         total_err = total_err,
-        total_ssim = total_ssim,
-        max_ssim = max_ssim,
+        total_count = total_count,
         top_ssim = top_ssim[:, :2],
         top_size = top_size[:, :2],
+        peak_contrib = peak_contrib,
+        density=tet_density,
+        alphas=alphas
     )
 
 @torch.no_grad()
@@ -189,118 +248,67 @@ def apply_densification(
     device: torch.device,
     sample_cam,
     sample_image,
-    target_addition
+    budget: int
 ):
-    """Turns accumulated statistics into actual vertex cloning / splitting."""
-    # ---------- Calculate scores from variances ------------------------------
-    total_T, s1_b, s2_b = stats.between_var_moments.T
-    # 1. Total Variance Score (for growing)
     s0_t, s1_t, s2_t = stats.total_var_moments.T
-    N_t = stats.total_var_count
     total_var_mu = safe_math.safe_div(s1_t, s0_t)
     total_var_std = (safe_math.safe_div(s2_t, s0_t) - total_var_mu**2).clip(min=0)
-    total_var_std[s0_t < 1] = 0
-
-    # 2. Between-Image Variance Score (for splitting)
-    N_b = stats.tet_view_count # Num views
-    between_var_mu = safe_math.safe_div(s1_b, N_b)
-    between_var_std = (safe_math.safe_div(s2_b, N_b) - between_var_mu**2).clip(min=0)
-    between_var_std[N_b < 2] = 0 # Need at least 2 views for variance
+    total_var_std[s0_t < 1e-2] = 0
 
-    # 3. Within-Image Variance Score (for splitting)
-    within_var = stats.total_within_var_votes[:, 0]
-    # within_var = stats.total_ssim / stats.tet_size.clip(min=1).sqrt()
     within_var = stats.top_ssim.sum(dim=1) / stats.top_size.sum(dim=1).clip(min=1).sqrt()
-    # within_var = stats.top_ssim[:, 0] / stats.tet_size.clip(min=1).sqrt()
-
-    between_var = stats.total_T * between_var_std # Weighted by summed s0
-    total_var = stats.total_err * total_var_std
-    # total_var = stats.total_T * safe_math.safe_div(total_var_std, between_var_std).clip(min=0, max=10)
-    total_var[(N_b < 2) | (s0_t < 1)] = 0
-    # within_var = stats.total_within_var / stats.tet_view_count.clip(min=1)
-    # within_var = safe_math.safe_div(stats.total_within_var, total_T)
-    # total_var += within_var
-    vertices = model.vertices
-    # circumcenters, _, tet_density, rgb, grd, sh = model.compute_batch_features(vertices, model.indices, 0, model.indices.shape[0])
-
-    # --- Masking and target calculation --------------------------------------
-    tet_density = model.calc_tet_density()
-    alphas = model.calc_tet_alpha(mode="max", density=tet_density)
-    mask_alive = (alphas >= args.clone_min_alpha) & (tet_density.reshape(-1) >= args.clone_min_density)
-    # area = topo_utils.tet_volumes(model.vertices[model.indices])
-    # ratio = safe_math.safe_div(alphas, area)
-    # mask_alive = ratio > args.clone_min_ratio
+
+    total_var = (stats.total_err / stats.total_count.clip(min=1)) * total_var_std
+    # N_b = stats.tet_view_count # Num views
+    # total_var[(N_b < 2) | (s0_t < 1)] = 0
+
+    # mask_alive = (stats.alphas >= args.clone_min_alpha) & (stats.density.reshape(-1) >= args.clone_min_density)
+
+
+    mask_alive = ((stats.peak_contrib > args.contrib_threshold) | (stats.alphas > args.clone_min_alpha)).int()
+    keep_verts = torch.zeros((model.vertices.shape[0]), dtype=torch.int, device=stats.alphas.device)
+    indices = model.indices.long()
+    reduce_type = "sum"
+    keep_verts.scatter_reduce_(dim=0, index=indices[..., 0], src=mask_alive, reduce=reduce_type)
+    keep_verts.scatter_reduce_(dim=0, index=indices[..., 1], src=mask_alive, reduce=reduce_type)
+    keep_verts.scatter_reduce_(dim=0, index=indices[..., 2], src=mask_alive, reduce=reduce_type)
+    keep_verts.scatter_reduce_(dim=0, index=indices[..., 3], src=mask_alive, reduce=reduce_type)
+
+
     total_var[~mask_alive] = 0
     within_var[~mask_alive] = 0
-    between_var[~mask_alive] = 0
-    # total_var = (total_var - between_var).clip(min=0)
-
-    target_addition = min(target_addition, stats.tet_view_count.shape[0])
-    if target_addition < 0:
-        return
-
-    # Assume args.percent_total_var and args.percent_within_var exist
-    target_total = int(args.percent_total * target_addition)
-    target_within = int(args.percent_within * target_addition)
-    target_between = int(max(0, target_addition - target_total - target_within))
-
-    grow_mask = torch.zeros_like(total_var, dtype=torch.bool)
-    within_mask = torch.zeros_like(grow_mask)
-    between_mask = torch.zeros_like(grow_mask)
-
-    # To prevent overlap, we select candidates sequentially
-    # and zero out the scores of selected tets in other categories.
-    temp_total_score = total_var.clone()
-    temp_within_score = within_var.clone()
-    temp_between_score = between_var.clone()
-
-    if target_total > 0:
-        top_total = torch.topk(temp_total_score, target_total).indices
-        grow_mask[top_total] = temp_total_score[top_total] > 0
-        temp_within_score[grow_mask] = 0
-        temp_between_score[grow_mask] = 0
-
-    if target_within > 0:
-        top_within = torch.topk(temp_within_score, target_within).indices
-        within_mask[top_within] = temp_within_score[top_within] > 0
-        temp_between_score[within_mask] = 0
-    
-    if target_between > 0:
-        top_between = torch.topk(temp_between_score, target_between).indices
-        between_mask[top_between] = temp_between_score[top_between] > 0
-
-    split_mask = within_mask | between_mask
-    clone_mask = split_mask | grow_mask
+    within_mask = (within_var > args.within_thresh)
+    total_mask = (total_var > args.total_thresh)
+    clone_mask = within_mask | total_mask
+    if clone_mask.sum() > budget:
+        true_indices = clone_mask.nonzero().squeeze(-1)
+        perm = torch.randperm(true_indices.size(0))
+        selected_indices = true_indices[perm[:budget]]
+        
+        clone_mask = torch.zeros_like(clone_mask, dtype=torch.bool)
+        clone_mask[selected_indices] = True
 
-    # ---------- debug renders -------------------------------------------------
     if args.output_path is not None:
 
         f = mask_alive.float().unsqueeze(1).expand(-1, 4).clone()
         color = torch.rand_like(f[:, :3])
-        # color = rgb + 0.5#torch.rand_like(f[:, :3])
         f[:, :3] = color
         f[:, 3] *= 2.0    # alpha
-        imageio.imwrite(args.output_path / f"alive_mask{iteration}.png",
-                        render_debug(f, model, sample_cam, 10))
+        # imageio.imwrite(args.output_path / f"alive_mask{iteration}.png",
+        #                 render_debug(f, model, sample_cam, 10))
         f = clone_mask.float().unsqueeze(1).expand(-1, 4).clone()
         f[:, :3] = color
         f[:, 3] *= 2.0    # alpha
-        imageio.imwrite(args.output_path / f"densify{iteration}.png",
-                        render_debug(f, model, sample_cam, 10))
-        imageio.imwrite(args.output_path / f"total_var{iteration}.png",
-                        render_debug(total_var[:, None],
-                                     model, sample_cam))
-        imageio.imwrite(args.output_path / f"within_var{iteration}.png",
-                        render_debug(within_var[:, None],
-                                     model, sample_cam))
-        imageio.imwrite(args.output_path / f"between_var{iteration}.png",
-                        render_debug(between_var[:, None],
-                                     model, sample_cam))
+        # imageio.imwrite(args.output_path / f"densify{iteration}.png",
+        #                 render_debug(f, model, sample_cam, 10))
+        # imageio.imwrite(args.output_path / f"total_var{iteration}.png",
+        #                 render_debug(total_var[:, None],
+        #                              model, sample_cam))
+        # imageio.imwrite(args.output_path / f"within_var{iteration}.png",
+        #                 render_debug(within_var[:, None],
+        #                              model, sample_cam))
         imageio.imwrite(args.output_path / f"im{iteration}.png",
                         cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))
 
-
-    # ---------- pick clone positions -----------------------------------------
     clone_indices = model.indices[clone_mask]
     split_point, bad = get_approx_ray_intersections(stats.within_var_rays)
     grow_point = safe_math.safe_div(
@@ -315,36 +323,17 @@ def apply_densification(
     random_locations = (model.vertices[clone_indices] * barycentric_weights).sum(dim=1)
     split_point[bad] = random_locations[bad]    # fall back
 
+    keep_verts = keep_verts > 0
+    print(f"Pruned: {(~keep_verts).sum()}")
+    # tet_optim.remove_points(keep_verts.reshape(-1))
     tet_optim.split(clone_indices,
                     split_point,
                     **args.as_dict())
 
-    # ---------- velocity-based additions -------------------------------------
-    raw_verts = model.contracted_vertices
-    vstate = tet_optim.vertex_optim.get_state_by_name("contracted_vertices")
-    velocity = vstate["exp_avg"] * args.speed_mul
-
-    if model.contract_vertices:
-        J_d = topo_utils.contraction_jacobian_d_in_chunks(
-            model.vertices[:raw_verts.shape[0]]).view(-1, 1)
-        speed = torch.linalg.norm(velocity * J_d, dim=1)
-    else:
-        speed = torch.linalg.norm(velocity, dim=1)
-        
-    if args.clone_velocity > 0:
-        new_verts = (raw_verts + velocity)[speed > args.clone_velocity]
-        tet_optim.add_points(new_verts, raw_verts=True)
-        num_cloned = new_verts.shape[0]
-    else:
-        num_cloned = 0
-
-    # ---------- housekeeping --------------------------------------------------
     gc.collect()
     torch.cuda.empty_cache()
 
     print(
-        f"#Grow: {grow_mask.sum():4d} #Split: {split_mask.sum():4d} | "
-        f"T_Total: {target_total:4d} T_Within: {target_within:4d} T_Between: {target_between:4d} | "
-        f"Total Avg: {total_var.mean():.4f} Within Avg: {within_var.mean():.4f} Between Avg: {between_var.mean():.4f}  | "
-        f"By Vel: {num_cloned}"
+        f"#Within: {within_mask.sum():4d} #Total: {total_mask.sum():4d} | "
+        f"Total Avg: {total_var.mean():.4f} Within Avg: {within_var.mean():.4f}"
     )
diff --git a/utils/hashgrid.py b/utils/hashgrid.py
index 5e6753a..cdf7f7c 100644
--- a/utils/hashgrid.py
+++ b/utils/hashgrid.py
@@ -63,12 +63,15 @@ class HashEmbedderOptimized(nn.Module):
 
         self.hash_mask = (1 << log2_hashmap_size) - 1
         self.out_dim = n_levels * n_features_per_level
-        self.n_output_dims = self.out_dim  # backwards compat
+        self.n_output_dims = self.out_dim
 
     def ingp_hash(self, coords):
-        xor_result = torch.zeros_like(coords[..., 0])
+        UINT32_MASK = 0xFFFFFFFF
+        xor_result = torch.zeros_like(coords[..., 0]).clone()
         for i in range(coords.shape[-1]):
-             xor_result ^= coords[..., i] * self.primes[i]
+            term = (coords[..., i] * self.primes[i]) & UINT32_MASK
+            xor_result ^= term
+
         return xor_result & self.hash_mask
 
     def get_voxel_vertices(self, x, level_idx):
@@ -89,7 +92,6 @@ class HashEmbedderOptimized(nn.Module):
         num_voxels = resolution ** 3
 
         if self.use_conditional_hashing and (num_voxels <= hashmap_size):
-            # --- Collision-free path: Use modulo for wrap-around ---
             res_sq = res_long * res_long
             wrapped_indices = voxel_indices % res_long
             hashed_voxel_indices = (
@@ -98,7 +100,6 @@ class HashEmbedderOptimized(nn.Module):
                 wrapped_indices[..., 2] * res_sq
             )
         else:
-            # --- Standard iNGP hash: Use raw integer indices ---
             hashed_voxel_indices = self.ingp_hash(voxel_indices)
         
         return fracs, hashed_voxel_indices
@@ -124,10 +125,6 @@ class HashEmbedderOptimized(nn.Module):
         return torch.cat(x_embedded_all, dim=-1)
 
     def forward_in_chunks(self, x, chunk_size=548576):
-    # def forward_in_chunks(self, x, chunk_size=65536):
-        """
-        Same as forward(), but processes 'x' in chunks to reduce memory usage.
-        """
         outputs = []
         start = 0
         while start < x.shape[0]:
@@ -135,4 +132,4 @@ class HashEmbedderOptimized(nn.Module):
             x_chunk = x[start:end]
             outputs.append(self.forward(x_chunk))
             start = end
-        return torch.cat(outputs, dim=0)
\ No newline at end of file
+        return torch.cat(outputs, dim=0)
diff --git a/utils/lmodel_util.py b/utils/lmodel_util.py
deleted file mode 100644
index 5932c64..0000000
--- a/utils/lmodel_util.py
+++ /dev/null
@@ -1,236 +0,0 @@
-import torch
-from torch import nn
-from utils import hashgrid
-import math
-
-from utils.graphics_utils import l2_normalize_th
-from utils.topo_utils import calculate_circumcenters_torch, fibonacci_spiral_on_sphere, calc_barycentric, sample_uniform_in_sphere, project_points_to_tetrahedra, contraction_jacobian_d_in_chunks
-from utils.safe_math import safe_exp, safe_div, safe_sqrt
-from utils.contraction import contract_mean_std
-from utils.contraction import contract_points, inv_contract_points
-from sh_slang.eval_sh import eval_sh
-from utils.hashgrid import HashEmbedderOptimized
-from icecream import ic
-import torch.nn.init as init # Common alias for torch.nn.init
-
-C0 = 0.28209479177387814
-def RGB2SH(rgb):
-    return (rgb - 0.5) / C0
-
-def SH2RGB(sh):
-    return sh * C0 + 0.5
-
-def gaussian_in_circumsphere(cc: torch.Tensor,       # (T,3)
-                             r:  torch.Tensor,       # (T,1)
-                             k:  int,
-                             trunc_sigma: float = 0.3) -> torch.Tensor:
-    """
-    Draw `k` 3D points from N(cc, (trunc_sigma*r)^2 I), truncated so xccr.
-
-    Returns: (T,k,3)
-    """
-    T = cc.shape[0]
-    # iid standard normal                                                          (T,k,3)
-    x  = torch.randn((T, k, 3), device=cc.device)
-
-    # scale by radius*trunc_sigma
-    x  = x * (trunc_sigma * r).unsqueeze(1)
-
-    # rejectionsampling for the few outofsphere samples ------------------------
-    inside = (x.norm(dim=-1, p=2) <= r.unsqueeze(1)).all(dim=-1)
-    while not inside.all():
-        # redraw only the failed rows (1% for =0.3)
-        mask   = ~inside
-        num    = mask.sum()
-        x[mask] = torch.randn((num, 3), device=x.device) * (trunc_sigma * r[mask])
-        inside  = (x.norm(dim=-1) <= r[mask]).all(dim=-1)
-
-    return cc.unsqueeze(1) + x                 # (T,k,3)
-
-def init_linear(m, gain):
-    """Standard Xavier-uniform + zero bias for any Linear layer."""
-    if isinstance(m, nn.Linear):
-        nn.init.xavier_uniform_(m.weight, gain)
-        if m.bias is not None:
-            nn.init.zeros_(m.bias)
-
-@torch.jit.script
-def pre_calc_cell_values(vertices, indices):
-    device = vertices.device
-    tets = vertices[indices]
-    circumcenter, radius = calculate_circumcenters_torch(tets.double())
-    return circumcenter
-
-@torch.jit.script
-def compute_vertex_colors_from_field(
-    element_verts: torch.Tensor,   # (T, 4, 3) - Renamed for clarity (V=4, D=3)
-    base:           torch.Tensor,   # (T, 3)    - (T, C)
-    gradients:      torch.Tensor,   # (T, 3, 3) - (T, C, D_grad=3)
-    circumcenters:  torch.Tensor    # (T, 3)    - (T, D=3)
-) -> torch.Tensor: # Returns vertex_colors (T, 4, 3)
-    """
-    Compute per-vertex colors for each element (e.g., tetrahedron).
-    
-    For each vertex:
-      color = base_for_channel + dot(gradient_for_channel, normalized(vertex - circumcenter))
-    """
-    offsets = element_verts - circumcenters[:, None, :]
-
-    grad_contrib = torch.einsum('tcd,tvd->tvc', gradients, offsets)
-    vertex_colors = base[:, None, :] + grad_contrib 
-    
-    return vertex_colors
-
-def offset_normalize(rgb, grd, base_density, d_grd, circumcenters, tets):
-    grd = grd.reshape(-1, 1, 3) * rgb.reshape(-1, 3, 1).mean(dim=1, keepdim=True).detach()
-    radius = torch.linalg.norm(tets - circumcenters[:, None, :], dim=-1, keepdim=True)[:, :1]
-    normed_grd = safe_div(grd, radius).float()
-    # normed_d_grd = safe_div(d_grd.reshape(-1, 3), radius.reshape(-1, 1)).float()
-    normed_d_grd = safe_div(base_density.reshape(-1, 1) * d_grd.reshape(-1, 3),
-                            radius.reshape(-1, 1)).float()
-
-    offsets = tets.detach() - circumcenters[:, None, :]
-
-    grad_contrib = torch.einsum('tcd,tvd->tvc', normed_grd, offsets)
-    vcolors = rgb[:, None, :] + grad_contrib 
-    base_color_v0_raw = vcolors[:, 0]
-
-    d_grad_contrib = torch.einsum('tcd,tvd->tvc', normed_d_grd.reshape(-1, 1, 3), offsets.reshape(-1, 4, 3))
-    dcolors = base_density.reshape(-1, 1) + d_grad_contrib.reshape(-1, 4)
-    base_density_v0 = dcolors[:, 0].reshape(-1, 1)
-
-    return base_color_v0_raw, normed_grd, base_density_v0, normed_d_grd
-
-def activate_output(camera_center, rgb, grd, base_density, d_grd,
-                    sh, indices, circumcenters, vertices, current_sh_deg, max_sh_deg):
-    tets = vertices[indices]
-    base_color_v0_raw, normed_grd, base_density_v0, normed_d_grd = offset_normalize(
-        rgb, grd, base_density, d_grd, circumcenters.float(), tets)
-    tet_color_raw = eval_sh(
-        tets.mean(dim=1),
-        RGB2SH(base_color_v0_raw).float(),
-        sh.reshape(-1, (max_sh_deg+1)**2 - 1, 3),
-        camera_center,
-        current_sh_deg)
-    base_color_v0 = torch.nn.functional.softplus(tet_color_raw.reshape(-1, 3, 1), beta=10)
-    features = torch.cat([
-        base_density_v0,
-        base_color_v0.reshape(-1, 3),
-        normed_grd.reshape(-1, 3),
-        normed_d_grd.reshape(-1, 3)
-    ], dim=1)
-    return features.float()
-
-class iNGPDWL(nn.Module):
-    def __init__(self, 
-                 sh_dim=0,
-                 scale_multi=0.5,
-                 log2_hashmap_size=16,
-                 base_resolution=16,
-                 per_level_scale=2,
-                 k_samples=1,
-                 trunc_sigma=0.3,
-                 L=10,
-                 hashmap_dim=4,
-                 hidden_dim=64,
-                 dg_init=0.1,
-                 g_init=1,
-                 s_init=1e-4,
-                 d_init=0.1,
-                 c_init=0.6,
-                 density_offset=-4,
-                 **kwargs):
-        super().__init__()
-        self.k_samples = k_samples
-        self.trunc_sigma = trunc_sigma
-        self.scale_multi = scale_multi
-        self.L = L
-        self.dim = hashmap_dim
-        self.per_level_scale = per_level_scale
-        self.base_resolution = base_resolution
-        self.density_offset = density_offset
-
-        self.encoding = hashgrid.HashEmbedderOptimized(
-            [torch.zeros((3)), torch.ones((3))],
-            self.L, n_features_per_level=self.dim,
-            log2_hashmap_size=log2_hashmap_size, base_resolution=base_resolution,
-            finest_resolution=base_resolution*per_level_scale**self.L)
-
-        def mk_head(n):
-            network = nn.Sequential(
-                nn.Linear(self.encoding.n_output_dims, hidden_dim),
-                nn.SELU(inplace=True),
-                nn.Linear(hidden_dim, hidden_dim),
-                nn.SELU(inplace=True),
-                nn.Linear(hidden_dim, n)
-            )
-            gain = nn.init.calculate_gain('relu')  # for example, if using ReLU activations
-            network.apply(lambda m: init_linear(m, gain))
-            return network
-        self.network = mk_head(1+12+sh_dim)
-
-        self.density_net   = mk_head(1)
-        self.color_net     = mk_head(3)
-        self.gradient_net  = mk_head(3)
-        self.d_gradient_net  = mk_head(3)
-        self.sh_net        = mk_head(sh_dim)
-
-        last = self.network[-1]
-        with torch.no_grad():
-            last.weight[4:, :].zero_()
-            last.bias[4:].zero_()
-            for network, eps in zip(
-                [self.d_gradient_net, self.gradient_net, self.sh_net, self.density_net, self.color_net], 
-                [dg_init, g_init, s_init, d_init, c_init]):
-                last = network[-1]
-                with torch.no_grad():
-                    init.uniform_(last.weight.data, a=-eps, b=eps)
-                    # nn.init.xavier_uniform_(m.weight, gain)
-                    last.bias.zero_()
-
-
-    def _encode(self, x: torch.Tensor, cr: torch.Tensor):
-        x = x.detach()
-        output = self.encoding(x).float()
-        output = output.reshape(-1, self.dim, self.L)
-        cr = cr.float() * self.scale_multi
-        n = torch.arange(self.L, device=x.device).reshape(1, 1, -1)
-        erf_x = safe_div(torch.tensor(1.0, device=x.device),
-                         safe_sqrt(self.per_level_scale * 4*n*cr.reshape(-1, 1, 1)))
-        scaling = torch.erf(erf_x)
-        output = output * scaling
-        return output
-
-
-    def forward(self, x, cr):
-        if self.k_samples > 1:
-
-            # ------- draw Gaussian jitters inside the sphere -----------------
-            eps = torch.randn((x.shape[0], self.k_samples, 3),
-                              device=x.device)
-            eps = eps * (self.trunc_sigma * cr/4).view(-1,1,1)   # scale
-            samples_xf = (x.unsqueeze(1) + eps).reshape(-1, 3)
-
-            samples_xf.clamp_(0.0, 1.0)
-            samples_cr = cr.repeat_interleave(self.k_samples)
-
-            # ------- encode every jitter and mean in feature space -----------
-            output = self._encode(samples_xf, samples_cr)            # (B*k,F)
-            output = output.view(x.shape[0], self.k_samples, -1).mean(dim=1)
-        else:
-            output = self._encode(x, cr)
-
-        h = output.reshape(-1, self.L * self.dim)
-
-        sigma = self.density_net(h)
-        rgb = self.color_net(h)
-        field_samples = self.gradient_net(h)
-        sh  = self.sh_net(h)
-
-        rgb = rgb.reshape(-1, 3, 1) + 0.5
-        # density = safe_exp(sigma+self.density_offset)
-        density = sigma + self.density_offset
-        grd = torch.tanh(field_samples.reshape(-1, 1, 3)) / math.sqrt(3)
-        d_grd = self.d_gradient_net(h)
-        return density, rgb.reshape(-1, 3), grd, d_grd, sh
-
diff --git a/utils/mesh_util.py b/utils/mesh_util.py
index a56ac1d..a39f1bf 100644
--- a/utils/mesh_util.py
+++ b/utils/mesh_util.py
@@ -1,113 +1,193 @@
 import numpy as np
-from collections import defaultdict, deque
+from collections import deque, defaultdict
+# from skimage.draw import polygon, polygon_perimeter
+from PIL import Image
+from pathlib import Path
+# import xatlas
+# import trimesh
+
+def tet_to_vert_color(verts, tets, tet_v_rgb):
+    n_verts = len(verts)
+    n_ch = tet_v_rgb.shape[-1]
+
+    v_rgb = np.zeros((n_verts, n_ch), dtype=np.float32)
+    flat_t_idx = tets.flatten()
+    flat_t_rgb = tet_v_rgb.reshape(-1, n_ch)
+
+    np.add.at(v_rgb, flat_t_idx, flat_t_rgb)
+    
+    v_counts = np.bincount(flat_t_idx, minlength=n_verts)
+    
+    non_zero = v_counts > 0
+    v_rgb[non_zero] /= v_counts[non_zero, np.newaxis]
+
+    return v_rgb
 
 def extract_meshes(rgb, verts, tets):
-    # ---------- 1.  collect candidate faces ------------------------------------
-    # Each tet contributes four faces, always in counter-clockwise order if you
-    # keep the vertex you *omit* in first position.
-    faces = np.stack([
-        tets[:, [1, 2, 3]],
-        tets[:, [0, 3, 2]],
-        tets[:, [0, 1, 3]],
-        tets[:, [0, 2, 1]],
-    ], axis=1).reshape(-1, 3)                   # (4*M, 3)
-
-    # Associate RGB values with faces
-    face_rgb = rgb.reshape(-1, 3)
-
-    # ---------- 2.  locate boundary faces --------------------------------------
-    faces_sorted = np.sort(faces, axis=1)       # canonical key, orientation lost
-    faces_key = np.ascontiguousarray(faces_sorted).view(
-        np.dtype((np.void, faces_sorted.dtype.itemsize * 3))
-    )
-    keys, inv, counts = np.unique(faces_key, return_inverse=True, return_counts=True)
-    boundary_mask   = counts[inv] == 1
-    boundary_faces  = faces[boundary_mask]      # keeps the original winding
-    boundary_rgb    = face_rgb[boundary_mask]   # RGB values for boundary faces
-
-    # Build edgeface lookup for shared edge connectivity
-    # This maps a canonical edge (sorted tuple of two vertex indices) to a list of face indices
-    e2f = defaultdict(list)
-    for fi, tri in enumerate(boundary_faces):
-        # Extract edges from the current triangle (v0, v1, v2)
-        edges = [
-            tuple(sorted((tri[0], tri[1]))),
-            tuple(sorted((tri[1], tri[2]))),
-            tuple(sorted((tri[2], tri[0])))
-        ]
-        for edge in edges:
-            e2f[edge].append(fi)
-
-    # Build faceface lookup based on shared edges
-    # This maps a face index to a list of other face indices that share an edge with it
-    f2f = defaultdict(list)
-    for fi, tri in enumerate(boundary_faces):
-        edges = [
-            tuple(sorted((tri[0], tri[1]))),
-            tuple(sorted((tri[1], tri[2]))),
-            tuple(sorted((tri[2], tri[0])))
-        ]
-        for edge in edges:
-            # Get all faces that share this specific edge
-            shared_faces = e2f[edge]
-            for other_fi in shared_faces:
-                if other_fi != fi: # Ensure we don't add the face to its own adjacency list
-                    f2f[fi].append(other_fi)
-
-    # Flood-fill over triangles that share at least one edge
+    # 1. Pre-calculate all vertex colors correctly
+    v_rgb = tet_to_vert_color(verts, tets, rgb)
+
+    # 2. Build tetrahedron adjacency graph
+    face_map = defaultdict(list)
+    all_faces = np.stack([
+        tets[:, [1, 3, 2]], tets[:, [0, 2, 3]],
+        tets[:, [0, 3, 1]], tets[:, [0, 1, 2]],
+    ], axis=1)
+
+    for ti, tet_faces in enumerate(all_faces):
+        for face in tet_faces:
+            key = tuple(sorted(face))
+            face_map[key].append(ti)
+    
+    tet_adj = defaultdict(list)
+    for key, t_indices in face_map.items():
+        if len(t_indices) == 2:
+            t0, t1 = t_indices
+            tet_adj[t0].append(t1)
+            tet_adj[t1].append(t0)
+
+    # 3. Flood-fill across tetrahedra to find connected volumes
     components, seen = [], set()
-    for seed in range(len(boundary_faces)):
-        if seed in seen:
+    for i in range(len(tets)):
+        if i in seen:
             continue
-        q, comp = deque([seed]), []
+        comp = []
+        q = deque([i])
         while q:
-            f = q.popleft()
-            if f in seen:
+            ti = q.popleft()
+            if ti in seen:
                 continue
-            seen.add(f)
-            comp.append(f)
-            # Extend the queue with all faces that share an edge with the current face 'f'
-            q.extend(f2f[f])
+            seen.add(ti)
+            comp.append(ti)
+            q.extend(tet_adj[ti])
         components.append(np.array(comp, dtype=np.int64))
 
+    # 4. Extract boundary mesh from each volume
     meshes = []
-    for comp in components:
-        tris = boundary_faces[comp]             # (F,3)
-        tris_rgb = boundary_rgb[comp]           # RGB values for triangles
-        unique_vs, new_idx = np.unique(tris, return_inverse=True)
-        tris_reindexed = new_idx.reshape(-1, 3)
-
-        # Calculate vertex colors by averaging face colors
-        vertex_colors = np.zeros((len(unique_vs), boundary_rgb.shape[1]), dtype=np.float32)
-        vertex_face_counts = np.zeros(len(unique_vs), dtype=np.int32)
-
-        # Map original vertex indices to reindexed vertex indices
-        orig_to_new_v_map = {orig_v: new_v_idx for new_v_idx, orig_v in enumerate(unique_vs)}
-
-        for face_idx_in_comp, face_orig_indices in enumerate(tris):
-            current_face_rgb = tris_rgb[face_idx_in_comp]
-            for orig_v_idx in face_orig_indices:
-                new_v_idx = orig_to_new_v_map[orig_v_idx]
-                vertex_colors[new_v_idx] += current_face_rgb
-                vertex_face_counts[new_v_idx] += 1
+    for comp_t_indices in components:
+        comp_tets = tets[comp_t_indices]
+        
+        comp_faces = np.stack([
+            comp_tets[:, [1, 3, 2]], comp_tets[:, [0, 2, 3]],
+            comp_tets[:, [0, 3, 1]], comp_tets[:, [0, 1, 2]],
+        ], axis=1).reshape(-1, 3)
+
+        faces_key = np.sort(comp_faces, axis=1)
+        keys, inv, counts = np.unique(
+            faces_key, axis=0, return_inverse=True, return_counts=True
+        )
         
-        # Avoid division by zero for isolated vertices if any
-        vertex_colors = np.divide(vertex_colors, vertex_face_counts[:, np.newaxis], 
-                                  out=np.zeros_like(vertex_colors), 
-                                  where=vertex_face_counts[:, np.newaxis] != 0)
-
-
-        meshes.append(
-            dict(
-                vertex = dict(
-                    x = verts[unique_vs, 0].astype(np.float32),
-                    y = verts[unique_vs, 1].astype(np.float32),
-                    z = verts[unique_vs, 2].astype(np.float32),
-                    r = vertex_colors[:, 0].astype(np.float32), # Separate R channel
-                    g = vertex_colors[:, 1].astype(np.float32), # Separate G channel
-                    b = vertex_colors[:, 2].astype(np.float32)), # Separate B channel
-                face    = dict(
-                    vertex_indices=tris_reindexed.astype(np.int32),
-                    rgb=tris_rgb.astype(np.float32)) # Include face RGB values
-            ))
+        b_mask = counts[inv] == 1
+        b_faces = comp_faces[b_mask]
+
+        if len(b_faces) == 0:
+            continue
+            
+        unique_vs, new_idx = np.unique(b_faces, return_inverse=True)
+        
+        meshes.append(dict(
+            vertex=dict(
+                x=verts[unique_vs, 0].astype(np.float32),
+                y=verts[unique_vs, 1].astype(np.float32),
+                z=verts[unique_vs, 2].astype(np.float32),
+                r=v_rgb[unique_vs, 0].astype(np.float32),
+                g=v_rgb[unique_vs, 1].astype(np.float32),
+                b=v_rgb[unique_vs, 2].astype(np.float32),
+            ),
+            face=dict(
+                vertex_indices=new_idx.reshape(-1, 3).astype(np.int32)
+            )
+        ))
+        
+    return meshes
+
+def extract_meshes_per_face_color(rgb, verts, tets):
+    n_ch = rgb.shape[-1]
+    color_names = ['r', 'g', 'b', 'a'][:n_ch]
+
+    # Step 1 & 2: Find connected components (This part is correct)
+    face_definitions = np.array([[0, 1, 2], [0, 3, 1], [0, 2, 3], [1, 3, 2]], dtype=np.int32)
+    all_faces = tets[:, face_definitions].reshape(-1, 3)
+    
+    face_map = defaultdict(list)
+    for i in range(len(all_faces)):
+        ti, _ = divmod(i, 4)
+        key = tuple(sorted(all_faces[i]))
+        face_map[key].append(ti)
+
+    tet_adj = defaultdict(list)
+    for key, t_indices in face_map.items():
+        if len(t_indices) == 2:
+            t0, t1 = t_indices
+            tet_adj[t0].append(t1)
+            tet_adj[t1].append(t0)
+            
+    components, seen = [], set()
+    for i in range(len(tets)):
+        if i in seen: continue
+        comp = []; q = deque([i])
+        while q:
+            ti = q.popleft()
+            if ti in seen: continue
+            seen.add(ti); comp.append(ti); q.extend(tet_adj[ti])
+        components.append(np.array(comp, dtype=np.int64))
+
+    # Step 3: Extract boundary meshes
+    meshes = []
+    for comp_t_indices in components:
+        comp_tets = tets[comp_t_indices]
+        comp_rgb = rgb[comp_t_indices]
+        
+        comp_all_faces = comp_tets[:, face_definitions].reshape(-1, 3)
+        face_tet_idx = np.arange(len(comp_tets)).repeat(4)
+        
+        faces_key = np.sort(comp_all_faces, axis=1)
+        _, inv, counts = np.unique(faces_key, axis=0, return_inverse=True, return_counts=True)
+        
+        b_mask = counts[inv] == 1
+        b_faces = comp_all_faces[b_mask]
+        b_face_tet_idx = face_tet_idx[b_mask]
+
+        if len(b_faces) == 0:
+            continue
+            
+        # --- NEW, EXPLICIT COLOR LOOKUP ---
+        n_new_verts = len(b_faces) * 3
+        new_verts_pos = np.zeros((n_new_verts, 3), dtype=np.float32)
+        new_verts_rgb = np.zeros((n_new_verts, n_ch), dtype=np.float32)
+        new_f_idx = np.arange(n_new_verts).reshape(-1, 3)
+
+        v_counter = 0
+        for i in range(len(b_faces)):
+            face_global_indices = b_faces[i]
+            tet_local_idx = b_face_tet_idx[i]
+
+            source_tet_v_indices = comp_tets[tet_local_idx]
+            source_tet_colors = comp_rgb[tet_local_idx]
+
+            for v_global_idx in face_global_indices:
+                # Find the local index (0-3) of the vertex in the tetrahedron
+                local_v_idx = np.where(source_tet_v_indices == v_global_idx)[0][0]
+
+                # Assign the correct position and color
+                new_verts_pos[v_counter] = verts[v_global_idx]
+                new_verts_rgb[v_counter] = source_tet_colors[local_v_idx]
+                v_counter += 1
+        # --- END NEW LOGIC ---
+
+        v_dict = dict(
+            x=new_verts_pos[:, 0].astype(np.float32),
+            y=new_verts_pos[:, 1].astype(np.float32),
+            z=new_verts_pos[:, 2].astype(np.float32),
+        )
+        for i, name in enumerate(color_names):
+            v_dict[name] = new_verts_rgb[:, i].astype(np.float32)
+
+        meshes.append(dict(
+            vertex=v_dict,
+            face=dict(
+                vertex_indices=new_f_idx.astype(np.int32)
+            )
+        ))
+            
     return meshes
diff --git a/utils/model_util.py b/utils/model_util.py
index 405d326..b4e38e4 100644
--- a/utils/model_util.py
+++ b/utils/model_util.py
@@ -1,17 +1,11 @@
 import torch
 from torch import nn
 from utils import hashgrid
-import math
 
-from utils.graphics_utils import l2_normalize_th
-from utils.topo_utils import calculate_circumcenters_torch, fibonacci_spiral_on_sphere, calc_barycentric, sample_uniform_in_sphere, project_points_to_tetrahedra, contraction_jacobian_d_in_chunks
+from utils.topo_utils import calculate_circumcenters_torch
 from utils.safe_math import safe_exp, safe_div, safe_sqrt
-from utils.contraction import contract_mean_std
-from utils.contraction import contract_points, inv_contract_points
-from sh_slang.eval_sh_py import eval_sh
-from utils.hashgrid import HashEmbedderOptimized
 from icecream import ic
-import torch.nn.init as init # Common alias for torch.nn.init
+import math
 
 C0 = 0.28209479177387814
 def RGB2SH(rgb):
@@ -105,24 +99,15 @@ def compute_vertex_colors_from_field(
       color = base_for_channel + dot(gradient_for_channel, normalized(vertex - circumcenter))
     """
     offsets = element_verts - circumcenters[:, None, :]
-
-    # grad_contrib: (T, V, C)
-    # gradients: (T, C, D) = (T, 3, 3)
-    # normalized_offsets: (T, V, D) = (T, 4, 3)
-    # einsum: 'tcd,tvd->tvc'
-    # t: batch (Elements)
-    # c: color channels (of gradient)
-    # d: spatial dimensions (of gradient and offset)
-    # v: vertices
-    # Output: for each element, for each vertex, for each color channel
     grad_contrib = torch.einsum('tcd,tvd->tvc', gradients, offsets)
     vertex_colors = base[:, None, :] + grad_contrib 
     
     return vertex_colors
 
 def offset_normalize(rgb, grd, circumcenters, tets):
-    grd = grd.reshape(-1, 1, 3) * rgb.reshape(-1, 3, 1).mean(dim=1, keepdim=True).detach()
-    radius = torch.linalg.norm(tets - circumcenters[:, None, :], dim=-1, keepdim=True)[:, :1]
+    # grd = grd.reshape(-1, 1, 3) * rgb.reshape(-1, 3, 1).mean(dim=1, keepdim=True).detach()
+    grd = grd.reshape(-1, 1, 3)# * rgb.reshape(-1, 3, 1).max(dim=1, keepdim=True).values.detach()
+    radius = torch.linalg.norm(tets[:, :1] - circumcenters[:, None, :], dim=-1, keepdim=True)
     normed_grd = safe_div(grd, radius)
     vcolors = compute_vertex_colors_from_field(
         tets.detach(), rgb.reshape(-1, 3), normed_grd.float(), circumcenters.float().detach())
@@ -130,34 +115,60 @@ def offset_normalize(rgb, grd, circumcenters, tets):
     base_color_v0_raw = vcolors[:, 0]
     return base_color_v0_raw, normed_grd
 
-def activate_output(camera_center, density, rgb, grd, sh, indices, circumcenters, vertices, current_sh_deg, max_sh_deg):
-    tets = vertices[indices]
-    base_color_v0_raw, normed_grd = offset_normalize(rgb, grd, circumcenters, tets)
-    tet_color_raw = eval_sh(
-        tets.mean(dim=1),
-        RGB2SH(base_color_v0_raw),
-        sh.reshape(-1, (max_sh_deg+1)**2 - 1, 3).half(),
-        camera_center,
-        current_sh_deg).float()
-    base_color_v0 = torch.nn.functional.softplus(tet_color_raw.reshape(-1, 3, 1), beta=10)
-    features = torch.cat([density, base_color_v0.reshape(-1, 3), normed_grd.reshape(-1, 3)], dim=1)
+@torch.jit.script
+def activate_output(camera_center, tet_color_raw, density, grd, circumcenters, tets):
+    tet_color = torch.nn.functional.softplus(tet_color_raw.reshape(-1, 3, 1), beta=10)
+    base_color_v0, normed_grd = offset_normalize(
+        tet_color, grd, circumcenters.detach(), tets.detach())
+    # offset = ((camera_center - tets[:, 0]) * normed_grd.reshape(-1, 3)).sum(dim=-1)
+    features = torch.cat([
+        density,
+        base_color_v0.reshape(-1, 3),# + offset.reshape(-1, 1),
+        normed_grd.reshape(-1, 3)
+    ], dim=1)
     return features.float()
 
+class GloMLP(torch.nn.Module):
+    def __init__(self, input_dim: int, output_dim: int):
+        super().__init__()
+        self.mlp = torch.nn.Sequential(
+            torch.nn.Linear(input_dim, 128),
+            torch.nn.SiLU(),
+            torch.nn.Linear(128, 256),
+            torch.nn.SiLU(),
+            torch.nn.Linear(256, 128),
+            torch.nn.SiLU(),
+            torch.nn.Linear(128, output_dim*2),
+        )
+        last = self.mlp[-1]
+        with torch.no_grad():
+            nn.init.xavier_uniform_(last.weight, 1e-3)
+            last.bias.zero_()
+
+    def forward(self, glo_latent, input_x):
+        out = self.mlp(glo_latent)
+        a, b = torch.split(out, out.shape[-1] // 2, dim=-1)
+        return input_x * torch.exp(a.clip(max=3)).reshape(1, -1) + b.reshape(1, -1)
+
+
 class iNGPDW(nn.Module):
     def __init__(self, 
                  sh_dim=0,
+                 glo_dim=0,
                  scale_multi=0.5,
                  log2_hashmap_size=16,
                  base_resolution=16,
                  per_level_scale=2,
                  L=10,
                  hashmap_dim=4,
+                 sh_hidden_dim=64,
                  hidden_dim=64,
                  g_init=1,
                  s_init=1e-4,
                  d_init=0.1,
                  c_init=0.6,
                  density_offset=-4,
+                 ablate_gradient = False,
                  **kwargs):
         super().__init__()
         self.scale_multi = scale_multi
@@ -167,45 +178,51 @@ class iNGPDW(nn.Module):
         self.base_resolution = base_resolution
         self.density_offset = density_offset
 
+        # self.gmul = 1 / math.sqrt(3) if not ablate_gradient else 0
+        self.gmul = 1 if not ablate_gradient else 0
+
         self.encoding = hashgrid.HashEmbedderOptimized(
             [torch.zeros((3)), torch.ones((3))],
             self.L, n_features_per_level=self.dim,
             log2_hashmap_size=log2_hashmap_size, base_resolution=base_resolution,
             finest_resolution=base_resolution*per_level_scale**self.L)
 
-        def mk_head(n):
+        self.n_output_dims = self.encoding.n_output_dims
+
+        def mk_head(n, hidden_dim):
             network = nn.Sequential(
-                nn.Linear(self.encoding.n_output_dims, hidden_dim),
+                nn.Linear(self.n_output_dims, hidden_dim),
+                nn.SELU(inplace=True),
+                nn.Linear(hidden_dim, hidden_dim),
                 nn.SELU(inplace=True),
                 nn.Linear(hidden_dim, hidden_dim),
                 nn.SELU(inplace=True),
                 nn.Linear(hidden_dim, n)
             )
-            gain = nn.init.calculate_gain('relu')  # for example, if using ReLU activations
+            gain = nn.init.calculate_gain('relu')
             network.apply(lambda m: init_linear(m, gain))
             return network
-        self.network = mk_head(1+12+sh_dim)
+        self.hidden_dim = hidden_dim
+        self.sh_hidden_dim = sh_hidden_dim
 
-        self.density_net   = mk_head(1)
-        self.color_net     = mk_head(3)
-        self.gradient_net  = mk_head(3)
-        self.sh_net        = mk_head(sh_dim)
+        self.density_net   = mk_head(1, hidden_dim)
+        self.color_net     = mk_head(3, hidden_dim)
+        self.gradient_net  = mk_head(3, hidden_dim)
+        self.sh_net        = mk_head(sh_dim, sh_hidden_dim)
+        self.glo_net = GloMLP(glo_dim, self.n_output_dims)
+        self.glo_dim = glo_dim
 
-        last = self.network[-1]
         with torch.no_grad():
-            last.weight[4:, :].zero_()
-            last.bias[4:].zero_()
             for network, eps in zip(
                 [self.gradient_net, self.sh_net, self.density_net, self.color_net], 
                 [g_init, s_init, d_init, c_init]):
                 last = network[-1]
                 with torch.no_grad():
-                    init.uniform_(last.weight.data, a=-eps, b=eps)
-                    # nn.init.xavier_uniform_(m.weight, gain)
+                    # init.uniform_(last.weight.data, a=-eps, b=eps)
+                    nn.init.xavier_uniform_(last.weight, eps)
                     last.bias.zero_()
 
-
-    def _encode(self, x: torch.Tensor, cr: torch.Tensor):
+    def encode(self, x: torch.Tensor, cr: torch.Tensor):
         x = x.detach()
         output = self.encoding(x).float()
         output = output.reshape(-1, self.dim, self.L)
@@ -215,21 +232,97 @@ class iNGPDW(nn.Module):
                          safe_sqrt(self.per_level_scale * 4*n*cr.reshape(-1, 1, 1)))
         scaling = torch.erf(erf_x)
         output = output * scaling
-        return output
+        return output.reshape(-1, self.L * self.dim)
+
 
+    def forward(self, x, cr, glo):
+        h = self.encode(x, cr)
+
+        sigma = self.density_net(h)
+        if glo is not None:
+            hglo = self.glo_net(glo, h)
+        else:
+            hglo = h
+        rgb = self.color_net(hglo)
+        field_samples = self.gradient_net(hglo)
+        sh  = self.sh_net(hglo)
+
+        rgb = rgb.reshape(-1, 3, 1) + 0.5
+        # rgb = torch.sigmoid(rgb.reshape(-1, 3, 1))
+        density = safe_exp(3*sigma+self.density_offset)
+        grd = torch.tanh(field_samples.reshape(-1, 1, 3)) * self.gmul
+        # grd = field_samples.reshape(-1, 1, 3)
+        # grd = rgb * torch.tanh(field_samples.reshape(-1, 3, 3))  # shape (T, 3, 3)
+        return density, rgb.reshape(-1, 3), grd, sh
+
+class Heads(nn.Module):
+    def __init__(self, 
+                 n_output_dims,
+                 sh_dim,
+                 glo_dim=0,
+                 scale_multi=0.5,
+                 log2_hashmap_size=16,
+                 base_resolution=16,
+                 per_level_scale=2,
+                 L=10,
+                 hashmap_dim=4,
+                 sh_hidden_dim=64,
+                 hidden_dim=64,
+                 g_init=1,
+                 s_init=1e-4,
+                 d_init=0.1,
+                 c_init=0.6,
+                 density_offset=-4,
+                 ablate_gradient = False,
+                 **kwargs):
+        super().__init__()
+        self.scale_multi = scale_multi
+        self.L = L
+        self.dim = hashmap_dim
+        self.per_level_scale = per_level_scale
+        self.base_resolution = base_resolution
+        self.density_offset = density_offset
 
-    def forward(self, x, cr):
-        output = self._encode(x, cr)
+        self.gmul = 1 / math.sqrt(3) if not ablate_gradient else 0
 
-        h = output.reshape(-1, self.L * self.dim)
+        def mk_head(n, hidden_dim):
+            network = nn.Sequential(
+                nn.Linear(n_output_dims, hidden_dim),
+                nn.SELU(inplace=True),
+                nn.Linear(hidden_dim, hidden_dim),
+                nn.SELU(inplace=True),
+                nn.Linear(hidden_dim, hidden_dim),
+                nn.SELU(inplace=True),
+                nn.Linear(hidden_dim, n)
+            )
+            gain = nn.init.calculate_gain('relu')
+            network.apply(lambda m: init_linear(m, gain))
+            return network
 
+        self.hidden_dim = hidden_dim
+        self.sh_hidden_dim = sh_hidden_dim
+        self.density_net   = mk_head(1, hidden_dim)
+        self.color_net     = mk_head(3, hidden_dim)
+        self.gradient_net  = mk_head(3, hidden_dim)
+        self.sh_net        = mk_head(sh_dim, sh_hidden_dim)
+        self.glo_net = GloMLP(glo_dim, n_output_dims)
+        self.glo_dim = glo_dim
+
+    def forward(self, h, glo):
+        h = h.float()
         sigma = self.density_net(h)
-        rgb = self.color_net(h)
-        field_samples = self.gradient_net(h)
-        sh  = self.sh_net(h).half()
+        if glo is not None:
+            hglo = self.glo_net(glo, h)
+        else:
+            hglo = h
+        rgb = self.color_net(hglo)
+        field_samples = self.gradient_net(hglo)
+        sh  = self.sh_net(hglo)
 
         rgb = rgb.reshape(-1, 3, 1) + 0.5
-        density = safe_exp(sigma+self.density_offset)
-        grd = torch.tanh(field_samples.reshape(-1, 1, 3)) / math.sqrt(3)
+        # rgb = torch.sigmoid(rgb.reshape(-1, 3, 1))
+        density = safe_exp(3*sigma+self.density_offset)
+        grd = torch.tanh(field_samples.reshape(-1, 1, 3)) * self.gmul
+        # grd = field_samples.reshape(-1, 1, 3)
         # grd = rgb * torch.tanh(field_samples.reshape(-1, 3, 3))  # shape (T, 3, 3)
         return density, rgb.reshape(-1, 3), grd, sh
diff --git a/utils/phong_shading.py b/utils/phong_shading.py
deleted file mode 100644
index 5929cb3..0000000
--- a/utils/phong_shading.py
+++ /dev/null
@@ -1,46 +0,0 @@
-import torch
-from utils.graphics_utils import l2_normalize_th
-from utils.safe_math import safe_exp, safe_div, safe_sqrt, safe_pow, safe_cos, safe_sin, remove_zero, safe_arctan2
-# @torch.jit.script
-def to_sphere(coordinates):
-    return torch.stack([
-        safe_cos(coordinates[..., 0]) * safe_sin(coordinates[..., 1]),
-        safe_sin(coordinates[..., 0]) * safe_sin(coordinates[..., 1]),
-        safe_cos(coordinates[..., 1]),
-    ], dim=-1)
-
-@torch.jit.script
-def light_function(base_color, reflection_dirs, light_colors, light_roughness, view_dirs, eps:float=torch.finfo(torch.float32).eps):
-    similarity = ((reflection_dirs * view_dirs).sum(dim=-1, keepdim=True) + 1)/2
-    mask = similarity > 0
-    spec_intensity = torch.where(mask, (similarity.clip(min=eps) ** light_roughness), 0)
-    spec_color = (light_colors * spec_intensity).sum(dim=1)
-    return torch.nn.functional.softplus(base_color + spec_color + 0.5, beta=10)
-
-@torch.jit.script
-def activate_lights(base_color_raw, lights, light_offset: float, dir_offset):
-    base_color = base_color_raw
-    light_colors = lights[:, :, :3]#+light_offset
-    # ic(base_color_raw, light_colors)
-    light_roughness = 4*safe_exp(lights[:, :, 3:4]).clip(max=100)
-    # ic(light_roughness)
-    num_lights = lights.shape[1]
-    reflection_dirs = 4*lights[:, :, 4:6] + dir_offset.reshape(1, -1, 2)[:, :num_lights]
-    return base_color, light_colors, light_roughness, reflection_dirs
-
-@torch.jit.script
-def compute_tet_color(base_color_raw, lights, vertices, indices, camera_center, light_offset: float, dir_offset):
-    base_color, light_colors, light_roughness, reflection_dirs = activate_lights(
-        base_color_raw, lights, light_offset, dir_offset)
-    reflection_dirs = to_sphere(reflection_dirs)
-    # reflection_dirs = lambert_to_sphere(reflection_dirs)
-    barycenters = vertices[indices].mean(dim=1)
-    view_dirs = l2_normalize_th(camera_center - barycenters).reshape(-1, 1, 3)
-    return light_function(base_color, reflection_dirs, light_colors, light_roughness, view_dirs)
-
-def compute_vert_color(base_color_raw, lights, vertices, camera_center, light_offset: float, dir_offset):
-    base_color, light_colors, light_roughness, reflection_dirs = activate_lights(
-        base_color_raw, lights, light_offset, dir_offset)
-    reflection_dirs = to_sphere(reflection_dirs)
-    view_dirs = l2_normalize_th(camera_center - vertices).reshape(-1, 1, 3)
-    return light_function(base_color, reflection_dirs, light_colors, light_roughness, view_dirs)
diff --git a/utils/test_util.py b/utils/test_util.py
index 959d1c7..afd6c92 100644
--- a/utils/test_util.py
+++ b/utils/test_util.py
@@ -7,7 +7,7 @@ import imageio
 from tqdm import tqdm
 from submodules.lpipsPyTorch import LPIPSEval
 from fused_ssim import fused_ssim
-from utils.train_util import render
+from delaunay_rasterization import render
 from scipy.spatial import Delaunay
 import matplotlib.pyplot as plt
 
diff --git a/utils/topo_utils.py b/utils/topo_utils.py
index 257ed7b..c1a3300 100644
--- a/utils/topo_utils.py
+++ b/utils/topo_utils.py
@@ -6,11 +6,11 @@ import time
 import torch
 from torch.autograd.functional import jacobian
 from icecream import ic
-from submodules.spectral_norm3 import compute_spectral_norm3
 from utils.contraction import contract_points, contraction_jacobian, contraction_jacobian_d_in_chunks
 import math
 from scipy.spatial import ConvexHull
 from utils.safe_math import safe_div
+# from submodules.spectral_norm3 import compute_spectral_norm3
 
 def tet_volumes(tets):
     v0 = tets[:, 0]
@@ -25,9 +25,42 @@ def tet_volumes(tets):
     mat = torch.stack((a, b, c), dim=1)
     det = torch.det(mat)
     
-    vol = torch.abs(det) / 6.0
+    vol = det / 6.0
     return vol
 
+def tet_surface_areas(tets):
+    v0 = tets[:, 0]
+    v1 = tets[:, 1]
+    v2 = tets[:, 2]
+    v3 = tets[:, 3]
+    
+    # Compute area of each face using cross product
+    area0 = 0.5 * torch.norm(torch.cross(v1 - v0, v2 - v0, dim=1), dim=1)
+    area1 = 0.5 * torch.norm(torch.cross(v1 - v0, v3 - v0, dim=1), dim=1)
+    area2 = 0.5 * torch.norm(torch.cross(v2 - v0, v3 - v0, dim=1), dim=1)
+    area3 = 0.5 * torch.norm(torch.cross(v2 - v1, v3 - v1, dim=1), dim=1)
+    
+    # Total surface area is the sum of all face areas
+    surface_area = area0 + area1 + area2 + area3
+    return surface_area
+
+@torch.no_grad()
+def tet_denom(tets):
+    v0 = tets[:, 0]
+    v1 = tets[:, 1]
+    v2 = tets[:, 2]
+    v3 = tets[:, 3]
+
+    # Compute vectors relative to v0
+    a = v1 - v0
+    b = v2 - v0
+    c = v3 - v0
+
+    cross_bc = torch.cross(b, c, dim=-1)
+    denominator = 2.0 * torch.sum(a * cross_bc, dim=-1)
+
+    return denominator
+
 @torch.jit.script
 def calc_barycentric(points, tets):
     """
@@ -239,7 +272,7 @@ def compute_vertex_sensitivity(indices: torch.Tensor, vertices: torch.Tensor,
     return tet_sens, vertex_sensitivity.reshape(num_vertices, -1)
 
 def fibonacci_spiral_on_sphere(n_points: int, 
-                               radius: float = 1.0, 
+                               radius = 1.0, 
                                device: str = 'cpu') -> torch.Tensor:
     """
     Generate points on a sphere (approximately evenly) via a Fibonacci spiral.
@@ -404,4 +437,4 @@ def max_density_contrast(vertices, indices, density,
     # PyTorch  2.1
     contrast.scatter_reduce_(0, src_idx, edge_val, reduce="amax")
 
-    return contrast
\ No newline at end of file
+    return contrast
diff --git a/utils/train_util.py b/utils/train_util.py
index 3e342ae..f810d03 100644
--- a/utils/train_util.py
+++ b/utils/train_util.py
@@ -1,24 +1,38 @@
 import torch
 import time
 import math
-from data.camera import Camera
-from utils import optim
-from sh_slang.eval_sh import eval_sh
-from delaunay_rasterization.internal.alphablend_tiled_slang_interp import AlphaBlendTiledRender as Render
-from delaunay_rasterization.internal.alphablend_tiled_slang_linear import AlphaBlendTiledRender as LinearRender
-from delaunay_rasterization.internal.render_grid import RenderGrid
-from delaunay_rasterization.internal.tile_shader_slang import vertex_and_tile_shader, point2image
 import numpy as np
 from utils import topo_utils
 from icecream import ic
 import math
-from utils.contraction import contraction_jacobian
 from utils.graphics_utils import l2_normalize_th
-import matplotlib.pyplot as plt
-from delaunay_rasterization.internal.alphablend_tiled_slang import render_constant_color
 from data.camera import focal2fov
+from pathlib import Path, PosixPath
+import json
+
+class CustomEncoder(json.JSONEncoder):
+    def default(self, o):
+        if isinstance(o, PosixPath):
+            return str(o)
+        return super().default(o)
+
+class SimpleSampler:
+    def __init__(self, total_num_samples, batch_size):
+        self.total_num_samples = total_num_samples
+        self.batch_size = batch_size
+        self.curr = total_num_samples
+        self.ids = None
+
+    def nextids(self, batch_size=None):
+        batch_size = self.batch_size if batch_size is None else batch_size
+        self.curr += batch_size
+        if self.curr + batch_size > self.total_num_samples:
+            # self.ids = torch.LongTensor(np.random.permutation(self.total_num_samples))
+            self.ids = torch.randperm(self.total_num_samples, dtype=torch.long, device=device)
+            self.curr = 0
+        ids = self.ids[self.curr : self.curr + batch_size]
+        return ids
 
-cmap = plt.get_cmap("jet")
 
 class ClippedGradients(torch.autograd.Function):
     @staticmethod
@@ -47,139 +61,6 @@ class ScaledGradients(torch.autograd.Function):
         lr_matrix, = ctx.saved_tensors
         return grad_output * lr_matrix, None
 
-def get_slang_projection_matrix(znear, zfar, fy, fx, height, width, device):
-    tanHalfFovX = width/(2*fx)
-    tanHalfFovY = height/(2*fy)
-
-    top = tanHalfFovY * znear
-    bottom = -top
-    right = tanHalfFovX * znear
-    left = -right
-
-    z_sign = 1.0
-
-    P = torch.tensor([
-       [2.0 * znear / (right - left),     0.0,                          (right + left) / (right - left), 0.0 ],
-       [0.0,                              2.0 * znear / (top - bottom), (top + bottom) / (top - bottom), 0.0 ],
-       [0.0,                              0.0,                          z_sign * zfar / (zfar - znear),  -(zfar * znear) / (zfar - znear) ],
-       [0.0,                              0.0,                          z_sign,                          0.0 ]
-    ], device=device)
-
-    return P
-
-def common_camera_properties_from_gsplat(viewmats, Ks, height, width):
-    """ Fetches all the Camera properties from the inria defined object"""
-    zfar = 100.0
-    znear = 0.01
-  
-    world_view_transform = viewmats
-    fx = Ks[0,0]
-    fy = Ks[1,1]
-    projection_matrix = get_slang_projection_matrix(znear, zfar, fy, fx, height, width, Ks.device)
-    fovx = focal2fov(fx, width)
-    fovy = focal2fov(fy, height)
-
-    cam_pos = viewmats.inverse()[:, 3]
-
-    return world_view_transform, projection_matrix, cam_pos, fovy, fovx
-
-def inverse_sigmoid(y):
-    return torch.log(y / (1 - y))
-
-def rgbs_activation(rgbs_raw):
-    # rgbs = torch.cat([torch.nn.functional.softplus(1e-1*rgbs_raw[:, :3]), safe_exp(rgbs_raw[:, 3:])], dim=1)
-    rgbs = torch.cat([torch.sigmoid(rgbs_raw[:, :3]), safe_exp(rgbs_raw[:, 3:])], dim=1)
-    return rgbs
-
-def safe_exp(x):
-    return x.clip(max=5).exp()
-
-def safe_trig_helper(x, fn, t=100 * torch.pi):
-    """Helper function used by safe_cos/safe_sin: mods x before sin()/cos()."""
-    return fn(torch.nan_to_num(torch.where(torch.abs(x) < t, x, x % t)))
-
-
-def safe_cos(x):
-    """jnp.cos() on a TPU may NaN out for large values."""
-    return safe_trig_helper(x, torch.cos)
-
-
-def safe_sin(x):
-    """jnp.sin() on a TPU may NaN out for large values."""
-    return safe_trig_helper(x, torch.sin)
-
-
-def render(camera: Camera, model, cell_values=None, tile_size=16, min_t=0.1,
-           scene_scaling=1, clip_multi=0, ray_jitter=None,
-           **kwargs):
-    device = model.device
-    if ray_jitter is None:
-        ray_jitter = 0.5*torch.ones((camera.image_height, camera.image_width, 2), device=device)
-    else:
-        assert(ray_jitter.shape[0] == camera.image_height)
-        assert(ray_jitter.shape[1] == camera.image_width)
-        assert(ray_jitter.shape[2] == 2)
-    vertices = model.vertices
-    
-    render_grid = RenderGrid(camera.image_height,
-                             camera.image_width,
-                             tile_height=tile_size,
-                             tile_width=tile_size)
-    tcam = dict(
-        tile_height=tile_size,
-        tile_width=tile_size,
-        grid_height=render_grid.grid_height,
-        grid_width=render_grid.grid_width,
-        min_t=min_t,
-        **camera.to_dict(device)
-    )
-    sorted_tetra_idx, tile_ranges, vs_tetra, circumcenter, mask, _ = vertex_and_tile_shader(
-        model.indices,
-        vertices,
-        tcam,
-        render_grid)
-    extras = {}
-    if cell_values is None:
-        cell_values = torch.zeros((mask.shape[0], model.feature_dim), device=circumcenter.device)
-        if mask.sum() > 0 and model.mask_values:
-            normed_cc, cell_values[mask] = model.get_cell_values(camera, mask, circumcenter[mask])
-        else:
-            normed_cc, cell_values = model.get_cell_values(camera, all_circumcenters=circumcenter)
-        if clip_multi > 0 and not model.frozen:
-            with torch.no_grad():
-                tet_sens, sensitivity = topo_utils.compute_vertex_sensitivity(model.indices[mask],
-                                                                            vertices, normed_cc, True)
-                                                                            # vertices, normed_cc, model.contract_vertices)
-                scaling = clip_multi*sensitivity.reshape(-1, 1).clip(min=1e-5)
-            vertices = ClippedGradients.apply(vertices, scaling)
-
-    mod = LinearRender if model.linear else Render
-    image_rgb, distortion_img, tet_alive = mod.apply(
-        sorted_tetra_idx,
-        tile_ranges,
-        model.indices,
-        vertices,
-        cell_values,
-        render_grid,
-        tcam,
-        ray_jitter)
-    alpha = image_rgb.permute(2,0,1)[3, ...]
-    total_density = (distortion_img[:, :, 2]**2).clip(min=1e-6)
-    distortion_loss = (((distortion_img[:, :, 0] - distortion_img[:, :, 1]) + distortion_img[:, :, 4]) / total_density).clip(min=0)
-    
-    render_pkg = {
-        'render': image_rgb.permute(2,0,1)[:3, ...],
-        'alpha': alpha,
-        'distortion_img': distortion_img,
-        'distortion_loss': distortion_loss.mean(),
-        'visibility_filter': mask,
-        'circumcenters': circumcenter,
-        'density': cell_values[:, 0],
-        'mask': mask,
-        **extras
-    }
-    return render_pkg
-
 def get_expon_lr_func(
     lr_init, lr_final, lr_delay_steps=0, lr_delay_mult=1.0, max_steps=1000000
 ):
@@ -253,6 +134,8 @@ class SpikingLR:
 
     def __call__(self, iteration):
         base_f = self.base_function(iteration)
+        if self.duration == 0:
+            return base_f
         if iteration < self.peak_start:
             return base_f
         elif iteration > self.peak_end:
@@ -263,116 +146,50 @@ class SpikingLR:
         height = self.peak_height_fn(peak_ind) - self.base_function(peak_ind)
         return base_f + self.peak_fn(last_peak, height)
 
-def render_debug(render_tensor, model, camera, density_multi=1):
-
-    # Convert to RGB (NxMx3) using the colormap
-    _, features = model.get_cell_values(camera)
-    tet_grad_color = torch.zeros((features.shape[0], 4), device=features.device)
-    if render_tensor.shape[1] == 1:
-        tensor_min, tensor_max = render_tensor.min(), torch.quantile(render_tensor, 0.99)
-        normalized_tensor = ((render_tensor - tensor_min) / (tensor_max - tensor_min)).clip(0, 1)
-        normalized_tensor = torch.as_tensor(
-            cmap(normalized_tensor.reshape(-1).cpu().numpy())).float().cuda()
-    else:
-        normalized_tensor = render_tensor
-    tet_grad_color[:, :normalized_tensor.shape[1]] = normalized_tensor
-    if render_tensor.shape[1] < 4:
-        tet_grad_color[:, 3] = features[:, 0] * density_multi# * render_tensor.reshape(-1)
-    render_pkg = render_constant_color(model.indices, model.vertices, None, camera, cell_values=tet_grad_color)
-
-    image = render_pkg['render']
-    image = image.permute(1, 2, 0)
-    image = (image.detach().cpu().numpy() * 255).clip(min=0, max=255).astype(np.uint8)
-
-    del render_pkg, render_tensor
-    return image
-
-def select_n(tet_err_weight, N):
-    rgbs_threshold = torch.sort(tet_err_weight).values[-min(int(N), tet_err_weight.shape[0])]
-    clone_mask = (tet_err_weight > rgbs_threshold)
-    return clone_mask
-
-def get_approx_ray_intersections(split_rays_data, epsilon=1e-7):
-    """
-    Calculates the approximate intersection point for pairs of line segments.
-
-    The intersection is defined as the midpoint of the shortest segment
-    connecting the two input line segments.
-
-    Args:
-        split_rays_data (torch.Tensor): Tensor of shape (N, 2, 6).
-            - N: Number of segment pairs.
-            - 2: Represents the two segments in a pair.
-            - 6: Contains [Ax, Ay, Az, Bx, By, Bz] for each segment,
-                 where A and B are the segment endpoints.
-                 Based on current Python code:
-                 A = average_P_exit, B = average_P_entry
-        epsilon (float): Small value to handle parallel lines and avoid
-                         division by zero if a segment has zero length.
-
-    Returns:
-        torch.Tensor: Tensor of shape (N, 3) representing the approximate
-                      "intersection" points (midpoints of closest approach).
-    """
-    # Segment 1 endpoints
-    p1_a = split_rays_data[:, 0, 0:3]  # Endpoint A of first segments (N, 3)
-    p1_b = split_rays_data[:, 0, 3:6]  # Endpoint B of first segments (N, 3)
-    # Segment 2 endpoints
-    p2_a = split_rays_data[:, 1, 0:3]  # Endpoint A of second segments (N, 3)
-    p2_b = split_rays_data[:, 1, 3:6]  # Endpoint B of second segments (N, 3)
-
-    # Define segment origins and direction vectors
-    # Segment S1: o1 + s * d1, for s in [0, 1]
-    # Segment S2: o2 + t * d2, for t in [0, 1]
-    o1 = p1_a
-    d1 = p1_b - p1_a  # Direction vector for segment 1 (from A to B)
-    o2 = p2_a
-    d2 = p2_b - p2_a  # Direction vector for segment 2 (from A to B)
-
-    # Calculate terms for finding closest points on the infinite lines
-    # containing the segments (based on standard formulas, e.g., Christer Ericson's "Real-Time Collision Detection")
-    v_o = o1 - o2 # Vector from origin of line 2 to origin of line 1
-
-    a = torch.sum(d1 * d1, dim=1)  # Squared length of d1
-    b = torch.sum(d1 * d2, dim=1)  # Dot product of d1 and d2
-    c = torch.sum(d2 * d2, dim=1)  # Squared length of d2
-    d = torch.sum(d1 * v_o, dim=1) # d1 dot (o1 - o2)
-    e = torch.sum(d2 * v_o, dim=1) # d2 dot (o1 - o2)
-
-    denom = a * c - b * b
-    
-    # Parameters for closest points on the *infinite lines*
-    # s_line = (b*e - c*d) / denom
-    # t_line = (a*e - b*d) / denom (this t_line corresponds to -t in some formulations, careful with sign)
-    # The t_line should be for the parameterization o2 + t*d2.
-    # If P1 = o1 + s*d1 and P2 = o2 + t*d2, and we minimize ||P1-P2||^2,
-    # by setting derivatives w.r.t s and t to 0, we get:
-    # s * (d1.d1) - t * (d1.d2) = -d1.(o1-o2) = d1.v_o = d
-    # s * (d1.d2) - t * (d2.d2) = -d2.(o1-o2) = d2.v_o = e
-    # Solving this system:
-    # s_line = (d*c - e*b) / denom
-    # t_line = (d*b - e*a) / denom -> this results in parameter for -d2 if system set up for P1-P2
-    # Or, more directly for t_line for P2 = o2 + t*d2: t_line = (b*d - a*e) / denom
-    
-    s_line_num = (b * e) - (c * d)
-    t_line_num = (a * e) - (b * d) # This corresponds to t_c = (a*e - b*d)/denom from previous thoughts for P(t) = O2 + tD2
-
-    # Handle near-zero denominator (lines are parallel or one segment is a point)
-    # We compute with a safe denominator, then clamp. Clamping is key for segments.
-    denom_safe = torch.where(denom.abs() < epsilon, torch.ones_like(denom), denom)
-    
-    s_line = s_line_num / denom_safe
-    t_line = t_line_num / denom_safe # Note: This t_line is for the parameter of d2 (from o2)
-
-    # Clamp parameters to [0, 1] to stay within the segments
-    bad_intersect = (s_line < 0) | (t_line < 0) | (s_line > 1) | (t_line > 1)
-    s_seg = torch.clamp(s_line, 0.0, 1.0)
-    t_seg = torch.clamp(t_line, 0.0, 1.0)
-
-    # Points of closest approach on the segments
-    pc1 = o1 + s_seg.unsqueeze(1) * d1
-    pc2 = o2 + t_seg.unsqueeze(1) * d2
-    
-    p_int = (pc1 + pc2) / 2.0
-                        
-    return p_int, bad_intersect
+class TwoPhaseLR:
+    def __init__(self, max_i, start_i, period_i, settle_i, 
+                 lr_peak, lr_end_peak, lr_trough, lr_final):
+        self.max_i = max_i
+        self.start_i = start_i
+        self.settle_i = settle_i
+        self.period_i = period_i
+        self.lr_peak = lr_peak
+        self.lr_end_peak = lr_end_peak
+        self.lr_trough = lr_trough
+        self.lr_final = lr_final
+
+        n_cycles = settle_i / period_i
+        self.gamma = (lr_end_peak / lr_peak) ** (1 / n_cycles) if n_cycles > 0 else 1
+
+    def __call__(self, i):
+        # Phase 1: Spiking with decaying cosine annealing
+        if i < self.start_i:
+            return get_expon_lr_func(self.lr_peak, self.lr_trough, max_steps=self.start_i)(i)
+        elif self.start_i <= i <= self.settle_i:
+            cycle = math.floor((i-self.start_i) / self.period_i)
+            t_cycle = (i-self.start_i) % self.period_i
+            
+            lr_max = self.lr_peak * (self.gamma ** cycle)
+            
+            height = (lr_max - self.lr_trough)
+            # lr = self.lr_trough + 0.5 * height * \
+            #      (1 + math.cos(math.pi * t_cycle / self.period_i))
+            t = t_cycle / self.period_i
+            lr = self.lr_trough + np.exp(np.log(height) * (1 - t) + np.log(1e-6) * t)
+            
+            return lr
+
+        # Phase 2: Final settling cosine decay
+        else:
+            if i >= self.max_i:
+                return self.lr_final
+
+            t_settle = i - self.settle_i
+            d_settle = self.max_i - self.settle_i
+            if d_settle <= 0:
+                return self.lr_final
+            
+            lr = self.lr_final + 0.5 * (self.lr_trough - self.lr_final) * \
+                 (1 + math.cos(math.pi * t_settle / d_settle))
+
+            return lr
diff --git a/utils/viz_util.py b/utils/viz_util.py
deleted file mode 100644
index a04c52c..0000000
--- a/utils/viz_util.py
+++ /dev/null
@@ -1,55 +0,0 @@
-import ipywidgets as widgets
-from IPython.display import display, clear_output
-from PIL import Image
-import IPython.display as ipd
-import numpy as np
-
-def create_image_viewer(image_list, start_index=0, skip_value=1):
-    """
-    Creates an interactive viewer to flip through a list of loaded images.
-
-    Parameters:
-        image_list (list): A list of PIL images or file paths.
-    """
-    if not image_list:
-        print("No images to display.")
-        return
-    
-    index = {'value': start_index}  # Mutable dictionary to track image index
-    output = widgets.Output()  # Output widget for updating the display
-
-    def show_image():
-        """Updates the displayed image."""
-        with output:
-            clear_output(wait=True)  # Clear previous image before updating
-            img = image_list[index['value']]
-            if isinstance(img, str):  # If path, open the image
-                img = Image.open(img)
-            if isinstance(img, np.ndarray):  # If path, open the image
-                img = Image.fromarray((img*255).astype(np.uint8))
-            display(img)
-            print(f"Image {index['value'] + 1} / {len(image_list)}")  # Display image number
-
-    def next_image(b):
-        """Moves to the next image."""
-        index['value'] = (index['value'] + skip_value) % len(image_list)
-        show_image()
-
-    def prev_image(b):
-        """Moves to the previous image."""
-        index['value'] = (index['value'] - skip_value) % len(image_list)
-        show_image()
-
-    # Create buttons
-    prev_button = widgets.Button(description="Previous")
-    next_button = widgets.Button(description="Next")
-
-    prev_button.on_click(prev_image)
-    next_button.on_click(next_image)
-
-    # Display UI
-    display(widgets.HBox([prev_button, next_button]))
-    display(output)
-
-    # Show first image
-    show_image()
